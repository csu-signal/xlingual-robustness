{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a093c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eeef27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_languages = pd.read_csv('../finetuning_BERT_NER/all_languages_pair_3', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aff4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataProcessor import SectionTitleData,DataProcessor\n",
    "dp=DataProcessor()\n",
    "st=SectionTitleData(dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed839b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language_accuracy=pd.DataFrame(columns=[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\"total tokens of l1\",\"total tokens of l2\",\n",
    "#                                     \"eval_loss\",\"precision\",\"recall\",\"f1_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89403286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n",
      "fr\n",
      "br\n",
      "br\n",
      "br\n",
      "br\n",
      "br\n",
      "ar\n",
      "fa\n",
      "ar\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "fa\n",
      "ar\n",
      "hi\n",
      "ar\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "en\n",
      "sco\n",
      "en\n",
      "sco\n",
      "sco\n",
      "sco\n",
      "sco\n",
      "sco\n",
      "en\n",
      "cy\n",
      "en\n",
      "cy\n",
      "cy\n",
      "cy\n",
      "cy\n",
      "cy\n",
      "es\n",
      "ca\n",
      "es\n",
      "ca\n",
      "ca\n",
      "ca\n",
      "ca\n",
      "ca\n",
      "cs\n",
      "sk\n",
      "cs\n",
      "sk\n",
      "sk\n",
      "sk\n",
      "sk\n",
      "sk\n",
      "id\n",
      "ms\n",
      "id\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "ms\n",
      "fr\n",
      "oc\n",
      "fr\n",
      "oc\n",
      "oc\n",
      "oc\n",
      "oc\n",
      "oc\n",
      "nl\n",
      "af\n",
      "nl\n",
      "af\n",
      "af\n",
      "af\n",
      "af\n",
      "af\n",
      "nl\n",
      "af\n",
      "nl\n",
      "af\n",
      "af\n",
      "af\n",
      "af\n",
      "af\n",
      "it\n",
      "scn\n",
      "it\n",
      "scn\n",
      "scn\n",
      "scn\n",
      "scn\n",
      "scn\n",
      "es\n",
      "an\n",
      "es\n",
      "an\n",
      "an\n",
      "an\n",
      "an\n",
      "an\n",
      "es\n",
      "ast\n",
      "es\n",
      "ast\n",
      "ast\n",
      "ast\n",
      "ast\n",
      "ast\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "from SimpleTransformers import titleModel\n",
    "titles_indices={'A':0,'B':1,'C':2,'D':3}\n",
    "training_percentage=.8\n",
    "for l_index,lang2 in all_languages.iterrows():\n",
    "#     if(l_index==0):\n",
    "        language_source=lang2['l1']\n",
    "        language_target=lang2['l2']\n",
    "\n",
    "        print(language_source)\n",
    "        print(language_target)\n",
    "\n",
    "        with open(path+language_source+'/title_source_training.pkl','rb') as file:\n",
    "            source_training=pickle.load(file)\n",
    "\n",
    "        with open(path+language_source+'/title_source_test.pkl','rb') as file:\n",
    "            source_test=pickle.load(file)\n",
    "\n",
    "        with open(path+language_target+'/title_target_training.pkl','rb') as file:\n",
    "            target_training=pickle.load(file)\n",
    "\n",
    "        with open(path+language_target+'/title_target_test.pkl','rb') as file:\n",
    "            target_test=pickle.load(file)\n",
    "        \n",
    "        source_training_titles=[]\n",
    "        for example in source_training:\n",
    "            source_training_titles.append(example.endings[titles_indices[example.label]])\n",
    "        with open(path+language_source+'/only_title_source_training.pkl','wb') as file:\n",
    "            pickle.dump(source_training_titles,file)\n",
    "            \n",
    "        source_test_titles=[]\n",
    "        for example in source_test:\n",
    "            source_test_titles.append(example.endings[titles_indices[example.label]])\n",
    "        with open(path+language_source+'/only_title_source_test.pkl','wb') as file:\n",
    "            pickle.dump(source_test_titles,file)\n",
    "    \n",
    "        target_training_titles=[]\n",
    "        for example in target_training:\n",
    "            target_training_titles.append(example.endings[titles_indices[example.label]])\n",
    "        with open(path+language_target+'/only_title_target_training.pkl','wb') as file:\n",
    "            pickle.dump(target_training_titles,file)\n",
    "            \n",
    "        target_test_titles=[]\n",
    "        for example in target_test:\n",
    "            target_test_titles.append(example.endings[titles_indices[example.label]])\n",
    "        with open(path+language_target+'/only_title_target_test.pkl','wb') as file:\n",
    "            pickle.dump(target_test_titles,file)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb3b5e",
   "metadata": {},
   "source": [
    "# Title Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6011fed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pylev\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "title1_processed=[]\n",
    "title2_processed=[]\n",
    "for i1,title1 in enumerate(source_training_titles):\n",
    "    title1=title1.translate(str.maketrans('', '', string.punctuation))\n",
    "    title1_processed.append([word for word in title1.lower().split() if word not in stopwords.words(\"french\")])\n",
    "# for title2 in target_test_titles:\n",
    "#     title2_processed.append([word for word in title2.split() if word not in stopwords.words(\"french\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "184a5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2257 1627\n",
      "Poésie/Poésie:1.0\n",
      "6690 1627\n",
      "C et G/G:1.0\n",
      "7878 1627\n",
      "Poésie/Poésie:1.0\n",
      "7885 1627\n",
      "Poésie/Poésie:1.0\n",
      "14977 1627\n",
      "Poésie/Poésie:1.0\n",
      "17405 1627\n",
      "Poésie/Poésie:1.0\n",
      "18032 1627\n",
      "E/E:1.0\n",
      "26518 1627\n",
      "Poésie/Poésie:1.0\n",
      "28862 1627\n",
      "Poésie/Poésie:1.0\n",
      "28926 1627\n",
      "Arme de/Arme:1.0\n",
      "30012 1627\n",
      "Guerre du Viêt Nam/Unvaniñ Viêt Nam:0.6666666666666667\n",
      "31113 1627\n",
      "Poésie/Poésie:1.0\n",
      "33185 1627\n",
      "Poésie/Poésie:1.0\n",
      "34871 1627\n",
      "Media/Media:1.0\n",
      "41358 1627\n",
      "Poésie/Poésie:1.0\n",
      "41610 1627\n",
      "Redadeg 2008/Hent redadeg 2008:0.6666666666666667\n",
      "45694 1627\n",
      "Poésie/Poésie:1.0\n",
      "54994 1627\n",
      "Poésie/Poésie:1.0\n",
      "60120 1627\n",
      "Latin/Latin:1.0\n",
      "62300 1627\n",
      "Poésie/Poésie:1.0\n"
     ]
    }
   ],
   "source": [
    "for i1,title1_split in enumerate(title1_processed):\n",
    "    min_dis=1\n",
    "    min_title=''\n",
    "    for i2,title2 in enumerate(target_test_titles):\n",
    "        title2_split=title2.lower().split(\" \")\n",
    "        dis=pylev.levenshtein(title1_split,title2_split)/max(len(title1_split),len(title2_split))\n",
    "        if(dis<min_dis):\n",
    "            min_dis=dis\n",
    "            min_title=title2\n",
    "    if(1-min_dis)>.5:\n",
    "        print(i1,i2)\n",
    "        print(source_training_titles[i1]+\"/\"+min_title+\":\"+str(1-min_dis))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19846d60",
   "metadata": {},
   "source": [
    "# Text Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c88c468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "overlap_df=pd.DataFrame(columns=['l1','l2','total tokens of l1',\n",
    "                                 'total tokens of l2',\n",
    "                                 'shared non-u in l1',\n",
    "                                 'shared non-u in l2',\n",
    "                                 'shared unique in both',\n",
    "                                 'non-un token overlap in l1',\n",
    "                                 'non-un token overlap in l2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f70c4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /s/chopin/l/grad/shadim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n",
      "fa\n",
      "['fa', 'fa', 8876932, 2207020, 8236921, 2110998, 120752, 0.9279017795788004, 0.9564924649527418]\n",
      "fa\n",
      "fa\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m input_text\u001b[38;5;241m=\u001b[39mexample\u001b[38;5;241m.\u001b[39mcontexts[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m sample_lines\u001b[38;5;241m=\u001b[39minput_text\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m---> 39\u001b[0m sample_lines_tokenized \u001b[38;5;241m=\u001b[39m [word_tokenize(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m sample_lines]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m sample_lines_tokenized:\n\u001b[1;32m     41\u001b[0m     source_texts\u001b[38;5;241m.\u001b[39mextend(remove_punctuation_stop(line,language_source))\n",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m input_text\u001b[38;5;241m=\u001b[39mexample\u001b[38;5;241m.\u001b[39mcontexts[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m sample_lines\u001b[38;5;241m=\u001b[39minput_text\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m---> 39\u001b[0m sample_lines_tokenized \u001b[38;5;241m=\u001b[39m [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m sample_lines]\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m sample_lines_tokenized:\n\u001b[1;32m     41\u001b[0m     source_texts\u001b[38;5;241m.\u001b[39mextend(remove_punctuation_stop(line,language_source))\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/nltk/tokenize/__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/nltk/tokenize/__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m ]\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/nltk/tokenize/destructive.py:178\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    175\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mENDING_QUOTES:\n\u001b[0;32m--> 178\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[1;32m    181\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from SimpleTransformers import titleModel\n",
    "titles_indices={'A':0,'B':1,'C':2,'D':3}\n",
    "training_percentage=.8\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    return [token for token in input_text if token not in set(string.punctuation)]\n",
    "\n",
    "def remove_punctuation_stop(input_text,lang):\n",
    "        stop_words=stopwords.stopwords(lang)\n",
    "        remove_set=set(string.punctuation).union(set(stop_words))\n",
    "        return [token for token in input_text if not token in remove_set]\n",
    "\n",
    "for l_index,lang2 in all_languages.iterrows():\n",
    "\n",
    "    if not ((lang2['l1']==overlap_df['l1'])&(lang2['l2']==overlap_df['l2'])).any():\n",
    "                language_source='fa'\n",
    "                language_target='fa'\n",
    "                print(language_source)\n",
    "                print(language_target)            \n",
    "\n",
    "#         if not os.path.isfile(path+language_source+'/only_text_nostop_source_training.pkl'):\n",
    "            \n",
    "                with open(path+language_source+'/title_training.pkl','rb') as file:\n",
    "                    source_training=pickle.load(file)\n",
    "                source_texts=[]\n",
    "                for example in source_training:\n",
    "                    input_text=example.contexts[0]\n",
    "                    sample_lines=input_text.splitlines()\n",
    "                    sample_lines_tokenized = [word_tokenize(line) for line in sample_lines]\n",
    "                    for line in sample_lines_tokenized:\n",
    "                        source_texts.extend(remove_punctuation_stop(line,language_source))\n",
    "#                 with open(path+language_source+'/only_text_nostop_source_training.pkl','wb') as file:\n",
    "#                     pickle.dump(source_texts,file)            \n",
    "#         else:\n",
    "#             with open(path+language_source+'/only_text_nostop_source_training.pkl','rb') as file:\n",
    "#                 source_texts=pickle.load(file)\n",
    "\n",
    "        \n",
    "\n",
    "#         if not os.path.isfile(path+language_target+'/only_text_nostop_target_test.pkl'):\n",
    "                with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "                    target_test=pickle.load(file)\n",
    "                target_texts=[]\n",
    "                for example in target_test:\n",
    "                    input_text=example.contexts[0]\n",
    "                    sample_lines=input_text.splitlines()\n",
    "                    sample_lines_tokenized = [word_tokenize(line) for line in sample_lines]\n",
    "                    for line in sample_lines_tokenized:\n",
    "                        target_texts.extend(remove_punctuation_stop(line,language_target))\n",
    "#                 with open(path+language_target+'/only_text_nostop_target_test.pkl','wb') as file:\n",
    "#                     pickle.dump(target_texts,file) \n",
    "#         else:\n",
    "#             with open(path+language_target+'/only_text_nostop_target_test.pkl','rb') as file:\n",
    "#                 target_texts=pickle.load(file)\n",
    "\n",
    "                shared_unique=set(source_texts).intersection(set(target_texts))\n",
    "\n",
    "                count1=Counter(source_texts)\n",
    "                occur1=[count1[token] for token in shared_unique]\n",
    "                all_occur1=sum(occur1)\n",
    "                count2=Counter(target_texts)\n",
    "                occur2=[count2[token] for token in shared_unique]\n",
    "                all_occur2=sum(occur2)\n",
    "\n",
    "\n",
    "                info=[language_source,language_target,len(source_texts),\n",
    "                     len(target_texts),all_occur1,all_occur2,len(shared_unique),\n",
    "                               all_occur1/len(source_texts),all_occur2/len(target_texts)]\n",
    "                print(info)\n",
    "                overlap_df.loc[len(overlap_df.index)]=info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672dff29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "allsection=[]\n",
    "for i in source_training:\n",
    "    allsection.append(i.contexts[0])\n",
    "for i in target_test:\n",
    "    allsection.append(i.contexts[0])\n",
    "x=[item for item, count in collections.Counter(allsection).items() if count > 1]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e52660",
   "metadata": {},
   "source": [
    "# First 128 tokens overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18e4b2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /s/chopin/l/grad/shadim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model to GPU...\n",
      "fa\n",
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fa', 'fa', 2923352, 728608, 2656608, 718380, 102950, 0.908754060407368, 0.985962273266283]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'overlap_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 156\u001b[0m\n\u001b[1;32m    152\u001b[0m info\u001b[38;5;241m=\u001b[39m[language_source,language_target,\u001b[38;5;28mlen\u001b[39m(source_training_texts),\n\u001b[1;32m    153\u001b[0m      \u001b[38;5;28mlen\u001b[39m(target_test_texts),all_occur1,all_occur2,\u001b[38;5;28mlen\u001b[39m(shared_unique),\n\u001b[1;32m    154\u001b[0m                all_occur1\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(source_training_texts),all_occur2\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(target_test_texts)]\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(info)\n\u001b[0;32m--> 156\u001b[0m \u001b[43moverlap_df\u001b[49m\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(overlap_df\u001b[38;5;241m.\u001b[39mindex)]\u001b[38;5;241m=\u001b[39minfo\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overlap_df' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from SimpleTransformers import titleModel\n",
    "titles_indices={'A':0,'B':1,'C':2,'D':3}\n",
    "training_percentage=.8\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import stopwordsiso as stopwords\n",
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"        \n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "device = torch.device('cuda')\n",
    "torch. cuda. empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import string\n",
    "import stopwordsiso as stopwords\n",
    "from transformers import BertTokenizerFast,XLMRobertaTokenizerFast\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    ")\n",
    "from transformers import (\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForTokenClassification,\n",
    ")\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "import collections\n",
    "import string\n",
    "import pickle\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "run=1\n",
    "analyzed_langs=[]\n",
    "values_langs=[]\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "                \"bert\": (BertConfig, BertForTokenClassification, BertTokenizerFast),\n",
    "                \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForTokenClassification, XLMRobertaTokenizerFast),\n",
    "            }\n",
    "max_seq_length=128\n",
    "\n",
    "def remove_punctuation_stop(input_text,lang):\n",
    "        stop_words=stopwords.stopwords(lang)\n",
    "        remove_set=set(string.punctuation).union(set(stop_words))\n",
    "        return [token for token in input_text if token not in remove_set]\n",
    "\n",
    "for l_index,lang2 in all_languages.iterrows():\n",
    "\n",
    "    \n",
    "        language_source='fa'\n",
    "        language_target='fa'\n",
    "        print(language_source)\n",
    "        print(language_target)  \n",
    "        \n",
    "        model_type=lang2['model_type']\n",
    "        \n",
    "        if(model_type=='bert'):\n",
    "            model_name='bert-base-multilingual-cased'\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "            model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "            fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "        if(model_type=='xlmroberta'):\n",
    "            model_name='xlm-roberta-base'\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "            model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "            fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "        \n",
    "        with open(path+language_source+'/title_training.pkl','rb') as file:\n",
    "                    source_training=pickle.load(file)\n",
    "                \n",
    "        with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "                    target_test=pickle.load(file)\n",
    "                \n",
    "#         dataset_source=pd.read_csv(path+language_source+\"/title_dataset.csv\", sep=',', encoding='utf-8')\n",
    "#         similar_contexts=[item for item, count in collections.Counter(dataset_source['sectionText']).items() if count > 1]\n",
    "  \n",
    "        \n",
    "        source_training_texts=[]\n",
    "        stop_words=stopwords.stopwords(language_source)\n",
    "        remove_set=set(string.punctuation).union(set(stop_words))\n",
    "        for example in source_training:\n",
    "            input_text=example.contexts[0]\n",
    "            if not input_text in similar_contexts:\n",
    "                words=input_text.split(\" \")\n",
    "#                 print(words)\n",
    "                input_ids=fastTokenizer(words,max_length=max_seq_length,truncation='longest_first').input_ids\n",
    "                word_ind=0\n",
    "                acc_len=0\n",
    "                while acc_len<min(max_seq_length,len(words)):\n",
    "                    if(len(input_ids)>word_ind):\n",
    "                        acc_len+=len(input_ids[word_ind])-2\n",
    "                        word_ind+=1\n",
    "                    else:\n",
    "                        print(example)\n",
    "                        print(input_ids)\n",
    "                        words=[]\n",
    "                        break\n",
    "                words=words[:word_ind]\n",
    "#                 print(words)\n",
    "                for token in words:\n",
    "                    if not token in remove_set:\n",
    "                        source_training_texts.append(token)\n",
    "\n",
    "        target_test_texts=[]\n",
    "        stop_words=stopwords.stopwords(language_target)\n",
    "        remove_set=set(string.punctuation).union(set(stop_words))\n",
    "        for example in target_test:\n",
    "            input_text=example.contexts[0]\n",
    "            if not input_text in similar_contexts:\n",
    "                words=input_text.split(\" \")\n",
    "                input_ids=fastTokenizer(words,max_length=max_seq_length,truncation='longest_first').input_ids\n",
    "                word_ind=0\n",
    "                acc_len=0\n",
    "                while acc_len<min(max_seq_length,len(words)):\n",
    "                    if(len(input_ids)>word_ind):\n",
    "                        acc_len+=len(input_ids[word_ind])-2\n",
    "                        word_ind+=1\n",
    "                    else:\n",
    "                        print(example)\n",
    "                        print(input_ids)\n",
    "                        words=[]\n",
    "                        break\n",
    "                words=words[:word_ind]\n",
    "                for token in words:\n",
    "                    if not token in remove_set:\n",
    "                        target_test_texts.append(token)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        shared_unique=set(source_training_texts).intersection(set(target_test_texts))\n",
    "\n",
    "        count1=Counter(source_training_texts)\n",
    "        occur1=[count1[token] for token in shared_unique]\n",
    "        all_occur1=sum(occur1)\n",
    "        count2=Counter(target_test_texts)\n",
    "        occur2=[count2[token] for token in shared_unique]\n",
    "        all_occur2=sum(occur2)\n",
    "\n",
    "\n",
    "        info=[language_source,language_target,len(source_training_texts),\n",
    "             len(target_test_texts),all_occur1,all_occur2,len(shared_unique),\n",
    "                       all_occur1/len(source_training_texts),all_occur2/len(target_test_texts)]\n",
    "        print(info)\n",
    "        overlap_df.loc[len(overlap_df.index)]=info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bdfc0",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdeb87ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119547\n",
      "250002\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,XLMRobertaTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)\n",
    "print(len(tokenizer.vocab))\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base', do_lower_case=True)\n",
    "print(tokenizer.vocab_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
