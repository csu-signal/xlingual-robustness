{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbae848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c2a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "languages_info={\n",
    "    'fr':'french',\n",
    "    'br':'breton',\n",
    "    'ar':'arabic',\n",
    "    'fa':'persian',\n",
    "    'hi':'hindi',\n",
    "    'en':'english',\n",
    "    'sco':'scots',\n",
    "    'cy':'welsh',\n",
    "    'es':'spanish',\n",
    "    'ca':'catalan',\n",
    "    'cs':'czech',\n",
    "    'sk':'slovak',\n",
    "    'id':'indonesian',\n",
    "    'ms':'malay',\n",
    "    'oc':'occitan',\n",
    "    'nl':'dutch',\n",
    "    'af':'afrikaans',\n",
    "    'it':'italian',\n",
    "    'scn':'sicilian',\n",
    "    'an':'aragonese',\n",
    "    'ast':'asturian'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2494643c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>total tokens of l1</th>\n",
       "      <th>total tokens of l2</th>\n",
       "      <th>shared non-u in l1</th>\n",
       "      <th>shared non-u in l2</th>\n",
       "      <th>shared unique in both</th>\n",
       "      <th>non-un token overlap in l1</th>\n",
       "      <th>non-un token overlap in l2</th>\n",
       "      <th>l1-name</th>\n",
       "      <th>l2-name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>total tokens of l1_y</th>\n",
       "      <th>total tokens of l2_y</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>9902636</td>\n",
       "      <td>151294</td>\n",
       "      <td>2401566</td>\n",
       "      <td>52615</td>\n",
       "      <td>10241</td>\n",
       "      <td>0.242518</td>\n",
       "      <td>0.347767</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>1628</td>\n",
       "      <td>1.462501</td>\n",
       "      <td>0.654987</td>\n",
       "      <td>0.621483</td>\n",
       "      <td>0.637795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>9902636</td>\n",
       "      <td>151294</td>\n",
       "      <td>2401566</td>\n",
       "      <td>52615</td>\n",
       "      <td>10241</td>\n",
       "      <td>0.242518</td>\n",
       "      <td>0.347767</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>1628</td>\n",
       "      <td>1.344928</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.572890</td>\n",
       "      <td>0.575096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>11179923</td>\n",
       "      <td>2220121</td>\n",
       "      <td>2123622</td>\n",
       "      <td>700385</td>\n",
       "      <td>19044</td>\n",
       "      <td>0.189950</td>\n",
       "      <td>0.315472</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.264355</td>\n",
       "      <td>0.674797</td>\n",
       "      <td>0.636829</td>\n",
       "      <td>0.655263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>11179923</td>\n",
       "      <td>2220121</td>\n",
       "      <td>2123622</td>\n",
       "      <td>700385</td>\n",
       "      <td>19044</td>\n",
       "      <td>0.189950</td>\n",
       "      <td>0.315472</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>0.653479</td>\n",
       "      <td>0.775051</td>\n",
       "      <td>0.782030</td>\n",
       "      <td>0.778525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>11179923</td>\n",
       "      <td>1191915</td>\n",
       "      <td>457321</td>\n",
       "      <td>68610</td>\n",
       "      <td>6012</td>\n",
       "      <td>0.040906</td>\n",
       "      <td>0.057563</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>8527</td>\n",
       "      <td>1.243306</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.624820</td>\n",
       "      <td>0.631936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>11179923</td>\n",
       "      <td>1191915</td>\n",
       "      <td>457321</td>\n",
       "      <td>68610</td>\n",
       "      <td>6012</td>\n",
       "      <td>0.040906</td>\n",
       "      <td>0.057563</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>8527</td>\n",
       "      <td>0.772180</td>\n",
       "      <td>0.762512</td>\n",
       "      <td>0.744609</td>\n",
       "      <td>0.753455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>8664807</td>\n",
       "      <td>165078</td>\n",
       "      <td>5597573</td>\n",
       "      <td>63180</td>\n",
       "      <td>15541</td>\n",
       "      <td>0.646012</td>\n",
       "      <td>0.382728</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>8664807</td>\n",
       "      <td>165078</td>\n",
       "      <td>5597573</td>\n",
       "      <td>63180</td>\n",
       "      <td>15541</td>\n",
       "      <td>0.646012</td>\n",
       "      <td>0.382728</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>1023</td>\n",
       "      <td>1.173925</td>\n",
       "      <td>0.724891</td>\n",
       "      <td>0.677551</td>\n",
       "      <td>0.700422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>8664807</td>\n",
       "      <td>513413</td>\n",
       "      <td>3652889</td>\n",
       "      <td>102740</td>\n",
       "      <td>13676</td>\n",
       "      <td>0.421578</td>\n",
       "      <td>0.200112</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>3047</td>\n",
       "      <td>1.492519</td>\n",
       "      <td>0.627530</td>\n",
       "      <td>0.647632</td>\n",
       "      <td>0.637423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>8664807</td>\n",
       "      <td>513413</td>\n",
       "      <td>3652889</td>\n",
       "      <td>102740</td>\n",
       "      <td>13676</td>\n",
       "      <td>0.421578</td>\n",
       "      <td>0.200112</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>3047</td>\n",
       "      <td>1.694407</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.588313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>9241595</td>\n",
       "      <td>2166845</td>\n",
       "      <td>4762768</td>\n",
       "      <td>984794</td>\n",
       "      <td>62472</td>\n",
       "      <td>0.515362</td>\n",
       "      <td>0.454483</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>0.966566</td>\n",
       "      <td>0.797028</td>\n",
       "      <td>0.785356</td>\n",
       "      <td>0.791149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>9241595</td>\n",
       "      <td>2166845</td>\n",
       "      <td>4762768</td>\n",
       "      <td>984794</td>\n",
       "      <td>62472</td>\n",
       "      <td>0.515362</td>\n",
       "      <td>0.454483</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.382355</td>\n",
       "      <td>0.731319</td>\n",
       "      <td>0.718556</td>\n",
       "      <td>0.724881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>9604009</td>\n",
       "      <td>1410131</td>\n",
       "      <td>4289988</td>\n",
       "      <td>690897</td>\n",
       "      <td>78886</td>\n",
       "      <td>0.446687</td>\n",
       "      <td>0.489952</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>80001</td>\n",
       "      <td>12235</td>\n",
       "      <td>0.906951</td>\n",
       "      <td>0.804369</td>\n",
       "      <td>0.803583</td>\n",
       "      <td>0.803976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>9604009</td>\n",
       "      <td>1410131</td>\n",
       "      <td>4289988</td>\n",
       "      <td>690897</td>\n",
       "      <td>78886</td>\n",
       "      <td>0.446687</td>\n",
       "      <td>0.489952</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80001</td>\n",
       "      <td>12235</td>\n",
       "      <td>0.745653</td>\n",
       "      <td>0.792391</td>\n",
       "      <td>0.786971</td>\n",
       "      <td>0.789672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>8704064</td>\n",
       "      <td>1216154</td>\n",
       "      <td>6846218</td>\n",
       "      <td>1015319</td>\n",
       "      <td>72805</td>\n",
       "      <td>0.786554</td>\n",
       "      <td>0.834861</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>80001</td>\n",
       "      <td>12069</td>\n",
       "      <td>1.000841</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.794485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>8704064</td>\n",
       "      <td>1216154</td>\n",
       "      <td>6846218</td>\n",
       "      <td>1015319</td>\n",
       "      <td>72805</td>\n",
       "      <td>0.786554</td>\n",
       "      <td>0.834861</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80001</td>\n",
       "      <td>12069</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.799471</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.792533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>9902636</td>\n",
       "      <td>561896</td>\n",
       "      <td>4074077</td>\n",
       "      <td>228067</td>\n",
       "      <td>17127</td>\n",
       "      <td>0.411413</td>\n",
       "      <td>0.405888</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>2757</td>\n",
       "      <td>1.109227</td>\n",
       "      <td>0.784397</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.781073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>9902636</td>\n",
       "      <td>561896</td>\n",
       "      <td>4074077</td>\n",
       "      <td>228067</td>\n",
       "      <td>17127</td>\n",
       "      <td>0.411413</td>\n",
       "      <td>0.405888</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>2757</td>\n",
       "      <td>1.000687</td>\n",
       "      <td>0.698592</td>\n",
       "      <td>0.697609</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>7512100</td>\n",
       "      <td>821846</td>\n",
       "      <td>3849288</td>\n",
       "      <td>440531</td>\n",
       "      <td>34082</td>\n",
       "      <td>0.512412</td>\n",
       "      <td>0.536026</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>5933</td>\n",
       "      <td>1.165977</td>\n",
       "      <td>0.792568</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.794715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>7512100</td>\n",
       "      <td>821846</td>\n",
       "      <td>3849288</td>\n",
       "      <td>440531</td>\n",
       "      <td>34082</td>\n",
       "      <td>0.512412</td>\n",
       "      <td>0.536026</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>5933</td>\n",
       "      <td>0.921793</td>\n",
       "      <td>0.752218</td>\n",
       "      <td>0.748641</td>\n",
       "      <td>0.750426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>10010452</td>\n",
       "      <td>46683</td>\n",
       "      <td>2585770</td>\n",
       "      <td>17911</td>\n",
       "      <td>4478</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>80001</td>\n",
       "      <td>286</td>\n",
       "      <td>1.095371</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>10010452</td>\n",
       "      <td>46683</td>\n",
       "      <td>2585770</td>\n",
       "      <td>17911</td>\n",
       "      <td>4478</td>\n",
       "      <td>0.258307</td>\n",
       "      <td>0.383673</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80001</td>\n",
       "      <td>286</td>\n",
       "      <td>1.250472</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>9241595</td>\n",
       "      <td>163721</td>\n",
       "      <td>4408011</td>\n",
       "      <td>61982</td>\n",
       "      <td>13236</td>\n",
       "      <td>0.476975</td>\n",
       "      <td>0.378583</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.857678</td>\n",
       "      <td>0.854478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>9241595</td>\n",
       "      <td>163721</td>\n",
       "      <td>4408011</td>\n",
       "      <td>61982</td>\n",
       "      <td>13236</td>\n",
       "      <td>0.476975</td>\n",
       "      <td>0.378583</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.734883</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>0.767790</td>\n",
       "      <td>0.772128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>9241595</td>\n",
       "      <td>3750593</td>\n",
       "      <td>6558837</td>\n",
       "      <td>1529127</td>\n",
       "      <td>95889</td>\n",
       "      <td>0.709708</td>\n",
       "      <td>0.407703</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>17101</td>\n",
       "      <td>0.745001</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.845844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>9241595</td>\n",
       "      <td>3750593</td>\n",
       "      <td>6558837</td>\n",
       "      <td>1529127</td>\n",
       "      <td>95889</td>\n",
       "      <td>0.709708</td>\n",
       "      <td>0.407703</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>17101</td>\n",
       "      <td>0.740191</td>\n",
       "      <td>0.767686</td>\n",
       "      <td>0.772445</td>\n",
       "      <td>0.770058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>593572</td>\n",
       "      <td>151294</td>\n",
       "      <td>491380</td>\n",
       "      <td>147913</td>\n",
       "      <td>36541</td>\n",
       "      <td>0.827836</td>\n",
       "      <td>0.977653</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>6511</td>\n",
       "      <td>1628</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>0.747283</td>\n",
       "      <td>0.703325</td>\n",
       "      <td>0.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>593572</td>\n",
       "      <td>151294</td>\n",
       "      <td>491380</td>\n",
       "      <td>147913</td>\n",
       "      <td>36541</td>\n",
       "      <td>0.827836</td>\n",
       "      <td>0.977653</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>6511</td>\n",
       "      <td>1628</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.255754</td>\n",
       "      <td>0.245098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>8885122</td>\n",
       "      <td>2220121</td>\n",
       "      <td>8374217</td>\n",
       "      <td>2200587</td>\n",
       "      <td>187300</td>\n",
       "      <td>0.942499</td>\n",
       "      <td>0.991201</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>1.640287</td>\n",
       "      <td>0.677596</td>\n",
       "      <td>0.634271</td>\n",
       "      <td>0.655218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>8885122</td>\n",
       "      <td>2220121</td>\n",
       "      <td>8374217</td>\n",
       "      <td>2200587</td>\n",
       "      <td>187300</td>\n",
       "      <td>0.942499</td>\n",
       "      <td>0.991201</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>0.619075</td>\n",
       "      <td>0.805126</td>\n",
       "      <td>0.810070</td>\n",
       "      <td>0.807590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>4678256</td>\n",
       "      <td>1191915</td>\n",
       "      <td>4310961</td>\n",
       "      <td>1175949</td>\n",
       "      <td>120576</td>\n",
       "      <td>0.921489</td>\n",
       "      <td>0.986605</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>34104</td>\n",
       "      <td>8527</td>\n",
       "      <td>0.820731</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.732821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>4678256</td>\n",
       "      <td>1191915</td>\n",
       "      <td>4310961</td>\n",
       "      <td>1175949</td>\n",
       "      <td>120576</td>\n",
       "      <td>0.921489</td>\n",
       "      <td>0.986605</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>34104</td>\n",
       "      <td>8527</td>\n",
       "      <td>0.636657</td>\n",
       "      <td>0.776657</td>\n",
       "      <td>0.774796</td>\n",
       "      <td>0.775726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>635818</td>\n",
       "      <td>165078</td>\n",
       "      <td>565771</td>\n",
       "      <td>162857</td>\n",
       "      <td>26638</td>\n",
       "      <td>0.889832</td>\n",
       "      <td>0.986546</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>4091</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.441682</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.841046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>635818</td>\n",
       "      <td>165078</td>\n",
       "      <td>565771</td>\n",
       "      <td>162857</td>\n",
       "      <td>26638</td>\n",
       "      <td>0.889832</td>\n",
       "      <td>0.986546</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>4091</td>\n",
       "      <td>1023</td>\n",
       "      <td>0.695841</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.739220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>2044276</td>\n",
       "      <td>513413</td>\n",
       "      <td>1911282</td>\n",
       "      <td>508744</td>\n",
       "      <td>53220</td>\n",
       "      <td>0.934943</td>\n",
       "      <td>0.990906</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>12186</td>\n",
       "      <td>3047</td>\n",
       "      <td>0.724366</td>\n",
       "      <td>0.775815</td>\n",
       "      <td>0.795265</td>\n",
       "      <td>0.785420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>2044276</td>\n",
       "      <td>513413</td>\n",
       "      <td>1911282</td>\n",
       "      <td>508744</td>\n",
       "      <td>53220</td>\n",
       "      <td>0.934943</td>\n",
       "      <td>0.990906</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>12186</td>\n",
       "      <td>3047</td>\n",
       "      <td>0.757319</td>\n",
       "      <td>0.693824</td>\n",
       "      <td>0.735376</td>\n",
       "      <td>0.713996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>8482790</td>\n",
       "      <td>2166845</td>\n",
       "      <td>7984822</td>\n",
       "      <td>2148362</td>\n",
       "      <td>199366</td>\n",
       "      <td>0.941297</td>\n",
       "      <td>0.991470</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>0.752348</td>\n",
       "      <td>0.861305</td>\n",
       "      <td>0.865797</td>\n",
       "      <td>0.863545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>8482790</td>\n",
       "      <td>2166845</td>\n",
       "      <td>7984822</td>\n",
       "      <td>2148362</td>\n",
       "      <td>199366</td>\n",
       "      <td>0.941297</td>\n",
       "      <td>0.991470</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>80000</td>\n",
       "      <td>20001</td>\n",
       "      <td>0.560116</td>\n",
       "      <td>0.840754</td>\n",
       "      <td>0.840923</td>\n",
       "      <td>0.840838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>5711267</td>\n",
       "      <td>1410131</td>\n",
       "      <td>5133872</td>\n",
       "      <td>1391987</td>\n",
       "      <td>237640</td>\n",
       "      <td>0.898902</td>\n",
       "      <td>0.987133</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>48936</td>\n",
       "      <td>12235</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.837652</td>\n",
       "      <td>0.831922</td>\n",
       "      <td>0.834777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>5711267</td>\n",
       "      <td>1410131</td>\n",
       "      <td>5133872</td>\n",
       "      <td>1391987</td>\n",
       "      <td>237640</td>\n",
       "      <td>0.898902</td>\n",
       "      <td>0.987133</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>48936</td>\n",
       "      <td>12235</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>0.823645</td>\n",
       "      <td>0.821498</td>\n",
       "      <td>0.822570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>4935785</td>\n",
       "      <td>1216154</td>\n",
       "      <td>4577744</td>\n",
       "      <td>1201668</td>\n",
       "      <td>125020</td>\n",
       "      <td>0.927460</td>\n",
       "      <td>0.988089</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>48273</td>\n",
       "      <td>12069</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.836406</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.830664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>4935785</td>\n",
       "      <td>1216154</td>\n",
       "      <td>4577744</td>\n",
       "      <td>1201668</td>\n",
       "      <td>125020</td>\n",
       "      <td>0.927460</td>\n",
       "      <td>0.988089</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>48273</td>\n",
       "      <td>12069</td>\n",
       "      <td>0.594083</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.805844</td>\n",
       "      <td>0.812173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>2212980</td>\n",
       "      <td>561896</td>\n",
       "      <td>2033029</td>\n",
       "      <td>555100</td>\n",
       "      <td>68429</td>\n",
       "      <td>0.918684</td>\n",
       "      <td>0.987905</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>11024</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.792883</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.808720</td>\n",
       "      <td>0.800836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>2212980</td>\n",
       "      <td>561896</td>\n",
       "      <td>2033029</td>\n",
       "      <td>555100</td>\n",
       "      <td>68429</td>\n",
       "      <td>0.918684</td>\n",
       "      <td>0.987905</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>11024</td>\n",
       "      <td>2757</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.692924</td>\n",
       "      <td>0.729958</td>\n",
       "      <td>0.710959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>3357563</td>\n",
       "      <td>821846</td>\n",
       "      <td>3072632</td>\n",
       "      <td>811391</td>\n",
       "      <td>100657</td>\n",
       "      <td>0.915138</td>\n",
       "      <td>0.987279</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>23728</td>\n",
       "      <td>5933</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>0.802989</td>\n",
       "      <td>0.803535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>3357563</td>\n",
       "      <td>821846</td>\n",
       "      <td>3072632</td>\n",
       "      <td>811391</td>\n",
       "      <td>100657</td>\n",
       "      <td>0.915138</td>\n",
       "      <td>0.987279</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>23728</td>\n",
       "      <td>5933</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.360054</td>\n",
       "      <td>0.354278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>182677</td>\n",
       "      <td>46683</td>\n",
       "      <td>144428</td>\n",
       "      <td>44860</td>\n",
       "      <td>12526</td>\n",
       "      <td>0.790620</td>\n",
       "      <td>0.960949</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1141</td>\n",
       "      <td>286</td>\n",
       "      <td>0.829185</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.658065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>182677</td>\n",
       "      <td>46683</td>\n",
       "      <td>144428</td>\n",
       "      <td>44860</td>\n",
       "      <td>12526</td>\n",
       "      <td>0.790620</td>\n",
       "      <td>0.960949</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1141</td>\n",
       "      <td>286</td>\n",
       "      <td>1.384972</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>675340</td>\n",
       "      <td>163721</td>\n",
       "      <td>599236</td>\n",
       "      <td>161525</td>\n",
       "      <td>27375</td>\n",
       "      <td>0.887310</td>\n",
       "      <td>0.986587</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.584566</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>0.844860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>675340</td>\n",
       "      <td>163721</td>\n",
       "      <td>599236</td>\n",
       "      <td>161525</td>\n",
       "      <td>27375</td>\n",
       "      <td>0.887310</td>\n",
       "      <td>0.986587</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.812382</td>\n",
       "      <td>0.710425</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.699620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>15146753</td>\n",
       "      <td>3750593</td>\n",
       "      <td>14586264</td>\n",
       "      <td>3731477</td>\n",
       "      <td>223717</td>\n",
       "      <td>0.962996</td>\n",
       "      <td>0.994903</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>68400</td>\n",
       "      <td>17101</td>\n",
       "      <td>0.702621</td>\n",
       "      <td>0.856450</td>\n",
       "      <td>0.852124</td>\n",
       "      <td>0.854282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>15146753</td>\n",
       "      <td>3750593</td>\n",
       "      <td>14586264</td>\n",
       "      <td>3731477</td>\n",
       "      <td>223717</td>\n",
       "      <td>0.962996</td>\n",
       "      <td>0.994903</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>68400</td>\n",
       "      <td>17101</td>\n",
       "      <td>0.608224</td>\n",
       "      <td>0.811502</td>\n",
       "      <td>0.816533</td>\n",
       "      <td>0.814009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1   l2  total tokens of l1  total tokens of l2  shared non-u in l1  \\\n",
       "0    fr   br             9902636              151294             2401566   \n",
       "1    fr   br             9902636              151294             2401566   \n",
       "2    ar   fa            11179923             2220121             2123622   \n",
       "3    ar   fa            11179923             2220121             2123622   \n",
       "4    ar   hi            11179923             1191915              457321   \n",
       "5    ar   hi            11179923             1191915              457321   \n",
       "6    en  sco             8664807              165078             5597573   \n",
       "7    en  sco             8664807              165078             5597573   \n",
       "8    en   cy             8664807              513413             3652889   \n",
       "9    en   cy             8664807              513413             3652889   \n",
       "10   es   ca             9241595             2166845             4762768   \n",
       "11   es   ca             9241595             2166845             4762768   \n",
       "12   cs   sk             9604009             1410131             4289988   \n",
       "13   cs   sk             9604009             1410131             4289988   \n",
       "14   id   ms             8704064             1216154             6846218   \n",
       "15   id   ms             8704064             1216154             6846218   \n",
       "16   fr   oc             9902636              561896             4074077   \n",
       "17   fr   oc             9902636              561896             4074077   \n",
       "18   nl   af             7512100              821846             3849288   \n",
       "19   nl   af             7512100              821846             3849288   \n",
       "20   it  scn            10010452               46683             2585770   \n",
       "21   it  scn            10010452               46683             2585770   \n",
       "22   es   an             9241595              163721             4408011   \n",
       "23   es   an             9241595              163721             4408011   \n",
       "24   es  ast             9241595             3750593             6558837   \n",
       "25   es  ast             9241595             3750593             6558837   \n",
       "26   br   br              593572              151294              491380   \n",
       "27   br   br              593572              151294              491380   \n",
       "28   fa   fa             8885122             2220121             8374217   \n",
       "29   fa   fa             8885122             2220121             8374217   \n",
       "30   hi   hi             4678256             1191915             4310961   \n",
       "31   hi   hi             4678256             1191915             4310961   \n",
       "32  sco  sco              635818              165078              565771   \n",
       "33  sco  sco              635818              165078              565771   \n",
       "34   cy   cy             2044276              513413             1911282   \n",
       "35   cy   cy             2044276              513413             1911282   \n",
       "36   ca   ca             8482790             2166845             7984822   \n",
       "37   ca   ca             8482790             2166845             7984822   \n",
       "38   sk   sk             5711267             1410131             5133872   \n",
       "39   sk   sk             5711267             1410131             5133872   \n",
       "40   ms   ms             4935785             1216154             4577744   \n",
       "41   ms   ms             4935785             1216154             4577744   \n",
       "42   oc   oc             2212980              561896             2033029   \n",
       "43   oc   oc             2212980              561896             2033029   \n",
       "44   af   af             3357563              821846             3072632   \n",
       "45   af   af             3357563              821846             3072632   \n",
       "46  scn  scn              182677               46683              144428   \n",
       "47  scn  scn              182677               46683              144428   \n",
       "48   an   an              675340              163721              599236   \n",
       "49   an   an              675340              163721              599236   \n",
       "50  ast  ast            15146753             3750593            14586264   \n",
       "51  ast  ast            15146753             3750593            14586264   \n",
       "\n",
       "    shared non-u in l2  shared unique in both  non-un token overlap in l1  \\\n",
       "0                52615                  10241                    0.242518   \n",
       "1                52615                  10241                    0.242518   \n",
       "2               700385                  19044                    0.189950   \n",
       "3               700385                  19044                    0.189950   \n",
       "4                68610                   6012                    0.040906   \n",
       "5                68610                   6012                    0.040906   \n",
       "6                63180                  15541                    0.646012   \n",
       "7                63180                  15541                    0.646012   \n",
       "8               102740                  13676                    0.421578   \n",
       "9               102740                  13676                    0.421578   \n",
       "10              984794                  62472                    0.515362   \n",
       "11              984794                  62472                    0.515362   \n",
       "12              690897                  78886                    0.446687   \n",
       "13              690897                  78886                    0.446687   \n",
       "14             1015319                  72805                    0.786554   \n",
       "15             1015319                  72805                    0.786554   \n",
       "16              228067                  17127                    0.411413   \n",
       "17              228067                  17127                    0.411413   \n",
       "18              440531                  34082                    0.512412   \n",
       "19              440531                  34082                    0.512412   \n",
       "20               17911                   4478                    0.258307   \n",
       "21               17911                   4478                    0.258307   \n",
       "22               61982                  13236                    0.476975   \n",
       "23               61982                  13236                    0.476975   \n",
       "24             1529127                  95889                    0.709708   \n",
       "25             1529127                  95889                    0.709708   \n",
       "26              147913                  36541                    0.827836   \n",
       "27              147913                  36541                    0.827836   \n",
       "28             2200587                 187300                    0.942499   \n",
       "29             2200587                 187300                    0.942499   \n",
       "30             1175949                 120576                    0.921489   \n",
       "31             1175949                 120576                    0.921489   \n",
       "32              162857                  26638                    0.889832   \n",
       "33              162857                  26638                    0.889832   \n",
       "34              508744                  53220                    0.934943   \n",
       "35              508744                  53220                    0.934943   \n",
       "36             2148362                 199366                    0.941297   \n",
       "37             2148362                 199366                    0.941297   \n",
       "38             1391987                 237640                    0.898902   \n",
       "39             1391987                 237640                    0.898902   \n",
       "40             1201668                 125020                    0.927460   \n",
       "41             1201668                 125020                    0.927460   \n",
       "42              555100                  68429                    0.918684   \n",
       "43              555100                  68429                    0.918684   \n",
       "44              811391                 100657                    0.915138   \n",
       "45              811391                 100657                    0.915138   \n",
       "46               44860                  12526                    0.790620   \n",
       "47               44860                  12526                    0.790620   \n",
       "48              161525                  27375                    0.887310   \n",
       "49              161525                  27375                    0.887310   \n",
       "50             3731477                 223717                    0.962996   \n",
       "51             3731477                 223717                    0.962996   \n",
       "\n",
       "    non-un token overlap in l2     l1-name    l2-name  model_type  \\\n",
       "0                     0.347767      french     breton        bert   \n",
       "1                     0.347767      french     breton  xlmroberta   \n",
       "2                     0.315472      arabic    persian        bert   \n",
       "3                     0.315472      arabic    persian  xlmroberta   \n",
       "4                     0.057563      arabic      hindi        bert   \n",
       "5                     0.057563      arabic      hindi  xlmroberta   \n",
       "6                     0.382728     english      scots        bert   \n",
       "7                     0.382728     english      scots  xlmroberta   \n",
       "8                     0.200112     english      welsh        bert   \n",
       "9                     0.200112     english      welsh  xlmroberta   \n",
       "10                    0.454483     spanish    catalan        bert   \n",
       "11                    0.454483     spanish    catalan  xlmroberta   \n",
       "12                    0.489952       czech     slovak        bert   \n",
       "13                    0.489952       czech     slovak  xlmroberta   \n",
       "14                    0.834861  indonesian      malay        bert   \n",
       "15                    0.834861  indonesian      malay  xlmroberta   \n",
       "16                    0.405888      french    occitan        bert   \n",
       "17                    0.405888      french    occitan  xlmroberta   \n",
       "18                    0.536026       dutch    african        bert   \n",
       "19                    0.536026       dutch    african  xlmroberta   \n",
       "20                    0.383673     italian   sicilian        bert   \n",
       "21                    0.383673     italian   sicilian  xlmroberta   \n",
       "22                    0.378583     spanish  aragonese        bert   \n",
       "23                    0.378583     spanish  aragonese  xlmroberta   \n",
       "24                    0.407703     spanish   asturian        bert   \n",
       "25                    0.407703     spanish   asturian  xlmroberta   \n",
       "26                    0.977653      breton     breton        bert   \n",
       "27                    0.977653      breton     breton  xlmroberta   \n",
       "28                    0.991201     persian    persian        bert   \n",
       "29                    0.991201     persian    persian  xlmroberta   \n",
       "30                    0.986605       hindi      hindi        bert   \n",
       "31                    0.986605       hindi      hindi  xlmroberta   \n",
       "32                    0.986546       scots      scots        bert   \n",
       "33                    0.986546       scots      scots  xlmroberta   \n",
       "34                    0.990906       welsh      welsh        bert   \n",
       "35                    0.990906       welsh      welsh  xlmroberta   \n",
       "36                    0.991470     catalan    catalan        bert   \n",
       "37                    0.991470     catalan    catalan  xlmroberta   \n",
       "38                    0.987133      slovak     slovak        bert   \n",
       "39                    0.987133      slovak     slovak  xlmroberta   \n",
       "40                    0.988089       malay      malay        bert   \n",
       "41                    0.988089       malay      malay  xlmroberta   \n",
       "42                    0.987905     occitan    occitan        bert   \n",
       "43                    0.987905     occitan    occitan  xlmroberta   \n",
       "44                    0.987279     african    african        bert   \n",
       "45                    0.987279     african    african  xlmroberta   \n",
       "46                    0.960949    sicilian   sicilian        bert   \n",
       "47                    0.960949    sicilian   sicilian  xlmroberta   \n",
       "48                    0.986587   aragonese  aragonese        bert   \n",
       "49                    0.986587   aragonese  aragonese  xlmroberta   \n",
       "50                    0.994903    asturian   asturian        bert   \n",
       "51                    0.994903    asturian   asturian  xlmroberta   \n",
       "\n",
       "    total tokens of l1_y  total tokens of l2_y  eval_loss  precision  \\\n",
       "0                  80000                  1628   1.462501   0.654987   \n",
       "1                  80000                  1628   1.344928   0.577320   \n",
       "2                  80000                 20001   1.264355   0.674797   \n",
       "3                  80000                 20001   0.653479   0.775051   \n",
       "4                  80000                  8527   1.243306   0.639216   \n",
       "5                  80000                  8527   0.772180   0.762512   \n",
       "6                  80000                  1023   0.821415   0.804781   \n",
       "7                  80000                  1023   1.173925   0.724891   \n",
       "8                  80000                  3047   1.492519   0.627530   \n",
       "9                  80000                  3047   1.694407   0.562183   \n",
       "10                 80000                 20001   0.966566   0.797028   \n",
       "11                 80000                 20001   1.382355   0.731319   \n",
       "12                 80001                 12235   0.906951   0.804369   \n",
       "13                 80001                 12235   0.745653   0.792391   \n",
       "14                 80001                 12069   1.000841   0.803453   \n",
       "15                 80001                 12069   0.738019   0.799471   \n",
       "16                 80000                  2757   1.109227   0.784397   \n",
       "17                 80000                  2757   1.000687   0.698592   \n",
       "18                 80000                  5933   1.165977   0.792568   \n",
       "19                 80000                  5933   0.921793   0.752218   \n",
       "20                 80001                   286   1.095371   0.768293   \n",
       "21                 80001                   286   1.250472   0.586667   \n",
       "22                 80000                  1024   0.731461   0.851301   \n",
       "23                 80000                  1024   0.734883   0.776515   \n",
       "24                 80000                 17101   0.745001   0.846915   \n",
       "25                 80000                 17101   0.740191   0.767686   \n",
       "26                  6511                  1628   0.846165   0.747283   \n",
       "27                  6511                  1628   1.385766   0.235294   \n",
       "28                 80000                 20001   1.640287   0.677596   \n",
       "29                 80000                 20001   0.619075   0.805126   \n",
       "30                 34104                  8527   0.820731   0.734940   \n",
       "31                 34104                  8527   0.636657   0.776657   \n",
       "32                  4091                  1023   0.441682   0.829365   \n",
       "33                  4091                  1023   0.695841   0.743802   \n",
       "34                 12186                  3047   0.724366   0.775815   \n",
       "35                 12186                  3047   0.757319   0.693824   \n",
       "36                 80000                 20001   0.752348   0.861305   \n",
       "37                 80000                 20001   0.560116   0.840754   \n",
       "38                 48936                 12235   0.730048   0.837652   \n",
       "39                 48936                 12235   0.592436   0.823645   \n",
       "40                 48273                 12069   0.745011   0.836406   \n",
       "41                 48273                 12069   0.594083   0.818602   \n",
       "42                 11024                  2757   0.792883   0.793103   \n",
       "43                 11024                  2757   0.829809   0.692924   \n",
       "44                 23728                  5933   0.670639   0.804082   \n",
       "45                 23728                  5933   1.386294   0.348684   \n",
       "46                  1141                   286   0.829185   0.708333   \n",
       "47                  1141                   286   1.384972   0.582278   \n",
       "48                  4096                  1024   0.584566   0.843284   \n",
       "49                  4096                  1024   0.812382   0.710425   \n",
       "50                 68400                 17101   0.702621   0.856450   \n",
       "51                 68400                 17101   0.608224   0.811502   \n",
       "\n",
       "      recall  f1_score  \n",
       "0   0.621483  0.637795  \n",
       "1   0.572890  0.575096  \n",
       "2   0.636829  0.655263  \n",
       "3   0.782030  0.778525  \n",
       "4   0.624820  0.631936  \n",
       "5   0.744609  0.753455  \n",
       "6   0.824490  0.814516  \n",
       "7   0.677551  0.700422  \n",
       "8   0.647632  0.637423  \n",
       "9   0.616992  0.588313  \n",
       "10  0.785356  0.791149  \n",
       "11  0.718556  0.724881  \n",
       "12  0.803583  0.803976  \n",
       "13  0.786971  0.789672  \n",
       "14  0.785714  0.794485  \n",
       "15  0.785714  0.792533  \n",
       "16  0.777778  0.781073  \n",
       "17  0.697609  0.698100  \n",
       "18  0.796875  0.794715  \n",
       "19  0.748641  0.750426  \n",
       "20  0.759036  0.763636  \n",
       "21  0.530120  0.556962  \n",
       "22  0.857678  0.854478  \n",
       "23  0.767790  0.772128  \n",
       "24  0.844776  0.845844  \n",
       "25  0.772445  0.770058  \n",
       "26  0.703325  0.724638  \n",
       "27  0.255754  0.245098  \n",
       "28  0.634271  0.655218  \n",
       "29  0.810070  0.807590  \n",
       "30  0.730714  0.732821  \n",
       "31  0.774796  0.775726  \n",
       "32  0.853061  0.841046  \n",
       "33  0.734694  0.739220  \n",
       "34  0.795265  0.785420  \n",
       "35  0.735376  0.713996  \n",
       "36  0.865797  0.863545  \n",
       "37  0.840923  0.840838  \n",
       "38  0.831922  0.834777  \n",
       "39  0.821498  0.822570  \n",
       "40  0.825000  0.830664  \n",
       "41  0.805844  0.812173  \n",
       "42  0.808720  0.800836  \n",
       "43  0.729958  0.710959  \n",
       "44  0.802989  0.803535  \n",
       "45  0.360054  0.354278  \n",
       "46  0.614458  0.658065  \n",
       "47  0.554217  0.567901  \n",
       "48  0.846442  0.844860  \n",
       "49  0.689139  0.699620  \n",
       "50  0.852124  0.854282  \n",
       "51  0.816533  0.814009  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test', sep=',', encoding='utf-8')\n",
    "language_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68761682",
   "metadata": {},
   "source": [
    "# Perturbation 2- variant 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af89c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model to GPU...\n",
      "fr\n",
      "br\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "31.951311588287354\n",
      "fr\n",
      "br\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "58.122477531433105\n",
      "ar\n",
      "fa\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "379.91648030281067\n",
      "ar\n",
      "fa\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "740.7861998081207\n",
      "ar\n",
      "hi\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "759.468493938446\n",
      "ar\n",
      "hi\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "777.9024715423584\n",
      "en\n",
      "sco\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "794.1795251369476\n",
      "en\n",
      "sco\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "809.4425117969513\n",
      "en\n",
      "cy\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "894.4643342494965\n",
      "en\n",
      "cy\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "964.928911447525\n",
      "es\n",
      "ca\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "1798.1288714408875\n",
      "es\n",
      "ca\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "2580.6788854599\n",
      "cs\n",
      "sk\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "4075.684946537018\n",
      "cs\n",
      "sk\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "5751.141982555389\n",
      "id\n",
      "ms\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "6226.483278036118\n",
      "id\n",
      "ms\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "6751.984632492065\n",
      "fr\n",
      "oc\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "6872.823270559311\n",
      "fr\n",
      "oc\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "6963.970206022263\n",
      "nl\n",
      "af\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7238.957242965698\n",
      "nl\n",
      "af\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7528.776705741882\n",
      "it\n",
      "scn\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7539.499822378159\n",
      "it\n",
      "scn\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7549.863511800766\n",
      "es\n",
      "an\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7565.830262899399\n",
      "es\n",
      "an\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "7582.332416296005\n",
      "es\n",
      "ast\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "8633.51866889\n",
      "es\n",
      "ast\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "9646.43889760971\n",
      "br\n",
      "br\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "9659.914342164993\n",
      "br\n",
      "br\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "9672.21353673935\n",
      "fa\n",
      "fa\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10245.044901371002\n",
      "fa\n",
      "fa\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10910.73488163948\n",
      "hi\n",
      "hi\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10924.81971526146\n",
      "hi\n",
      "hi\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10937.416949510574\n",
      "sco\n",
      "sco\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10948.60016155243\n",
      "sco\n",
      "sco\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10958.601535081863\n",
      "cy\n",
      "cy\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "10980.251550912857\n",
      "cy\n",
      "cy\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "11007.757791757584\n",
      "ca\n",
      "ca\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "11761.014140367508\n",
      "ca\n",
      "ca\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "12585.45678973198\n",
      "sk\n",
      "sk\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "14001.782947301865\n",
      "sk\n",
      "sk\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/sk/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "15737.069489240646\n",
      "ms\n",
      "ms\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/ms/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "16342.114913463593\n",
      "ms\n",
      "ms\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/ms/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "16943.61493253708\n",
      "oc\n",
      "oc\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/oc/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17029.05425786972\n",
      "oc\n",
      "oc\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/oc/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17095.62441277504\n",
      "af\n",
      "af\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/af/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17465.976222753525\n",
      "af\n",
      "af\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/af/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17759.92369580269\n",
      "scn\n",
      "scn\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/scn/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17768.153076648712\n",
      "scn\n",
      "scn\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/scn/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17775.65757060051\n",
      "an\n",
      "an\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/an/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17794.0116584301\n",
      "an\n",
      "an\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/an/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "17812.91011452675\n",
      "ast\n",
      "ast\n",
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/ast/128_title_training_words_with_bert.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "18967.30554151535\n",
      "ast\n",
      "ast\n",
      "xlmroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU: NVIDIA RTX A6000\n",
      "    DONE.\n",
      "/s/red/a/nobackup/cwc-ro/shadim/languages/ast/128_title_training_words_with_xlmroberta.pkl\n",
      "not exist\n",
      "\n",
      "\n",
      "19800.780263900757\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"        \n",
    "import time\n",
    "program_starts = time.time()\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print('\\nLoading model to GPU...')\n",
    "device = torch.device('cuda')\n",
    "torch. cuda. empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import string\n",
    "import stopwordsiso as stopwords\n",
    "from transformers import BertTokenizerFast,XLMRobertaTokenizerFast\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    ")\n",
    "from DataProcessor import MultipleChoiceExample \n",
    "\n",
    "from transformers import (\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForTokenClassification,\n",
    ")\n",
    "cos = torch.nn.CosineSimilarity(dim=1)\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "import stopwordsiso as stopwords\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "run=1\n",
    "analyzed_langs=[]\n",
    "values_langs=[]\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "                \"bert\": (BertConfig, BertForTokenClassification, BertTokenizerFast),\n",
    "                \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForTokenClassification, XLMRobertaTokenizerFast),\n",
    "            }\n",
    "max_seq_length=128\n",
    "\n",
    "import pandas as pd\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test', sep=',', encoding='utf-8')\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "    language_source=lang2['l1']\n",
    "    language_target=lang2['l2']\n",
    "    print(language_source)\n",
    "    print(language_target)\n",
    "    model_type=lang2['model_type']\n",
    "#     if(not lang2['l1']+\"/\"+language_target in analyzed_langs):\n",
    "    if l_index>=0:\n",
    "        analyzed_langs.append(language_source+\"/\"+language_target)\n",
    "        print(model_type)\n",
    "        if(model_type=='bert'):\n",
    "            model_name='bert-base-multilingual-cased'\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "            model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "            fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "        if(model_type=='xlmroberta'):\n",
    "            model_name='xlm-roberta-base'\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "            model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "            fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "        print('  GPU:', torch.cuda.get_device_name(0))\n",
    "        desc = model.to(device)\n",
    "        print('    DONE.')\n",
    "\n",
    "        with open(path+language_source+'/title_training.pkl','rb') as file:\n",
    "                source_examples=pickle.load(file)\n",
    "\n",
    "        with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "                target_examples=pickle.load(file)\n",
    "\n",
    "                \n",
    "        if not os.path.isfile(path+language_source+'/128_title_training_examples_with_'+model_type+'.pkl'):\n",
    "            print(path+language_source+'/128_title_training_words_with_'+model_type+'.pkl')\n",
    "            print('not exist\\n\\n')\n",
    "            source_words=[]\n",
    "            source_stopwords=stopwords.stopwords(language_source)\n",
    "            s_ind=0\n",
    "            for ex_id,example in enumerate(source_examples):\n",
    "#                 words=example.contexts[0].lower().split(\" \")\n",
    "#                 print(example.contexts[0].lower())\n",
    "                words=tokenizer.tokenize(example.contexts[0].lower())\n",
    "#                 print(words)\n",
    "                input_ids=fastTokenizer(words,max_length=max_seq_length,truncation='longest_first').input_ids\n",
    "                word_ind=0\n",
    "                acc_len=0\n",
    "                while acc_len<128 and word_ind<len(words):\n",
    "                    if(len(input_ids[word_ind])>2):\n",
    "                        acc_len+=len(input_ids[word_ind])-2\n",
    "                        word_ind+=1\n",
    "                    else:\n",
    "                        word_ind+=1\n",
    "                words=words[:word_ind]\n",
    "                new_text=\" \".join(words)\n",
    "#                 print(words)\n",
    "                words=list(set(words)-source_stopwords)\n",
    "#                 words=list(words-set(source_words))\n",
    "#                 print(words)\n",
    "                if len(words)>0:\n",
    "                    source_words.extend(words)\n",
    "                source_examples[ex_id]=MultipleChoiceExample(example_id=example.example_id,\n",
    "                                        question=example.question,\n",
    "                                        contexts=[new_text,new_text,new_text,new_text],\n",
    "                                        endings=example.endings,\n",
    "                                        label=example.label)\n",
    "            with open(path+language_source+'/128_title_training_words_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(source_words,file)\n",
    "                    \n",
    "            with open(path+language_source+'/128_title_training_examples_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(source_examples,file)\n",
    "                    \n",
    "        else:\n",
    "            with open(path+language_source+'/128_title_training_examples_with_'+model_type+'.pkl','rb') as file:\n",
    "#                 source_words=pickle.load(file)\n",
    "                source_examples=pickle.load(file)\n",
    "            source_words=[]\n",
    "            source_stopwords=stopwords.stopwords(language_source)\n",
    "            s_ind=0\n",
    "            for ex_id,example in enumerate(source_examples):\n",
    "#                 words=example.contexts[0].lower().split(\" \")\n",
    "#                 print(example.contexts[0].lower())\n",
    "                words=tokenizer.tokenize(example.contexts[0].lower())\n",
    "#                 print(words)\n",
    "                words=list(set(words)-source_stopwords)\n",
    "#                 words=list(words-set(source_words))\n",
    "#                 print(words)\n",
    "                if len(words)>0:\n",
    "                    source_words.extend(words)\n",
    "            with open(path+language_source+'/128_title_training_words_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(source_words,file)\n",
    "\n",
    "\n",
    "\n",
    "        if not os.path.isfile(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl'):\n",
    "            print(path+language_target+'/128_title_test_words_with_'+model_type+'.pkl')\n",
    "            print('not exist\\n\\n')\n",
    "            target_words=[]\n",
    "            target_stopwords=set(stopwords.stopwords(language_target))\n",
    "            for ex_id,example in enumerate(target_examples):\n",
    "#                 words=example.contexts[0].lower().split(\" \")\n",
    "                words=tokenizer.tokenize(example.contexts[0].lower())\n",
    "                input_ids=fastTokenizer(words,max_length=max_seq_length,truncation='longest_first').input_ids\n",
    "                word_ind=0\n",
    "                acc_len=0\n",
    "                while acc_len<128 and word_ind<len(words):\n",
    "                    if(len(input_ids[word_ind])>2):\n",
    "                        acc_len+=len(input_ids[word_ind])-2\n",
    "                        word_ind+=1\n",
    "                    else:\n",
    "                        word_ind+=1\n",
    "                words=words[:word_ind]\n",
    "                new_text=\" \".join(words)\n",
    "                words=list(set(words)-target_stopwords)\n",
    "#                 words=list(words-set(target_words))\n",
    "                if len(words)>0:\n",
    "                    target_words.extend(words) \n",
    "                target_examples[ex_id]=MultipleChoiceExample(example_id=example.example_id,\n",
    "                                        question=example.question,\n",
    "                                        contexts=[new_text,new_text,new_text,new_text],\n",
    "                                        endings=example.endings,\n",
    "                                        label=example.label)\n",
    "            with open(path+language_target+'/128_title_test_words_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(target_words,file)\n",
    "                    \n",
    "            with open(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(target_examples,file)\n",
    "                    \n",
    "        else:\n",
    "            with open(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl','rb') as file:\n",
    "#                 target_words=pickle.load(file)\n",
    "                target_examples=pickle.load(file)\n",
    "            target_words=[]\n",
    "            target_stopwords=set(stopwords.stopwords(language_target))\n",
    "            for ex_id,example in enumerate(target_examples):\n",
    "#                 words=example.contexts[0].lower().split(\" \")\n",
    "                words=tokenizer.tokenize(example.contexts[0].lower())\n",
    "                words=list(set(words)-target_stopwords)\n",
    "#                 words=list(words-set(target_words))\n",
    "                if len(words)>0:\n",
    "                    target_words.extend(words) \n",
    "            with open(path+language_target+'/128_title_test_words_with_'+model_type+'.pkl','wb') as file:\n",
    "                    pickle.dump(target_words,file)\n",
    "\n",
    "\n",
    "                \n",
    "        same_target_words=set(target_words).intersection(set(source_words))\n",
    "        unsame_target_words=list(set(target_words)-same_target_words)\n",
    "        same_target_words=list(same_target_words)\n",
    "        same_target_embedding_vectors=[]\n",
    "        unsame_target_embedding_vectors=[]\n",
    "        max_length=10\n",
    "        batch_size=128\n",
    "        batches=batch(same_target_words, batch_size)\n",
    "        for w_ind,words in enumerate(batches):\n",
    "    #         print(\"batch: \"+str(w_ind)+\"from\"+str(len(same_target_words)/batch_size))\n",
    "            tokens=fastTokenizer(words, padding='max_length', truncation=True, max_length=max_length)\n",
    "            # print(tokens.input_ids)\n",
    "            tokens_tensor=torch.tensor(tokens.input_ids).to(device)\n",
    "            segments_tensors=torch.tensor(tokens.attention_mask).to(device)\n",
    "            outputs=model(tokens_tensor, segments_tensors)\n",
    "            same_target_embedding_vectors.extend(outputs.hidden_states[12][:,0].tolist())\n",
    "\n",
    "        batches=batch(unsame_target_words, batch_size)\n",
    "        for w_ind,words in enumerate(batches):\n",
    "    #         print(\"batch: \"+str(w_ind)+\"from\"+str(len(unsame_target_words)/batch_size))\n",
    "            tokens=fastTokenizer(words, padding='max_length', truncation=True, max_length=max_length)\n",
    "            tokens_tensor=torch.tensor(tokens.input_ids).to(device)\n",
    "            segments_tensors=torch.tensor(tokens.attention_mask).to(device)\n",
    "            outputs=model(tokens_tensor, segments_tensors)\n",
    "            unsame_target_embedding_vectors.extend(outputs.hidden_states[12][:,0].tolist())\n",
    "\n",
    "        tensor_same_target_embedding_vectors=torch.tensor(same_target_embedding_vectors)\n",
    "        tensor_unsame_target_embedding_vectors=torch.tensor(unsame_target_embedding_vectors)\n",
    "\n",
    "        alternate_words=dict()\n",
    "        no_cand_num=0\n",
    "\n",
    "        for e_id in range(len(same_target_words)):  \n",
    "            output_cos =cos(tensor_same_target_embedding_vectors[e_id], tensor_unsame_target_embedding_vectors)\n",
    "            alternate_words[same_target_words[e_id]]=unsame_target_words[output_cos.argmax()]\n",
    "\n",
    "\n",
    "        file=open(path+language_target+'/cosine_textword_with_'+language_source+\"_\"+model_type+str(run)+'.txt','w')\n",
    "        [file.write(word+'\\t'+alter+'\\n') for word,alter in zip(alternate_words.keys(),alternate_words.values())]\n",
    "        file.close()\n",
    "\n",
    "        file=open(path+language_target+'/unsame_textword_with_'+language_source+\"_\"+model_type+str(run)+'.txt','w')\n",
    "        [file.write(word+'\\n') for word in unsame_target_words]\n",
    "        file.close()\n",
    "\n",
    "\n",
    "        print(time.time()-program_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef46f320",
   "metadata": {},
   "source": [
    "# Generate target title file with only 128 tokens in each section text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4082312d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleChoiceExample(example_id=16421, question='', contexts=['● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम'], endings=['पूर्व रेक्टर', 'स्वतंत्र उज़्बेकिस्तान के विकास के दौरान, विश्वविद्यालय ने नए संकायों और प्रभागों की स्थापना देखी:', 'विश्वविद्यालय का इतिहास', 'शिक्षा संकाय'], label='B')\n",
      "[[0, 9476, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11704, 2], [0, 20, 2], [0, 225798, 2], [0, 59753, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 12839, 2], [0, 20, 2], [0, 25494, 92829, 4, 2], [0, 52894, 2], [0, 871, 2], [0, 150767, 2], [0, 9623, 2], [0, 128004, 2], [0, 871, 2], [0, 123009, 145656, 2], [0, 641, 2], [0, 93019, 7475, 134709, 4846, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11857, 2], [0, 20, 2], [0, 225798, 2], [0, 45929, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11891, 2], [0, 20, 2], [0, 192002, 2], [0, 164989, 2], [0, 17495, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 8272, 2], [0, 20, 2], [0, 225798, 2], [0, 48470, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 267, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 20831, 13, 2], [0, 287, 2], [0, 22369, 2], [0, 44269, 2], [0, 135311, 2], [0, 10550, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 3640, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 566, 238, 2], [0, 28161, 7, 2], [0, 287, 2], [0, 22369, 2], [0, 44269, 2], [0, 135311, 2], [0, 10550, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 64371, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 239015, 2], [0, 88096, 2], [0, 871, 2], [0, 239015, 2], [0, 54585, 2], [0, 218491, 2], [0, 114522, 83603, 10924, 1920, 99649, 2], [0, 109582, 2], [0, 80105, 10259, 71683, 1471, 2], [0, 6, 76081, 25784, 2], [0, 287, 2], [0, 22369, 2], [0, 227649, 2], [0, 164989, 2], [0, 135311, 2], [0, 10550, 2]]\n",
      "en\n",
      "sco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "sco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "cy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "cy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n",
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cs\n",
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "oc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "oc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n",
      "af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n",
      "af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "scn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "scn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "ast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "ast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br\n",
      "br\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br\n",
      "br\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n",
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fa\n",
      "fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultipleChoiceExample(example_id=16421, question='', contexts=['● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम', '● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1990 - अंतर्राष्ट्रीय संबंध संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1992 - अर्थशास्त्र, व्यवसाय और व्यावसायिक विकास संस्थान और कार्मिक का पुनर्प्रशिक्षण● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1995 - अंतर्राष्ट्रीय व्यापार संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1996 - मास्टर डिग्री विभाग● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 1999 - अंतर्राष्ट्रीय पर्यटन संकाय● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2018 - TSUE और USUE के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2019 - TSUE और IMC Krems के बीच संयुक्त शैक्षिक कार्यक्रम● \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 2021 - TSUE और लंदन विश्वविद्यालय और लंदन स्कूल ऑफ़ इकोनॉमिक्स एंड पॉलिटिकल साइंस के बीच डबल डिग्री शैक्षिक कार्यक्रम'], endings=['पूर्व रेक्टर', 'स्वतंत्र उज़्बेकिस्तान के विकास के दौरान, विश्वविद्यालय ने नए संकायों और प्रभागों की स्थापना देखी:', 'विश्वविद्यालय का इतिहास', 'शिक्षा संकाय'], label='B')\n",
      "[[0, 9476, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11704, 2], [0, 20, 2], [0, 225798, 2], [0, 59753, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 12839, 2], [0, 20, 2], [0, 25494, 92829, 4, 2], [0, 52894, 2], [0, 871, 2], [0, 150767, 2], [0, 9623, 2], [0, 128004, 2], [0, 871, 2], [0, 123009, 145656, 2], [0, 641, 2], [0, 93019, 7475, 134709, 4846, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11857, 2], [0, 20, 2], [0, 225798, 2], [0, 45929, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 11891, 2], [0, 20, 2], [0, 192002, 2], [0, 164989, 2], [0, 17495, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 8272, 2], [0, 20, 2], [0, 225798, 2], [0, 48470, 2], [0, 5469, 152210, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 267, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 20831, 13, 2], [0, 287, 2], [0, 22369, 2], [0, 44269, 2], [0, 135311, 2], [0, 10550, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 3640, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 566, 238, 2], [0, 28161, 7, 2], [0, 287, 2], [0, 22369, 2], [0, 44269, 2], [0, 135311, 2], [0, 10550, 109993, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 2], [0, 64371, 2], [0, 20, 2], [0, 6, 17750, 13, 2], [0, 871, 2], [0, 239015, 2], [0, 88096, 2], [0, 871, 2], [0, 239015, 2], [0, 54585, 2], [0, 218491, 2], [0, 114522, 83603, 10924, 1920, 99649, 2], [0, 109582, 2], [0, 80105, 10259, 71683, 1471, 2], [0, 6, 76081, 25784, 2], [0, 287, 2], [0, 22369, 2], [0, 227649, 2], [0, 164989, 2], [0, 135311, 2], [0, 10550, 2]]\n",
      "sco\n",
      "sco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sco\n",
      "sco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cy\n",
      "cy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cy\n",
      "cy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca\n",
      "ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ca\n",
      "ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n",
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk\n",
      "sk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms\n",
      "ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ms\n",
      "ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oc\n",
      "oc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oc\n",
      "oc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af\n",
      "af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af\n",
      "af\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scn\n",
      "scn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scn\n",
      "scn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an\n",
      "an\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ast\n",
      "ast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ast\n",
      "ast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"        \n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import stopwordsiso as stopwords\n",
    "from transformers import BertTokenizerFast,XLMRobertaTokenizerFast\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertForTokenClassification,\n",
    ")\n",
    "from DataProcessor import MultipleChoiceExample \n",
    "\n",
    "from transformers import (\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaForTokenClassification,\n",
    ")\n",
    "\n",
    "import string\n",
    "import pickle\n",
    "import stopwordsiso as stopwords\n",
    "\n",
    "run=1\n",
    "analyzed_langs=[]\n",
    "values_langs=[]\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "                \"bert\": (BertConfig, BertForTokenClassification, BertTokenizerFast),\n",
    "                \"xlmroberta\": (XLMRobertaConfig, XLMRobertaForTokenClassification, XLMRobertaTokenizerFast),\n",
    "            }\n",
    "max_seq_length=128\n",
    "\n",
    "import pandas as pd\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test', sep=',', encoding='utf-8')\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "    language_source=lang2['l1']\n",
    "    language_target=lang2['l2']\n",
    "    print(language_source)\n",
    "    print(language_target)\n",
    "    model_type=lang2['model_type']\n",
    "\n",
    "    if(model_type=='bert'):\n",
    "        model_name='bert-base-multilingual-cased'\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "        fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "    if(model_type=='xlmroberta'):\n",
    "        model_name='xlm-roberta-base'\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "        model = model_class.from_pretrained(model_name, output_hidden_states = True)\n",
    "        fastTokenizer=tokenizer_class.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "    with open(path+language_target+'/title_target_test.pkl','rb') as file:\n",
    "            target_examples=pickle.load(file)\n",
    "\n",
    "    target_words=[]\n",
    "    target_stopwords=set(stopwords.stopwords(language_target))\n",
    "    for ex_id,example in enumerate(target_examples):\n",
    "        words=example.contexts[0].lower().split(\" \")\n",
    "        input_ids=fastTokenizer(words,max_length=max_seq_length,truncation='longest_first').input_ids\n",
    "        word_ind=0\n",
    "        acc_len=0\n",
    "        while acc_len<min(max_seq_length,len(words)):\n",
    "            if(len(input_ids)>word_ind):\n",
    "                acc_len+=len(input_ids[word_ind])-2\n",
    "                word_ind+=1\n",
    "            else:\n",
    "                print(example)\n",
    "                print(input_ids)\n",
    "                words=[]\n",
    "                break\n",
    "        words=words[:word_ind]\n",
    "        words=set(words)-target_stopwords\n",
    "        words=list(words-set(target_words))\n",
    "        if len(words)>0:\n",
    "            target_words.extend(words)\n",
    "        new_text=\" \".join(words) \n",
    "        target_examples[ex_id]=MultipleChoiceExample(example_id=example.example_id,\n",
    "                                question=example.question,\n",
    "                                contexts=[new_text,new_text,new_text,new_text],\n",
    "                                endings=example.endings,\n",
    "                                label=example.label)\n",
    "    with open(path+language_target+'/128_title_target_test_'+language_source+\"_\"+model_type+'.pkl','wb') as file:\n",
    "            pickle.dump(target_examples,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866a916",
   "metadata": {},
   "source": [
    "# number of similar words between training and test file, total words in test file and the percentage of overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec155398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'bert', 19071, 2081, 0.10911855697131771]\n",
      "['fr', 'br', 'xlmroberta', 18839, 2049, 0.10876373480545677]\n",
      "['ar', 'fa', 'bert', 113338, 10136, 0.08943161163952072]\n",
      "['ar', 'fa', 'xlmroberta', 123368, 10706, 0.0867810129044809]\n",
      "['ar', 'hi', 'bert', 60105, 2196, 0.03653606189168954]\n",
      "['ar', 'hi', 'xlmroberta', 71554, 2444, 0.03415602202532353]\n",
      "['en', 'sco', 'bert', 16491, 6270, 0.38020738584682556]\n",
      "['en', 'sco', 'xlmroberta', 15798, 6024, 0.38131409039118874]\n",
      "['en', 'cy', 'bert', 31185, 3091, 0.09911816578483246]\n",
      "['en', 'cy', 'xlmroberta', 34017, 3339, 0.0981568039509657]\n",
      "['es', 'ca', 'bert', 135394, 20416, 0.15078954754272716]\n",
      "['es', 'ca', 'xlmroberta', 136607, 20559, 0.1504974122848756]\n",
      "['cs', 'sk', 'bert', 125840, 28576, 0.22708200890019073]\n",
      "['cs', 'sk', 'xlmroberta', 141555, 31570, 0.22302285330790153]\n",
      "['id', 'ms', 'bert', 87454, 33264, 0.38035996066503536]\n",
      "['id', 'ms', 'xlmroberta', 94633, 35242, 0.37240708843638054]\n",
      "['fr', 'oc', 'bert', 36944, 5180, 0.14021221307925508]\n",
      "['fr', 'oc', 'xlmroberta', 36327, 5099, 0.14036391664602085]\n",
      "['nl', 'af', 'bert', 57558, 12496, 0.21710274853191563]\n",
      "['nl', 'af', 'xlmroberta', 60505, 12977, 0.21447814230228907]\n",
      "['it', 'scn', 'bert', 5796, 1451, 0.2503450655624569]\n",
      "['it', 'scn', 'xlmroberta', 5794, 1449, 0.25008629616845013]\n",
      "['es', 'an', 'bert', 15164, 5168, 0.34080717488789236]\n",
      "['es', 'an', 'xlmroberta', 14957, 5091, 0.34037574379889013]\n",
      "['es', 'ast', 'bert', 142888, 39481, 0.2763073176193942]\n",
      "['es', 'ast', 'xlmroberta', 137866, 38446, 0.2788649848403522]\n",
      "['br', 'br', 'bert', 19071, 11980, 0.6281789103874993]\n",
      "['br', 'br', 'xlmroberta', 18839, 11842, 0.6285896278995701]\n",
      "['fa', 'fa', 'bert', 113338, 100772, 0.8891280947255114]\n",
      "['fa', 'fa', 'xlmroberta', 123368, 109428, 0.8870047338045522]\n",
      "['hi', 'hi', 'bert', 60105, 54269, 0.9029032526412112]\n",
      "['hi', 'hi', 'xlmroberta', 71554, 64327, 0.8989993571288817]\n",
      "['sco', 'sco', 'bert', 16491, 10340, 0.6270086713965193]\n",
      "['sco', 'sco', 'xlmroberta', 15798, 9958, 0.6303329535384226]\n",
      "['cy', 'cy', 'bert', 31185, 19482, 0.6247234247234247]\n",
      "['cy', 'cy', 'xlmroberta', 34017, 21129, 0.6211306111650058]\n",
      "['ca', 'ca', 'bert', 135394, 82921, 0.6124422057107405]\n",
      "['ca', 'ca', 'xlmroberta', 136607, 83713, 0.612801686589999]\n",
      "['sk', 'sk', 'bert', 125840, 87356, 0.6941830896376351]\n",
      "['sk', 'sk', 'xlmroberta', 141555, 97865, 0.6913567164706298]\n",
      "['ms', 'ms', 'bert', 87454, 46463, 0.5312850184096782]\n",
      "['ms', 'ms', 'xlmroberta', 94633, 49736, 0.525567191148965]\n",
      "['oc', 'oc', 'bert', 36944, 27035, 0.731783239497618]\n",
      "['oc', 'oc', 'xlmroberta', 36327, 26623, 0.7328708673989044]\n",
      "['af', 'af', 'bert', 57558, 35732, 0.6207998888078112]\n",
      "['af', 'af', 'xlmroberta', 60505, 37494, 0.619684323609619]\n",
      "['scn', 'scn', 'bert', 5796, 4284, 0.7391304347826086]\n",
      "['scn', 'scn', 'xlmroberta', 5794, 4286, 0.7397307559544356]\n",
      "['an', 'an', 'bert', 15164, 10903, 0.7190055394355052]\n",
      "['an', 'an', 'xlmroberta', 14957, 10787, 0.7212007755565956]\n",
      "['ast', 'ast', 'bert', 142888, 90884, 0.6360506130675774]\n",
      "['ast', 'ast', 'xlmroberta', 137866, 88208, 0.6398096702595274]\n"
     ]
    }
   ],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"\n",
    "import pandas as pd\n",
    "title_128=pd.DataFrame(columns=[\"l1\",\"l2\",\"model_type\",\"total_word\",\"same_word\",\"percentage_word\"])\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test', sep=',', encoding='utf-8')\n",
    "run=1\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "    language_source=lang2['l1']\n",
    "    language_target=lang2['l2']\n",
    "    model_type=lang2['model_type']\n",
    "    \n",
    "    with open(path+language_target+'/128_title_target_test_'+language_source+\"_\"+model_type+'.pkl','rb') as file:\n",
    "        target_examples=pickle.load(file)\n",
    "    \n",
    "    total_word=0    \n",
    "    same_word=0\n",
    "    \n",
    "    same_file=open(path+language_target+'/cosine_textword_with_'+language_source+\"_\"+model_type+str(run)+'.txt','r').readlines()\n",
    "    same_words=dict()\n",
    "    for line in same_file:\n",
    "        line=line.strip().split(\"\\t\")\n",
    "        if len(line)==2:\n",
    "            same_words[line[0]]=line[1]\n",
    "    \n",
    "    unsame_file=open(path+language_target+'/unsame_textword_with_'+language_source+\"_\"+model_type+str(run)+'.txt','r').readlines()   \n",
    "    unsame_word_list=[]\n",
    "    for line in unsame_file:\n",
    "        unsame_word_list.append(line.strip())\n",
    "        \n",
    "    for ex_id,example in enumerate(target_examples):\n",
    "        words=example.contexts[0].split(\" \")\n",
    "        for w_i,word in enumerate(words):\n",
    "            total_word+=1\n",
    "            if word.lower() in same_words.keys():\n",
    "                same_word+=1\n",
    "    info=[lang2['l1'],lang2['l2'],lang2['model_type'],total_word,same_word,same_word/total_word]\n",
    "    title_128.loc[len(title_128.index)]=info\n",
    "    print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "906225b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_2_compare=pd.read_csv('perturb_22_compare',sep=',', encoding='utf-8')\n",
    "compare_128=title_128.merge(perturb_2_compare)\n",
    "all_lang_info=compare_128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a8fb7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>model_type</th>\n",
       "      <th>total_word</th>\n",
       "      <th>same_word</th>\n",
       "      <th>percentage_word</th>\n",
       "      <th>l1-name</th>\n",
       "      <th>l2-name</th>\n",
       "      <th>eval_loss_before</th>\n",
       "      <th>precision_before</th>\n",
       "      <th>recall_before</th>\n",
       "      <th>f1_score_before</th>\n",
       "      <th>eval_loss_after</th>\n",
       "      <th>precision_after</th>\n",
       "      <th>recall_after</th>\n",
       "      <th>f1_score_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>bert</td>\n",
       "      <td>19071</td>\n",
       "      <td>2081</td>\n",
       "      <td>0.109119</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>1.462501</td>\n",
       "      <td>0.654987</td>\n",
       "      <td>0.621483</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>1.586561</td>\n",
       "      <td>0.634465</td>\n",
       "      <td>0.621483</td>\n",
       "      <td>0.627907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>18839</td>\n",
       "      <td>2049</td>\n",
       "      <td>0.108764</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>1.344928</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.572890</td>\n",
       "      <td>0.575096</td>\n",
       "      <td>1.385799</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.544757</td>\n",
       "      <td>0.553966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>bert</td>\n",
       "      <td>113338</td>\n",
       "      <td>10136</td>\n",
       "      <td>0.089432</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>0.814855</td>\n",
       "      <td>0.779028</td>\n",
       "      <td>0.793696</td>\n",
       "      <td>0.786294</td>\n",
       "      <td>0.878111</td>\n",
       "      <td>0.757077</td>\n",
       "      <td>0.771797</td>\n",
       "      <td>0.764366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>123368</td>\n",
       "      <td>10706</td>\n",
       "      <td>0.086781</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>0.653479</td>\n",
       "      <td>0.775051</td>\n",
       "      <td>0.782030</td>\n",
       "      <td>0.778525</td>\n",
       "      <td>0.769919</td>\n",
       "      <td>0.720991</td>\n",
       "      <td>0.732501</td>\n",
       "      <td>0.726701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>bert</td>\n",
       "      <td>60105</td>\n",
       "      <td>2196</td>\n",
       "      <td>0.036536</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>1.243306</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.624820</td>\n",
       "      <td>0.631936</td>\n",
       "      <td>1.332172</td>\n",
       "      <td>0.623174</td>\n",
       "      <td>0.613321</td>\n",
       "      <td>0.618208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>71554</td>\n",
       "      <td>2444</td>\n",
       "      <td>0.034156</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.772180</td>\n",
       "      <td>0.762512</td>\n",
       "      <td>0.744609</td>\n",
       "      <td>0.753455</td>\n",
       "      <td>0.789876</td>\n",
       "      <td>0.755272</td>\n",
       "      <td>0.737901</td>\n",
       "      <td>0.746486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>bert</td>\n",
       "      <td>16491</td>\n",
       "      <td>6270</td>\n",
       "      <td>0.380207</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>1.359826</td>\n",
       "      <td>0.738589</td>\n",
       "      <td>0.726531</td>\n",
       "      <td>0.732510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>15798</td>\n",
       "      <td>6024</td>\n",
       "      <td>0.381314</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>1.173925</td>\n",
       "      <td>0.724891</td>\n",
       "      <td>0.677551</td>\n",
       "      <td>0.700422</td>\n",
       "      <td>1.706664</td>\n",
       "      <td>0.549587</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.546201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>bert</td>\n",
       "      <td>31185</td>\n",
       "      <td>3091</td>\n",
       "      <td>0.099118</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>1.492519</td>\n",
       "      <td>0.627530</td>\n",
       "      <td>0.647632</td>\n",
       "      <td>0.637423</td>\n",
       "      <td>1.698112</td>\n",
       "      <td>0.594378</td>\n",
       "      <td>0.618384</td>\n",
       "      <td>0.606143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>34017</td>\n",
       "      <td>3339</td>\n",
       "      <td>0.098157</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>1.694407</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.588313</td>\n",
       "      <td>1.696880</td>\n",
       "      <td>0.550129</td>\n",
       "      <td>0.596100</td>\n",
       "      <td>0.572193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>bert</td>\n",
       "      <td>135394</td>\n",
       "      <td>20416</td>\n",
       "      <td>0.150790</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>0.966566</td>\n",
       "      <td>0.797028</td>\n",
       "      <td>0.785356</td>\n",
       "      <td>0.791149</td>\n",
       "      <td>1.133535</td>\n",
       "      <td>0.758887</td>\n",
       "      <td>0.749448</td>\n",
       "      <td>0.754138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>136607</td>\n",
       "      <td>20559</td>\n",
       "      <td>0.150497</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>1.382355</td>\n",
       "      <td>0.731319</td>\n",
       "      <td>0.718556</td>\n",
       "      <td>0.724881</td>\n",
       "      <td>1.431457</td>\n",
       "      <td>0.686830</td>\n",
       "      <td>0.681043</td>\n",
       "      <td>0.683924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>bert</td>\n",
       "      <td>125840</td>\n",
       "      <td>28576</td>\n",
       "      <td>0.227082</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>0.906951</td>\n",
       "      <td>0.804369</td>\n",
       "      <td>0.803583</td>\n",
       "      <td>0.803976</td>\n",
       "      <td>1.182809</td>\n",
       "      <td>0.745964</td>\n",
       "      <td>0.737459</td>\n",
       "      <td>0.741687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>141555</td>\n",
       "      <td>31570</td>\n",
       "      <td>0.223023</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>0.745653</td>\n",
       "      <td>0.792391</td>\n",
       "      <td>0.786971</td>\n",
       "      <td>0.789672</td>\n",
       "      <td>1.000211</td>\n",
       "      <td>0.715641</td>\n",
       "      <td>0.712378</td>\n",
       "      <td>0.714006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>bert</td>\n",
       "      <td>87454</td>\n",
       "      <td>33264</td>\n",
       "      <td>0.380360</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>1.000841</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.794485</td>\n",
       "      <td>1.518809</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.688073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>94633</td>\n",
       "      <td>35242</td>\n",
       "      <td>0.372407</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.799471</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.792533</td>\n",
       "      <td>1.302344</td>\n",
       "      <td>0.607417</td>\n",
       "      <td>0.590260</td>\n",
       "      <td>0.598716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>bert</td>\n",
       "      <td>36944</td>\n",
       "      <td>5180</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>1.109227</td>\n",
       "      <td>0.784397</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.781073</td>\n",
       "      <td>1.298094</td>\n",
       "      <td>0.715877</td>\n",
       "      <td>0.722925</td>\n",
       "      <td>0.719384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>36327</td>\n",
       "      <td>5099</td>\n",
       "      <td>0.140364</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>1.000687</td>\n",
       "      <td>0.698592</td>\n",
       "      <td>0.697609</td>\n",
       "      <td>0.698100</td>\n",
       "      <td>1.140476</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.649789</td>\n",
       "      <td>0.642113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>bert</td>\n",
       "      <td>57558</td>\n",
       "      <td>12496</td>\n",
       "      <td>0.217103</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>1.165977</td>\n",
       "      <td>0.792568</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.794715</td>\n",
       "      <td>1.460615</td>\n",
       "      <td>0.731425</td>\n",
       "      <td>0.728940</td>\n",
       "      <td>0.730180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>60505</td>\n",
       "      <td>12977</td>\n",
       "      <td>0.214478</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>0.921793</td>\n",
       "      <td>0.752218</td>\n",
       "      <td>0.748641</td>\n",
       "      <td>0.750426</td>\n",
       "      <td>1.090811</td>\n",
       "      <td>0.661444</td>\n",
       "      <td>0.659647</td>\n",
       "      <td>0.660544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>bert</td>\n",
       "      <td>5796</td>\n",
       "      <td>1451</td>\n",
       "      <td>0.250345</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>1.095371</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>1.670566</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.674699</td>\n",
       "      <td>0.670659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>5794</td>\n",
       "      <td>1449</td>\n",
       "      <td>0.250086</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>1.250472</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>1.598252</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.457831</td>\n",
       "      <td>0.466258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>bert</td>\n",
       "      <td>15164</td>\n",
       "      <td>5168</td>\n",
       "      <td>0.340807</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.857678</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>1.059547</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.784091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>14957</td>\n",
       "      <td>5091</td>\n",
       "      <td>0.340376</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>0.734883</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>0.767790</td>\n",
       "      <td>0.772128</td>\n",
       "      <td>1.174952</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.599251</td>\n",
       "      <td>0.595903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>bert</td>\n",
       "      <td>142888</td>\n",
       "      <td>39481</td>\n",
       "      <td>0.276307</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>0.745001</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.845844</td>\n",
       "      <td>1.092844</td>\n",
       "      <td>0.767517</td>\n",
       "      <td>0.767164</td>\n",
       "      <td>0.767340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>137866</td>\n",
       "      <td>38446</td>\n",
       "      <td>0.278865</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>0.740191</td>\n",
       "      <td>0.767686</td>\n",
       "      <td>0.772445</td>\n",
       "      <td>0.770058</td>\n",
       "      <td>1.206258</td>\n",
       "      <td>0.606584</td>\n",
       "      <td>0.605052</td>\n",
       "      <td>0.605817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>bert</td>\n",
       "      <td>19071</td>\n",
       "      <td>11980</td>\n",
       "      <td>0.628179</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>0.747283</td>\n",
       "      <td>0.703325</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.915446</td>\n",
       "      <td>0.692105</td>\n",
       "      <td>0.672634</td>\n",
       "      <td>0.682231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>18839</td>\n",
       "      <td>11842</td>\n",
       "      <td>0.628590</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.255754</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>1.385762</td>\n",
       "      <td>0.236145</td>\n",
       "      <td>0.250639</td>\n",
       "      <td>0.243176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>bert</td>\n",
       "      <td>113338</td>\n",
       "      <td>100772</td>\n",
       "      <td>0.889128</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>1.071476</td>\n",
       "      <td>0.770006</td>\n",
       "      <td>0.781826</td>\n",
       "      <td>0.775871</td>\n",
       "      <td>1.151411</td>\n",
       "      <td>0.746278</td>\n",
       "      <td>0.759108</td>\n",
       "      <td>0.752638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>123368</td>\n",
       "      <td>109428</td>\n",
       "      <td>0.887005</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>0.619075</td>\n",
       "      <td>0.805126</td>\n",
       "      <td>0.810070</td>\n",
       "      <td>0.807590</td>\n",
       "      <td>0.732458</td>\n",
       "      <td>0.752338</td>\n",
       "      <td>0.757266</td>\n",
       "      <td>0.754794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>bert</td>\n",
       "      <td>60105</td>\n",
       "      <td>54269</td>\n",
       "      <td>0.902903</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.820731</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.732821</td>\n",
       "      <td>0.870144</td>\n",
       "      <td>0.721610</td>\n",
       "      <td>0.721610</td>\n",
       "      <td>0.721610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>71554</td>\n",
       "      <td>64327</td>\n",
       "      <td>0.898999</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>0.636657</td>\n",
       "      <td>0.776657</td>\n",
       "      <td>0.774796</td>\n",
       "      <td>0.775726</td>\n",
       "      <td>0.654847</td>\n",
       "      <td>0.765043</td>\n",
       "      <td>0.767609</td>\n",
       "      <td>0.766324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>bert</td>\n",
       "      <td>16491</td>\n",
       "      <td>10340</td>\n",
       "      <td>0.627009</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>0.441682</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.841046</td>\n",
       "      <td>0.668836</td>\n",
       "      <td>0.748062</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>0.767396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>15798</td>\n",
       "      <td>9958</td>\n",
       "      <td>0.630333</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>0.695841</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.739220</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.591928</td>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.564103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>bert</td>\n",
       "      <td>31185</td>\n",
       "      <td>19482</td>\n",
       "      <td>0.624723</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>0.724366</td>\n",
       "      <td>0.775815</td>\n",
       "      <td>0.795265</td>\n",
       "      <td>0.785420</td>\n",
       "      <td>0.827809</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.766017</td>\n",
       "      <td>0.757054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>34017</td>\n",
       "      <td>21129</td>\n",
       "      <td>0.621131</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>0.757319</td>\n",
       "      <td>0.693824</td>\n",
       "      <td>0.735376</td>\n",
       "      <td>0.713996</td>\n",
       "      <td>0.821257</td>\n",
       "      <td>0.678996</td>\n",
       "      <td>0.715877</td>\n",
       "      <td>0.696949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>bert</td>\n",
       "      <td>135394</td>\n",
       "      <td>82921</td>\n",
       "      <td>0.612442</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>0.752348</td>\n",
       "      <td>0.861305</td>\n",
       "      <td>0.865797</td>\n",
       "      <td>0.863545</td>\n",
       "      <td>0.962822</td>\n",
       "      <td>0.821151</td>\n",
       "      <td>0.827081</td>\n",
       "      <td>0.824106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>136607</td>\n",
       "      <td>83713</td>\n",
       "      <td>0.612802</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>0.560116</td>\n",
       "      <td>0.840754</td>\n",
       "      <td>0.840923</td>\n",
       "      <td>0.840838</td>\n",
       "      <td>0.723098</td>\n",
       "      <td>0.782346</td>\n",
       "      <td>0.778736</td>\n",
       "      <td>0.780537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>bert</td>\n",
       "      <td>125840</td>\n",
       "      <td>87356</td>\n",
       "      <td>0.694183</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.837652</td>\n",
       "      <td>0.831922</td>\n",
       "      <td>0.834777</td>\n",
       "      <td>1.021870</td>\n",
       "      <td>0.768926</td>\n",
       "      <td>0.757655</td>\n",
       "      <td>0.763249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>141555</td>\n",
       "      <td>97865</td>\n",
       "      <td>0.691357</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>0.823645</td>\n",
       "      <td>0.821498</td>\n",
       "      <td>0.822570</td>\n",
       "      <td>0.799069</td>\n",
       "      <td>0.730179</td>\n",
       "      <td>0.728990</td>\n",
       "      <td>0.729584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>bert</td>\n",
       "      <td>87454</td>\n",
       "      <td>46463</td>\n",
       "      <td>0.531285</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.836406</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.830664</td>\n",
       "      <td>1.168819</td>\n",
       "      <td>0.727332</td>\n",
       "      <td>0.718831</td>\n",
       "      <td>0.723057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>94633</td>\n",
       "      <td>49736</td>\n",
       "      <td>0.525567</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>0.594083</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.805844</td>\n",
       "      <td>0.812173</td>\n",
       "      <td>1.070858</td>\n",
       "      <td>0.643048</td>\n",
       "      <td>0.624675</td>\n",
       "      <td>0.633729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>bert</td>\n",
       "      <td>36944</td>\n",
       "      <td>27035</td>\n",
       "      <td>0.731783</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>0.792883</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.808720</td>\n",
       "      <td>0.800836</td>\n",
       "      <td>0.947769</td>\n",
       "      <td>0.752778</td>\n",
       "      <td>0.762307</td>\n",
       "      <td>0.757512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>36327</td>\n",
       "      <td>26623</td>\n",
       "      <td>0.732871</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.692924</td>\n",
       "      <td>0.729958</td>\n",
       "      <td>0.710959</td>\n",
       "      <td>0.921840</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.679325</td>\n",
       "      <td>0.667588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>bert</td>\n",
       "      <td>57558</td>\n",
       "      <td>35732</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>0.802989</td>\n",
       "      <td>0.803535</td>\n",
       "      <td>0.886460</td>\n",
       "      <td>0.747580</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.740918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>60505</td>\n",
       "      <td>37494</td>\n",
       "      <td>0.619684</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.360054</td>\n",
       "      <td>0.354278</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.339073</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.343394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>bert</td>\n",
       "      <td>5796</td>\n",
       "      <td>4284</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>0.829185</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.658065</td>\n",
       "      <td>0.943982</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.578313</td>\n",
       "      <td>0.592593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>5794</td>\n",
       "      <td>4286</td>\n",
       "      <td>0.739731</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>1.384972</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>1.385174</td>\n",
       "      <td>0.597403</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>bert</td>\n",
       "      <td>15164</td>\n",
       "      <td>10903</td>\n",
       "      <td>0.719006</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>0.584566</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>0.844860</td>\n",
       "      <td>0.759088</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>0.760300</td>\n",
       "      <td>0.744954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>14957</td>\n",
       "      <td>10787</td>\n",
       "      <td>0.721201</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>0.812382</td>\n",
       "      <td>0.710425</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.699620</td>\n",
       "      <td>0.980150</td>\n",
       "      <td>0.607287</td>\n",
       "      <td>0.561798</td>\n",
       "      <td>0.583658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>bert</td>\n",
       "      <td>142888</td>\n",
       "      <td>90884</td>\n",
       "      <td>0.636051</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>0.702621</td>\n",
       "      <td>0.856450</td>\n",
       "      <td>0.852124</td>\n",
       "      <td>0.854282</td>\n",
       "      <td>1.064554</td>\n",
       "      <td>0.783077</td>\n",
       "      <td>0.784156</td>\n",
       "      <td>0.783616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>137866</td>\n",
       "      <td>88208</td>\n",
       "      <td>0.639810</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>0.608224</td>\n",
       "      <td>0.811502</td>\n",
       "      <td>0.816533</td>\n",
       "      <td>0.814009</td>\n",
       "      <td>0.941325</td>\n",
       "      <td>0.695060</td>\n",
       "      <td>0.691389</td>\n",
       "      <td>0.693220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1   l2  model_type  total_word  same_word  percentage_word     l1-name  \\\n",
       "0    fr   br        bert       19071       2081         0.109119      french   \n",
       "1    fr   br  xlmroberta       18839       2049         0.108764      french   \n",
       "2    ar   fa        bert      113338      10136         0.089432      arabic   \n",
       "3    ar   fa  xlmroberta      123368      10706         0.086781      arabic   \n",
       "4    ar   hi        bert       60105       2196         0.036536      arabic   \n",
       "5    ar   hi  xlmroberta       71554       2444         0.034156      arabic   \n",
       "6    en  sco        bert       16491       6270         0.380207     english   \n",
       "7    en  sco  xlmroberta       15798       6024         0.381314     english   \n",
       "8    en   cy        bert       31185       3091         0.099118     english   \n",
       "9    en   cy  xlmroberta       34017       3339         0.098157     english   \n",
       "10   es   ca        bert      135394      20416         0.150790     spanish   \n",
       "11   es   ca  xlmroberta      136607      20559         0.150497     spanish   \n",
       "12   cs   sk        bert      125840      28576         0.227082       czech   \n",
       "13   cs   sk  xlmroberta      141555      31570         0.223023       czech   \n",
       "14   id   ms        bert       87454      33264         0.380360  indonesian   \n",
       "15   id   ms  xlmroberta       94633      35242         0.372407  indonesian   \n",
       "16   fr   oc        bert       36944       5180         0.140212      french   \n",
       "17   fr   oc  xlmroberta       36327       5099         0.140364      french   \n",
       "18   nl   af        bert       57558      12496         0.217103       dutch   \n",
       "19   nl   af  xlmroberta       60505      12977         0.214478       dutch   \n",
       "20   it  scn        bert        5796       1451         0.250345     italian   \n",
       "21   it  scn  xlmroberta        5794       1449         0.250086     italian   \n",
       "22   es   an        bert       15164       5168         0.340807     spanish   \n",
       "23   es   an  xlmroberta       14957       5091         0.340376     spanish   \n",
       "24   es  ast        bert      142888      39481         0.276307     spanish   \n",
       "25   es  ast  xlmroberta      137866      38446         0.278865     spanish   \n",
       "26   br   br        bert       19071      11980         0.628179      breton   \n",
       "27   br   br  xlmroberta       18839      11842         0.628590      breton   \n",
       "28   fa   fa        bert      113338     100772         0.889128     persian   \n",
       "29   fa   fa  xlmroberta      123368     109428         0.887005     persian   \n",
       "30   hi   hi        bert       60105      54269         0.902903       hindi   \n",
       "31   hi   hi  xlmroberta       71554      64327         0.898999       hindi   \n",
       "32  sco  sco        bert       16491      10340         0.627009       scots   \n",
       "33  sco  sco  xlmroberta       15798       9958         0.630333       scots   \n",
       "34   cy   cy        bert       31185      19482         0.624723       welsh   \n",
       "35   cy   cy  xlmroberta       34017      21129         0.621131       welsh   \n",
       "36   ca   ca        bert      135394      82921         0.612442     catalan   \n",
       "37   ca   ca  xlmroberta      136607      83713         0.612802     catalan   \n",
       "38   sk   sk        bert      125840      87356         0.694183      slovak   \n",
       "39   sk   sk  xlmroberta      141555      97865         0.691357      slovak   \n",
       "40   ms   ms        bert       87454      46463         0.531285       malay   \n",
       "41   ms   ms  xlmroberta       94633      49736         0.525567       malay   \n",
       "42   oc   oc        bert       36944      27035         0.731783     occitan   \n",
       "43   oc   oc  xlmroberta       36327      26623         0.732871     occitan   \n",
       "44   af   af        bert       57558      35732         0.620800     african   \n",
       "45   af   af  xlmroberta       60505      37494         0.619684     african   \n",
       "46  scn  scn        bert        5796       4284         0.739130    sicilian   \n",
       "47  scn  scn  xlmroberta        5794       4286         0.739731    sicilian   \n",
       "48   an   an        bert       15164      10903         0.719006   aragonese   \n",
       "49   an   an  xlmroberta       14957      10787         0.721201   aragonese   \n",
       "50  ast  ast        bert      142888      90884         0.636051    asturian   \n",
       "51  ast  ast  xlmroberta      137866      88208         0.639810    asturian   \n",
       "\n",
       "      l2-name  eval_loss_before  precision_before  recall_before  \\\n",
       "0      breton          1.462501          0.654987       0.621483   \n",
       "1      breton          1.344928          0.577320       0.572890   \n",
       "2     persian          0.814855          0.779028       0.793696   \n",
       "3     persian          0.653479          0.775051       0.782030   \n",
       "4       hindi          1.243306          0.639216       0.624820   \n",
       "5       hindi          0.772180          0.762512       0.744609   \n",
       "6       scots          0.821415          0.804781       0.824490   \n",
       "7       scots          1.173925          0.724891       0.677551   \n",
       "8       welsh          1.492519          0.627530       0.647632   \n",
       "9       welsh          1.694407          0.562183       0.616992   \n",
       "10    catalan          0.966566          0.797028       0.785356   \n",
       "11    catalan          1.382355          0.731319       0.718556   \n",
       "12     slovak          0.906951          0.804369       0.803583   \n",
       "13     slovak          0.745653          0.792391       0.786971   \n",
       "14      malay          1.000841          0.803453       0.785714   \n",
       "15      malay          0.738019          0.799471       0.785714   \n",
       "16    occitan          1.109227          0.784397       0.777778   \n",
       "17    occitan          1.000687          0.698592       0.697609   \n",
       "18    african          1.165977          0.792568       0.796875   \n",
       "19    african          0.921793          0.752218       0.748641   \n",
       "20   sicilian          1.095371          0.768293       0.759036   \n",
       "21   sicilian          1.250472          0.586667       0.530120   \n",
       "22  aragonese          0.731461          0.851301       0.857678   \n",
       "23  aragonese          0.734883          0.776515       0.767790   \n",
       "24   asturian          0.745001          0.846915       0.844776   \n",
       "25   asturian          0.740191          0.767686       0.772445   \n",
       "26     breton          0.846165          0.747283       0.703325   \n",
       "27     breton          1.385766          0.235294       0.255754   \n",
       "28    persian          1.071476          0.770006       0.781826   \n",
       "29    persian          0.619075          0.805126       0.810070   \n",
       "30      hindi          0.820731          0.734940       0.730714   \n",
       "31      hindi          0.636657          0.776657       0.774796   \n",
       "32      scots          0.441682          0.829365       0.853061   \n",
       "33      scots          0.695841          0.743802       0.734694   \n",
       "34      welsh          0.724366          0.775815       0.795265   \n",
       "35      welsh          0.757319          0.693824       0.735376   \n",
       "36    catalan          0.752348          0.861305       0.865797   \n",
       "37    catalan          0.560116          0.840754       0.840923   \n",
       "38     slovak          0.730048          0.837652       0.831922   \n",
       "39     slovak          0.592436          0.823645       0.821498   \n",
       "40      malay          0.745011          0.836406       0.825000   \n",
       "41      malay          0.594083          0.818602       0.805844   \n",
       "42    occitan          0.792883          0.793103       0.808720   \n",
       "43    occitan          0.829809          0.692924       0.729958   \n",
       "44    african          0.670639          0.804082       0.802989   \n",
       "45    african          1.386294          0.348684       0.360054   \n",
       "46   sicilian          0.829185          0.708333       0.614458   \n",
       "47   sicilian          1.384972          0.582278       0.554217   \n",
       "48  aragonese          0.584566          0.843284       0.846442   \n",
       "49  aragonese          0.812382          0.710425       0.689139   \n",
       "50   asturian          0.702621          0.856450       0.852124   \n",
       "51   asturian          0.608224          0.811502       0.816533   \n",
       "\n",
       "    f1_score_before  eval_loss_after  precision_after  recall_after  \\\n",
       "0          0.637795         1.586561         0.634465      0.621483   \n",
       "1          0.575096         1.385799         0.563492      0.544757   \n",
       "2          0.786294         0.878111         0.757077      0.771797   \n",
       "3          0.778525         0.769919         0.720991      0.732501   \n",
       "4          0.631936         1.332172         0.623174      0.613321   \n",
       "5          0.753455         0.789876         0.755272      0.737901   \n",
       "6          0.814516         1.359826         0.738589      0.726531   \n",
       "7          0.700422         1.706664         0.549587      0.542857   \n",
       "8          0.637423         1.698112         0.594378      0.618384   \n",
       "9          0.588313         1.696880         0.550129      0.596100   \n",
       "10         0.791149         1.133535         0.758887      0.749448   \n",
       "11         0.724881         1.431457         0.686830      0.681043   \n",
       "12         0.803976         1.182809         0.745964      0.737459   \n",
       "13         0.789672         1.000211         0.715641      0.712378   \n",
       "14         0.794485         1.518809         0.694444      0.681818   \n",
       "15         0.792533         1.302344         0.607417      0.590260   \n",
       "16         0.781073         1.298094         0.715877      0.722925   \n",
       "17         0.698100         1.140476         0.634615      0.649789   \n",
       "18         0.794715         1.460615         0.731425      0.728940   \n",
       "19         0.750426         1.090811         0.661444      0.659647   \n",
       "20         0.763636         1.670566         0.666667      0.674699   \n",
       "21         0.556962         1.598252         0.475000      0.457831   \n",
       "22         0.854478         1.059547         0.793103      0.775281   \n",
       "23         0.772128         1.174952         0.592593      0.599251   \n",
       "24         0.845844         1.092844         0.767517      0.767164   \n",
       "25         0.770058         1.206258         0.606584      0.605052   \n",
       "26         0.724638         0.915446         0.692105      0.672634   \n",
       "27         0.245098         1.385762         0.236145      0.250639   \n",
       "28         0.775871         1.151411         0.746278      0.759108   \n",
       "29         0.807590         0.732458         0.752338      0.757266   \n",
       "30         0.732821         0.870144         0.721610      0.721610   \n",
       "31         0.775726         0.654847         0.765043      0.767609   \n",
       "32         0.841046         0.668836         0.748062      0.787755   \n",
       "33         0.739220         0.998770         0.591928      0.538776   \n",
       "34         0.785420         0.827809         0.748299      0.766017   \n",
       "35         0.713996         0.821257         0.678996      0.715877   \n",
       "36         0.863545         0.962822         0.821151      0.827081   \n",
       "37         0.840838         0.723098         0.782346      0.778736   \n",
       "38         0.834777         1.021870         0.768926      0.757655   \n",
       "39         0.822570         0.799069         0.730179      0.728990   \n",
       "40         0.830664         1.168819         0.727332      0.718831   \n",
       "41         0.812173         1.070858         0.643048      0.624675   \n",
       "42         0.800836         0.947769         0.752778      0.762307   \n",
       "43         0.710959         0.921840         0.656250      0.679325   \n",
       "44         0.803535         0.886460         0.747580      0.734375   \n",
       "45         0.354278         1.386294         0.339073      0.347826   \n",
       "46         0.658065         0.943982         0.607595      0.578313   \n",
       "47         0.567901         1.385174         0.597403      0.554217   \n",
       "48         0.844860         0.759088         0.730216      0.760300   \n",
       "49         0.699620         0.980150         0.607287      0.561798   \n",
       "50         0.854282         1.064554         0.783077      0.784156   \n",
       "51         0.814009         0.941325         0.695060      0.691389   \n",
       "\n",
       "    f1_score_after  \n",
       "0         0.627907  \n",
       "1         0.553966  \n",
       "2         0.764366  \n",
       "3         0.726701  \n",
       "4         0.618208  \n",
       "5         0.746486  \n",
       "6         0.732510  \n",
       "7         0.546201  \n",
       "8         0.606143  \n",
       "9         0.572193  \n",
       "10        0.754138  \n",
       "11        0.683924  \n",
       "12        0.741687  \n",
       "13        0.714006  \n",
       "14        0.688073  \n",
       "15        0.598716  \n",
       "16        0.719384  \n",
       "17        0.642113  \n",
       "18        0.730180  \n",
       "19        0.660544  \n",
       "20        0.670659  \n",
       "21        0.466258  \n",
       "22        0.784091  \n",
       "23        0.595903  \n",
       "24        0.767340  \n",
       "25        0.605817  \n",
       "26        0.682231  \n",
       "27        0.243176  \n",
       "28        0.752638  \n",
       "29        0.754794  \n",
       "30        0.721610  \n",
       "31        0.766324  \n",
       "32        0.767396  \n",
       "33        0.564103  \n",
       "34        0.757054  \n",
       "35        0.696949  \n",
       "36        0.824106  \n",
       "37        0.780537  \n",
       "38        0.763249  \n",
       "39        0.729584  \n",
       "40        0.723057  \n",
       "41        0.633729  \n",
       "42        0.757512  \n",
       "43        0.667588  \n",
       "44        0.740918  \n",
       "45        0.343394  \n",
       "46        0.592593  \n",
       "47        0.575000  \n",
       "48        0.744954  \n",
       "49        0.583658  \n",
       "50        0.783616  \n",
       "51        0.693220  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "perturb_2_compare=pd.read_csv('perturb_2_compare',sep=',', encoding='utf-8')\n",
    "compare_128=title_128.merge(perturb_2_compare)\n",
    "all_lang_info=compare_128\n",
    "all_lang_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
