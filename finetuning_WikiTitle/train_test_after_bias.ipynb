{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2bf9a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l1-name</th>\n",
       "      <th>l2-name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.462501</td>\n",
       "      <td>0.654987</td>\n",
       "      <td>0.621483</td>\n",
       "      <td>0.637795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.344928</td>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.572890</td>\n",
       "      <td>0.575096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.814855</td>\n",
       "      <td>0.779028</td>\n",
       "      <td>0.793696</td>\n",
       "      <td>0.786294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.653479</td>\n",
       "      <td>0.775051</td>\n",
       "      <td>0.782030</td>\n",
       "      <td>0.778525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.243306</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.624820</td>\n",
       "      <td>0.631936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.772180</td>\n",
       "      <td>0.762512</td>\n",
       "      <td>0.744609</td>\n",
       "      <td>0.753455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.821415</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.824490</td>\n",
       "      <td>0.814516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.173925</td>\n",
       "      <td>0.724891</td>\n",
       "      <td>0.677551</td>\n",
       "      <td>0.700422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.492519</td>\n",
       "      <td>0.627530</td>\n",
       "      <td>0.647632</td>\n",
       "      <td>0.637423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.694407</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>0.616992</td>\n",
       "      <td>0.588313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.966566</td>\n",
       "      <td>0.797028</td>\n",
       "      <td>0.785356</td>\n",
       "      <td>0.791149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.382355</td>\n",
       "      <td>0.731319</td>\n",
       "      <td>0.718556</td>\n",
       "      <td>0.724881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.906951</td>\n",
       "      <td>0.804369</td>\n",
       "      <td>0.803583</td>\n",
       "      <td>0.803976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.745653</td>\n",
       "      <td>0.792391</td>\n",
       "      <td>0.786971</td>\n",
       "      <td>0.789672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.000841</td>\n",
       "      <td>0.803453</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.794485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.738019</td>\n",
       "      <td>0.799471</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.792533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.109227</td>\n",
       "      <td>0.784397</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.781073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.000687</td>\n",
       "      <td>0.698592</td>\n",
       "      <td>0.697609</td>\n",
       "      <td>0.698100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.165977</td>\n",
       "      <td>0.792568</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.794715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.921793</td>\n",
       "      <td>0.752218</td>\n",
       "      <td>0.748641</td>\n",
       "      <td>0.750426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.095371</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.763636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.250472</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.530120</td>\n",
       "      <td>0.556962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.857678</td>\n",
       "      <td>0.854478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.734883</td>\n",
       "      <td>0.776515</td>\n",
       "      <td>0.767790</td>\n",
       "      <td>0.772128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.745001</td>\n",
       "      <td>0.846915</td>\n",
       "      <td>0.844776</td>\n",
       "      <td>0.845844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.740191</td>\n",
       "      <td>0.767686</td>\n",
       "      <td>0.772445</td>\n",
       "      <td>0.770058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.846165</td>\n",
       "      <td>0.747283</td>\n",
       "      <td>0.703325</td>\n",
       "      <td>0.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.385766</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.255754</td>\n",
       "      <td>0.245098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.071476</td>\n",
       "      <td>0.770006</td>\n",
       "      <td>0.781826</td>\n",
       "      <td>0.775871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.619075</td>\n",
       "      <td>0.805126</td>\n",
       "      <td>0.810070</td>\n",
       "      <td>0.807590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.820731</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>0.730714</td>\n",
       "      <td>0.732821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.636657</td>\n",
       "      <td>0.776657</td>\n",
       "      <td>0.774796</td>\n",
       "      <td>0.775726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.441682</td>\n",
       "      <td>0.829365</td>\n",
       "      <td>0.853061</td>\n",
       "      <td>0.841046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.695841</td>\n",
       "      <td>0.743802</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.739220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.724366</td>\n",
       "      <td>0.775815</td>\n",
       "      <td>0.795265</td>\n",
       "      <td>0.785420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.757319</td>\n",
       "      <td>0.693824</td>\n",
       "      <td>0.735376</td>\n",
       "      <td>0.713996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.752348</td>\n",
       "      <td>0.861305</td>\n",
       "      <td>0.865797</td>\n",
       "      <td>0.863545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.560116</td>\n",
       "      <td>0.840754</td>\n",
       "      <td>0.840923</td>\n",
       "      <td>0.840838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.730048</td>\n",
       "      <td>0.837652</td>\n",
       "      <td>0.831922</td>\n",
       "      <td>0.834777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.592436</td>\n",
       "      <td>0.823645</td>\n",
       "      <td>0.821498</td>\n",
       "      <td>0.822570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.745011</td>\n",
       "      <td>0.836406</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.830664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.594083</td>\n",
       "      <td>0.818602</td>\n",
       "      <td>0.805844</td>\n",
       "      <td>0.812173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.792883</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.808720</td>\n",
       "      <td>0.800836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.829809</td>\n",
       "      <td>0.692924</td>\n",
       "      <td>0.729958</td>\n",
       "      <td>0.710959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>0.804082</td>\n",
       "      <td>0.802989</td>\n",
       "      <td>0.803535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.348684</td>\n",
       "      <td>0.360054</td>\n",
       "      <td>0.354278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.829185</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.614458</td>\n",
       "      <td>0.658065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.384972</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.554217</td>\n",
       "      <td>0.567901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.584566</td>\n",
       "      <td>0.843284</td>\n",
       "      <td>0.846442</td>\n",
       "      <td>0.844860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.812382</td>\n",
       "      <td>0.710425</td>\n",
       "      <td>0.689139</td>\n",
       "      <td>0.699620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.702621</td>\n",
       "      <td>0.856450</td>\n",
       "      <td>0.852124</td>\n",
       "      <td>0.854282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.608224</td>\n",
       "      <td>0.811502</td>\n",
       "      <td>0.816533</td>\n",
       "      <td>0.814009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1   l2     l1-name    l2-name  model_type  eval_loss  precision  \\\n",
       "0    fr   br      french     breton        bert   1.462501   0.654987   \n",
       "1    fr   br      french     breton  xlmroberta   1.344928   0.577320   \n",
       "2    ar   fa      arabic    persian        bert   0.814855   0.779028   \n",
       "3    ar   fa      arabic    persian  xlmroberta   0.653479   0.775051   \n",
       "4    ar   hi      arabic      hindi        bert   1.243306   0.639216   \n",
       "5    ar   hi      arabic      hindi  xlmroberta   0.772180   0.762512   \n",
       "6    en  sco     english      scots        bert   0.821415   0.804781   \n",
       "7    en  sco     english      scots  xlmroberta   1.173925   0.724891   \n",
       "8    en   cy     english      welsh        bert   1.492519   0.627530   \n",
       "9    en   cy     english      welsh  xlmroberta   1.694407   0.562183   \n",
       "10   es   ca     spanish    catalan        bert   0.966566   0.797028   \n",
       "11   es   ca     spanish    catalan  xlmroberta   1.382355   0.731319   \n",
       "12   cs   sk       czech     slovak        bert   0.906951   0.804369   \n",
       "13   cs   sk       czech     slovak  xlmroberta   0.745653   0.792391   \n",
       "14   id   ms  indonesian      malay        bert   1.000841   0.803453   \n",
       "15   id   ms  indonesian      malay  xlmroberta   0.738019   0.799471   \n",
       "16   fr   oc      french    occitan        bert   1.109227   0.784397   \n",
       "17   fr   oc      french    occitan  xlmroberta   1.000687   0.698592   \n",
       "18   nl   af       dutch    african        bert   1.165977   0.792568   \n",
       "19   nl   af       dutch    african  xlmroberta   0.921793   0.752218   \n",
       "20   it  scn     italian   sicilian        bert   1.095371   0.768293   \n",
       "21   it  scn     italian   sicilian  xlmroberta   1.250472   0.586667   \n",
       "22   es   an     spanish  aragonese        bert   0.731461   0.851301   \n",
       "23   es   an     spanish  aragonese  xlmroberta   0.734883   0.776515   \n",
       "24   es  ast     spanish   asturian        bert   0.745001   0.846915   \n",
       "25   es  ast     spanish   asturian  xlmroberta   0.740191   0.767686   \n",
       "26   br   br      breton     breton        bert   0.846165   0.747283   \n",
       "27   br   br      breton     breton  xlmroberta   1.385766   0.235294   \n",
       "28   fa   fa     persian    persian        bert   1.071476   0.770006   \n",
       "29   fa   fa     persian    persian  xlmroberta   0.619075   0.805126   \n",
       "30   hi   hi       hindi      hindi        bert   0.820731   0.734940   \n",
       "31   hi   hi       hindi      hindi  xlmroberta   0.636657   0.776657   \n",
       "32  sco  sco       scots      scots        bert   0.441682   0.829365   \n",
       "33  sco  sco       scots      scots  xlmroberta   0.695841   0.743802   \n",
       "34   cy   cy       welsh      welsh        bert   0.724366   0.775815   \n",
       "35   cy   cy       welsh      welsh  xlmroberta   0.757319   0.693824   \n",
       "36   ca   ca     catalan    catalan        bert   0.752348   0.861305   \n",
       "37   ca   ca     catalan    catalan  xlmroberta   0.560116   0.840754   \n",
       "38   sk   sk      slovak     slovak        bert   0.730048   0.837652   \n",
       "39   sk   sk      slovak     slovak  xlmroberta   0.592436   0.823645   \n",
       "40   ms   ms       malay      malay        bert   0.745011   0.836406   \n",
       "41   ms   ms       malay      malay  xlmroberta   0.594083   0.818602   \n",
       "42   oc   oc     occitan    occitan        bert   0.792883   0.793103   \n",
       "43   oc   oc     occitan    occitan  xlmroberta   0.829809   0.692924   \n",
       "44   af   af     african    african        bert   0.670639   0.804082   \n",
       "45   af   af     african    african  xlmroberta   1.386294   0.348684   \n",
       "46  scn  scn    sicilian   sicilian        bert   0.829185   0.708333   \n",
       "47  scn  scn    sicilian   sicilian  xlmroberta   1.384972   0.582278   \n",
       "48   an   an   aragonese  aragonese        bert   0.584566   0.843284   \n",
       "49   an   an   aragonese  aragonese  xlmroberta   0.812382   0.710425   \n",
       "50  ast  ast    asturian   asturian        bert   0.702621   0.856450   \n",
       "51  ast  ast    asturian   asturian  xlmroberta   0.608224   0.811502   \n",
       "\n",
       "      recall  f1_score  \n",
       "0   0.621483  0.637795  \n",
       "1   0.572890  0.575096  \n",
       "2   0.793696  0.786294  \n",
       "3   0.782030  0.778525  \n",
       "4   0.624820  0.631936  \n",
       "5   0.744609  0.753455  \n",
       "6   0.824490  0.814516  \n",
       "7   0.677551  0.700422  \n",
       "8   0.647632  0.637423  \n",
       "9   0.616992  0.588313  \n",
       "10  0.785356  0.791149  \n",
       "11  0.718556  0.724881  \n",
       "12  0.803583  0.803976  \n",
       "13  0.786971  0.789672  \n",
       "14  0.785714  0.794485  \n",
       "15  0.785714  0.792533  \n",
       "16  0.777778  0.781073  \n",
       "17  0.697609  0.698100  \n",
       "18  0.796875  0.794715  \n",
       "19  0.748641  0.750426  \n",
       "20  0.759036  0.763636  \n",
       "21  0.530120  0.556962  \n",
       "22  0.857678  0.854478  \n",
       "23  0.767790  0.772128  \n",
       "24  0.844776  0.845844  \n",
       "25  0.772445  0.770058  \n",
       "26  0.703325  0.724638  \n",
       "27  0.255754  0.245098  \n",
       "28  0.781826  0.775871  \n",
       "29  0.810070  0.807590  \n",
       "30  0.730714  0.732821  \n",
       "31  0.774796  0.775726  \n",
       "32  0.853061  0.841046  \n",
       "33  0.734694  0.739220  \n",
       "34  0.795265  0.785420  \n",
       "35  0.735376  0.713996  \n",
       "36  0.865797  0.863545  \n",
       "37  0.840923  0.840838  \n",
       "38  0.831922  0.834777  \n",
       "39  0.821498  0.822570  \n",
       "40  0.825000  0.830664  \n",
       "41  0.805844  0.812173  \n",
       "42  0.808720  0.800836  \n",
       "43  0.729958  0.710959  \n",
       "44  0.802989  0.803535  \n",
       "45  0.360054  0.354278  \n",
       "46  0.614458  0.658065  \n",
       "47  0.554217  0.567901  \n",
       "48  0.846442  0.844860  \n",
       "49  0.689139  0.699620  \n",
       "50  0.852124  0.854282  \n",
       "51  0.816533  0.814009  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"\n",
    "import pandas as pd\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test2', sep=',', encoding='utf-8')\n",
    "language_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d5dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449672d0",
   "metadata": {},
   "source": [
    "# Check model loads correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bdd359d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2c783d49394623b055c813c6d463a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b05b0ff61d24e2ebae164374184db86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'bert', 1.4203215206370634, 0.6602870813397129, 0.6715328467153284, 0.6658624849215923]\n",
      "fr\n",
      "br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7a7c1509a0492b83c542aa74ddc760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e551e4513954e338a5aeaef2b95de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'xlmroberta', 1.365322065704009, 0.6151832460732984, 0.5717761557177615, 0.5926860025220682]\n",
      "ar\n",
      "fa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3450eaa7bd543869f7f6053558d62cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b46ddbe1b2194f36a622ec00fcf49862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'fa', 'arabic', 'persian', 'bert', 0.8287904947876931, 0.7954591119148081, 0.7903773208225194, 0.7929100741037453]\n",
      "ar\n",
      "fa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bf6c64b69b4e26b8db4bf8719906fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1d6c42edee4292a196ab217035e789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'fa', 'arabic', 'persian', 'xlmroberta', 0.6655734070062638, 0.7806438712257548, 0.7793970852465562, 0.78001998001998]\n",
      "ar\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bb443762bd40de83b0585966a08894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d79c13273c4b8b8e08eaa137d6cfc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'hi', 'arabic', 'hindi', 'bert', 1.253680504354319, 0.6420592520641087, 0.6292241789623989, 0.635576923076923]\n",
      "ar\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bdfd55aac44605b13b2a71daf91781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc1ecf54e44cf2afd1a0b1d16a8888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'hi', 'arabic', 'hindi', 'xlmroberta', 0.7792593737069825, 0.7568850902184235, 0.7586863398381724, 0.757784644639886]\n",
      "en\n",
      "sco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304feefff0a44e0f84632bdb9febd517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d49822e5104ab8a3c2db1423e3083e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'sco', 'english', 'scots', 'bert', 0.7958689904771745, 0.8620689655172413, 0.8522727272727273, 0.8571428571428572]\n",
      "en\n",
      "sco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464931ca284d4bba93ab277dc7c34b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481855521ce84098ab84acb922b62170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'sco', 'english', 'scots', 'xlmroberta', 1.0957435686141253, 0.776, 0.7348484848484849, 0.754863813229572]\n",
      "en\n",
      "cy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218e7366b94049268cde0128fdc3fb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c510771b90b044a9b4b96e55427f3b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'cy', 'english', 'welsh', 'bert', 1.42500116986533, 0.6657754010695187, 0.6840659340659341, 0.6747967479674797]\n",
      "en\n",
      "cy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a8529caf4f420ead76d5999b405497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d3720c2364c97bcc4b924d6ec5b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'cy', 'english', 'welsh', 'xlmroberta', 1.5745613581190507, 0.6110363391655451, 0.6236263736263736, 0.6172671651937458]\n",
      "es\n",
      "ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cae6eac78124dcbbbb2a2406f2172b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5b2623722048399557238268025be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ca', 'spanish', 'catalan', 'bert', 0.9561295164629626, 0.7907542579075426, 0.7957559681697612, 0.7932472287196176]\n",
      "es\n",
      "ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b86eca1b62b4818b2735a9b6766b5f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a9f5d39c734f4b9424af09c2fd7d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ca', 'spanish', 'catalan', 'xlmroberta', 1.399935125397184, 0.7241867043847242, 0.7312793307488268, 0.7277157360406091]\n",
      "cs\n",
      "sk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bab3f4a9fa4f9b90224fa2e99d76d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec5a013ddee4914af7f8f7310e8f4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'sk', 'czech', 'slovak', 'bert', 0.918968291533867, 0.8033964728935337, 0.8028720626631853, 0.8031341821743389]\n",
      "cs\n",
      "sk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1e0f045af647d1bfad4afa1cbce581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773c78f53a1442a0b8393c3d68a3667b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'sk', 'czech', 'slovak', 'xlmroberta', 0.7228361157924718, 0.7958333333333333, 0.810378590078329, 0.8030401034928849]\n",
      "id\n",
      "ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b7adca51334f90a1f2248dabe440f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dddba59f4874d06907f1829a41ffa19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'ms', 'indonesian', 'malay', 'bert', 0.9432451630883077, 0.8044444444444444, 0.7935919055649241, 0.798981324278438]\n",
      "id\n",
      "ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f36ab12ddb4f0693aa12f27aa93dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdeaa7169ea4911bf080988eed4a740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'ms', 'indonesian', 'malay', 'xlmroberta', 0.7447572367146928, 0.7920689655172414, 0.7747048903878584, 0.7832907075873828]\n",
      "fr\n",
      "oc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "523e5326c04b472e8f1cc83799b70383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7173970b3ba840d39b2b66fb5e447bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'oc', 'french', 'occitan', 'bert', 1.1994165062219246, 0.7594405594405594, 0.7735042735042735, 0.766407904022583]\n",
      "fr\n",
      "oc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caaf32c7b2c47918ad5ea9741b7f0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e0cb65e5884c02811b772adf59d02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'oc', 'french', 'occitan', 'xlmroberta', 1.084406781813194, 0.672992700729927, 0.6566951566951567, 0.6647440519105984]\n",
      "nl\n",
      "af\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedf32b1dadc40e38cd089137881e67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7daac562f082478d9785b8c7ab86e984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nl', 'af', 'dutch', 'african', 'bert', 1.150735619885696, 0.7840112201963534, 0.7851123595505618, 0.7845614035087719]\n",
      "nl\n",
      "af\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d7fa1e0b1b4d009ebf29c980b6ce3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef854814d9344bcb816f78b7b74f714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nl', 'af', 'dutch', 'african', 'xlmroberta', 0.9018546889762621, 0.7559943582510579, 0.7528089887640449, 0.7543983110485574]\n",
      "it\n",
      "scn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a262ebbded4a1198243541cc9f31df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ac3ea0f7a94aeab4234ddda04209c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'scn', 'italian', 'sicilian', 'bert', 1.541083014673657, 0.7205882352941176, 0.7, 0.7101449275362319]\n",
      "it\n",
      "scn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf13845c3e234307a0619472695954a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58a27008c464b889db8839befdeb3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'scn', 'italian', 'sicilian', 'xlmroberta', 1.4989484349886577, 0.6307692307692307, 0.5857142857142857, 0.6074074074074074]\n",
      "es\n",
      "an\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007502d6d0144aacb48f36c8b106649e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a78c4c598b4b3dbe53c50a111e4d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'an', 'spanish', 'aragonese', 'bert', 0.7636704453034326, 0.8544061302681992, 0.867704280155642, 0.861003861003861]\n",
      "es\n",
      "an\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b33e26d03c4bfe8231bb4fd8c08569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49084507d8c4935bf16a8c732655623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'an', 'spanish', 'aragonese', 'xlmroberta', 0.741496785543859, 0.7725490196078432, 0.7665369649805448, 0.7695312500000001]\n",
      "es\n",
      "ast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009d429064d0417198dafc1591f8f229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba7b25234714570ba842601faf81f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ast', 'spanish', 'asturian', 'bert', 0.7775320139423907, 0.8421297372704022, 0.840761374187558, 0.8414449994192124]\n",
      "es\n",
      "ast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4163821dd774f57920be5cdc29c4521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8090b31449b14e939a8181b84cd22cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ast', 'spanish', 'asturian', 'xlmroberta', 0.7618372972761647, 0.7698245614035087, 0.7639275766016713, 0.7668647326109752]\n",
      "br\n",
      "br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20742f9e7165407bbf43419cbdb05d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'br', 'breton', 'breton', 'bert', 0.9357565144697825, 0.727735368956743, 0.6958637469586375, 0.7114427860696517]\n",
      "br\n",
      "br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01f73eb5f964fad859b5bc8740be786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'br', 'breton', 'breton', 'xlmroberta', 0.9653627311482149, 0.6331658291457286, 0.6131386861313869, 0.622991347342398]\n",
      "fa\n",
      "fa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf804b3a835f48109281bf6ce1035a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fa', 'fa', 'persian', 'persian', 'bert', 0.8221665163993835, 0.8200685069514406, 0.8125374326212816, 0.8162855996791015]\n",
      "fa\n",
      "fa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63139f1c239d4c7898a6f6fa36d9d87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fa', 'fa', 'persian', 'persian', 'xlmroberta', 0.6270987909793854, 0.8141129032258064, 0.8061489319225394, 0.8101113451700271]\n",
      "hi\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5650694142f243528478f3dd7b8a0800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'hindi', 'hindi', 'bert', 0.854560615144726, 0.7400670177118238, 0.7358400761542123, 0.7379474940334129]\n",
      "hi\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78d3d2ec38c4634ae037a3804c3da25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'hindi', 'hindi', 'xlmroberta', 0.6859361519266788, 0.7769340294257238, 0.7791527843883865, 0.7780418250950569]\n",
      "sco\n",
      "sco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7b8d67202453cad06f6e064c52695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sco', 'sco', 'scots', 'scots', 'bert', 0.3983541172929108, 0.8863636363636364, 0.8863636363636364, 0.8863636363636365]\n",
      "sco\n",
      "sco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4748baa9935f4269a3f06793b0fd5caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sco', 'sco', 'scots', 'scots', 'xlmroberta', 0.8419364523142576, 0.7479338842975206, 0.6856060606060606, 0.7154150197628457]\n",
      "cy\n",
      "cy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eb85e51c37466c8258e668a0aec6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cy', 'cy', 'welsh', 'welsh', 'bert', 0.6877046677594384, 0.7604871447902571, 0.771978021978022, 0.7661895023858214]\n",
      "cy\n",
      "cy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0cc5db795d414c91a9dc242808873b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cy', 'cy', 'welsh', 'welsh', 'xlmroberta', 0.7111840390910705, 0.7082228116710876, 0.7335164835164835, 0.7206477732793523]\n",
      "ca\n",
      "ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28881f7e75404a32b7608830a6c9e1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', 'ca', 'catalan', 'catalan', 'bert', 0.6843471073587306, 0.8592728758169934, 0.8583962456641502, 0.8588343370419516]\n",
      "ca\n",
      "ca\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2e2d71b7a8428ab96fc1f1d7df7aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', 'ca', 'catalan', 'catalan', 'xlmroberta', 0.6033706586473645, 0.835458409228901, 0.8422770863089165, 0.8388538914854704]\n",
      "sk\n",
      "sk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5466c1b1c44c219596ad2f226eda6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk', 'sk', 'slovak', 'slovak', 'bert', 0.7286072947592012, 0.8343658298011086, 0.8351827676240209, 0.8347740988419508]\n",
      "sk\n",
      "sk\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41ff717e6684b39aad29f541656e1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk', 'sk', 'slovak', 'slovak', 'xlmroberta', 0.5913257025455305, 0.8139757498404595, 0.8325718015665796, 0.8231687641174573]\n",
      "ms\n",
      "ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c15b54460774266b7eac78c9aafda32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', 'ms', 'malay', 'malay', 'bert', 0.7793272449263431, 0.8236868517898963, 0.8303541315345699, 0.8270070540812899]\n",
      "ms\n",
      "ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22d80750ead4ceca68630ae6c1bf998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', 'ms', 'malay', 'malay', 'xlmroberta', 0.6550744297181038, 0.7977378576180971, 0.8087689713322092, 0.8032155417852956]\n",
      "oc\n",
      "oc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c40eb6c97bf4bdfaccf42e4b7262c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oc', 'oc', 'occitan', 'occitan', 'bert', 0.8181968392996952, 0.773841961852861, 0.8091168091168092, 0.7910863509749304]\n",
      "oc\n",
      "oc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fe3509aca2d4abc9a9b0f99b27f46ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oc', 'oc', 'occitan', 'occitan', 'xlmroberta', 0.8594938824916708, 0.6855439642324889, 0.6552706552706553, 0.6700655498907502]\n",
      "af\n",
      "af\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa02e3011be46f99459811215c1ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', 'af', 'african', 'african', 'bert', 0.7513120580363918, 0.8130709768095573, 0.8125, 0.8127853881278538]\n",
      "af\n",
      "af\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3887ae1772e2434a83ad270e3da0b7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', 'af', 'african', 'african', 'xlmroberta', 0.6741255934173996, 0.7780979827089337, 0.7584269662921348, 0.7681365576102418]\n",
      "scn\n",
      "scn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8545b141e597433bbb865fbd77c8b773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scn', 'scn', 'sicilian', 'sicilian', 'bert', 0.9644769827524821, 0.6301369863013698, 0.6571428571428571, 0.6433566433566433]\n",
      "scn\n",
      "scn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301af2d53e9b419f836f43d6b49ef5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scn', 'scn', 'sicilian', 'sicilian', 'xlmroberta', 1.3860947688420613, 0.4, 0.4, 0.4000000000000001]\n",
      "an\n",
      "an\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bad14c20f1949f8bc83283ba5a1904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'an', 'aragonese', 'aragonese', 'bert', 0.4892498184926808, 0.8359375, 0.8326848249027238, 0.834307992202729]\n",
      "an\n",
      "an\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8912ea3522924e6c8d01dc2025817784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'an', 'aragonese', 'aragonese', 'xlmroberta', 0.7054369626566768, 0.7584745762711864, 0.6964980544747081, 0.7261663286004055]\n",
      "ast\n",
      "ast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e562c04a3c409d8263870030e3d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ast', 'ast', 'asturian', 'asturian', 'bert', 0.7321445061438391, 0.8533519553072626, 0.850974930362117, 0.8521617852161785]\n",
      "ast\n",
      "ast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b650f595a25a46fe9c463001e3529420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ast', 'ast', 'asturian', 'asturian', 'xlmroberta', 0.6097842022385936, 0.8052312003736571, 0.8003714020427113, 0.8027939464493598]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "main_file=pd.read_csv('all_langs_overlap_train_test4',sep=',', encoding='utf-8')[[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\n",
    "                                    \"eval_loss\",\"precision\",\"recall\",\"f1_score\"]]\n",
    "# main_file=pd.DataFrame(columns=[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\n",
    "#                                     \"eval_loss\",\"precision\",\"recall\",\"f1_score\"])\n",
    "run=2\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertForMultipleChoice\n",
    "from transformers import XLMRobertaForMultipleChoice\n",
    "from SimpleTransformers import titleModel\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "#     if l_index<26:\n",
    "        language_source=lang2['l1']\n",
    "        language_target=lang2['l2']\n",
    "#         if(language_target=='br'):\n",
    "        print(language_source)\n",
    "        print(language_target)\n",
    "        model_type=lang2['model_type']\n",
    "\n",
    "        with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "#         with open(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl','rb') as file:\n",
    "            target_test=pickle.load(file)\n",
    "        testdataset=target_test\n",
    "\n",
    "        if lang2['model_type']=='bert':\n",
    "#                 if not ((main_file['l1']==language_source)&(main_file['l2']==language_target)&(main_file['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_bert=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/bert'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'bert-base-multilingual-cased',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_bert, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=BertForMultipleChoice.from_pretrained(outputdir_bert)\n",
    "                \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=False,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                main_file.loc[l_index]=info\n",
    "                print(info)\n",
    "\n",
    "\n",
    "        if lang2['model_type']=='xlmroberta':\n",
    "#             if not ((main_file['l1']==language_source)&(main_file['l2']==language_target)&(main_file['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_xlmr=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/xlmroberta'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'xlm-roberta-base',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_xlmr, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=XLMRobertaForMultipleChoice.from_pretrained(outputdir_xlmr)\n",
    "                \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=False,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                main_file.loc[l_index]=info\n",
    "                print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea76bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file.to_csv('all_langs_overlap_train_test5',sep=',', encoding='utf-8',index=False)\n",
    "main_file.to_excel('all_langs_overlap_train_test5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75cf9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file2=pd.read_csv('all_langs_overlap_train_test4',sep=',', encoding='utf-8')[[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\n",
    "                                    \"eval_loss\",\"precision\",\"recall\",\"f1_score\"]]\n",
    "main_file2.to_csv('all_langs_overlap_train_test6',sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64ff34",
   "metadata": {},
   "source": [
    "# Perturbation 2-cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fd9d1cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d309ae976cc497188b481a9d0aca818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d794a12fa2d41b3b13acd3275c5bae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'bert', 265289, 21001, 0.07916272442506098, 1.5909405444182603, 0.6377171215880894, 0.6253041362530414, 0.6314496314496314]\n",
      "fr\n",
      "br\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cb9b9518c54c90b95c302ac069e5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e31de9fd7a4cbcaac2f1aab34f45d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'xlmroberta', 265289, 20873, 0.07868023174726431, 1.4877127058365767, 0.5613577023498695, 0.5231143552311436, 0.5415617128463476]\n",
      "ar\n",
      "fa\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f268734cbd40a6ac5b3b9021ec6092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m=\u001b[39mBertForMultipleChoice\u001b[38;5;241m.\u001b[39mfrom_pretrained(outputdir_bert)\n\u001b[1;32m     39\u001b[0m out_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m---> 40\u001b[0m results, model_outputs, preds_list,accuracy_result\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mperturb_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_overlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m info\u001b[38;5;241m=\u001b[39m[lang2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m],lang2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m],lang2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1-name\u001b[39m\u001b[38;5;124m'\u001b[39m],lang2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2-name\u001b[39m\u001b[38;5;124m'\u001b[39m],lang2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     43\u001b[0m      out_overlap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_O\u001b[39m\u001b[38;5;124m\"\u001b[39m],out_overlap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_O\u001b[39m\u001b[38;5;124m\"\u001b[39m],out_overlap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_O\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m/\u001b[39mout_overlap[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_O\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     44\u001b[0m      results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m],results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m],results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(info)\n",
      "File \u001b[0;32m~/finetuning_BERT_WikiTitle/SimpleTransformers.py:578\u001b[0m, in \u001b[0;36mtitleModel.eval_model\u001b[0;34m(self, eval_data, lang, source_lang, run, output_dir, verbose, perturb, perturb_type, out_overlap)\u001b[0m\n\u001b[1;32m    574\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_model_to_device()\n\u001b[0;32m--> 578\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_cache_examples\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mperturb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mperturb_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mout_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_overlap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m result, model_outputs, preds_list,accuracy_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset, output_dir)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mupdate(result)\n",
      "File \u001b[0;32m~/finetuning_BERT_WikiTitle/SimpleTransformers.py:899\u001b[0m, in \u001b[0;36mtitleModel.load_and_cache_examples\u001b[0;34m(self, examples, lang, source_lang, run, evaluate, no_cache, to_predict, perturb, perturb_type, out_overlap)\u001b[0m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Converting to features started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 899\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_multiple_choice_examples_to_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_seq_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_cache:\n\u001b[1;32m    907\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(features, cached_features_file)\n",
      "File \u001b[0;32m~/finetuning_BERT_WikiTitle/SimpleTransformers.py:112\u001b[0m, in \u001b[0;36mconvert_multiple_choice_examples_to_features\u001b[0;34m(examples, tokenizer, max_length, label_list, pad_token_segment_id, pad_on_left, pad_token, mask_padding_with_zero)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     text_b \u001b[38;5;241m=\u001b[39m example\u001b[38;5;241m.\u001b[39mquestion \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ending\n\u001b[0;32m--> 112\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_b\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongest_first\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_truncated_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_truncated_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    121\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention! you are cropping tokens (swag task is ok). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are training ARC and RACE and you are poping question + options,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou need to try to use a bigger max seq length!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     )\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2561\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2560\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2561\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2667\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2648\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2649\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2664\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2665\u001b[0m     )\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2670\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2684\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2685\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2686\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2740\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2730\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2731\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2732\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2733\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2738\u001b[0m )\n\u001b[0;32m-> 2740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils.py:649\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 649\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    653\u001b[0m     first_ids,\n\u001b[1;32m    654\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    668\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    669\u001b[0m )\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils.py:616\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils.py:547\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:244\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    242\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    247\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:419\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n\u001b[0;32m--> 419\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_chinese_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m orig_tokens \u001b[38;5;241m=\u001b[39m whitespace_tokenize(text)\n\u001b[1;32m    421\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:472\u001b[0m, in \u001b[0;36mBasicTokenizer._tokenize_chinese_chars\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    470\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[0;32m--> 472\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mord\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_chinese_char(cp):\n\u001b[1;32m    474\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# title_perturb_cosine=pd.read_csv('title_perturb_cosine',sep=',', encoding='utf-8')\n",
    "title_perturb_cosine=pd.DataFrame(columns=[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\n",
    "                                           \"total_O_l2\",\"same_O_l2\",\"percentage_O_l2\",\n",
    "                                            \"eval_loss\",\"precision\",\"recall\",\"f1_score\"])\n",
    "run=2\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertForMultipleChoice\n",
    "from transformers import XLMRobertaForMultipleChoice\n",
    "from SimpleTransformers import titleModel\n",
    "perturb_type='cosine_text'\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "        language_source=lang2['l1']\n",
    "        language_target=lang2['l2']\n",
    "#         if(language_target=='br'):\n",
    "        print(language_source)\n",
    "        print(language_target)\n",
    "        model_type=lang2['model_type']\n",
    "        print(model_type)\n",
    "        with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "            target_test=pickle.load(file)\n",
    "#         with open(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl','rb') as file:\n",
    "#             target_test=pickle.load(file)\n",
    "                \n",
    "        testdataset=target_test\n",
    "        if lang2['model_type']=='bert':\n",
    "#                 if not ((title_perturb_cosine['l1']==language_source)&(title_perturb_cosine['l2']==language_target)&(title_perturb_cosine['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_bert=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/bert'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'bert-base-multilingual-cased',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_bert, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=BertForMultipleChoice.from_pretrained(outputdir_bert)\n",
    "                     \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=True,perturb_type=perturb_type,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     out_overlap[\"all_O\"],out_overlap[\"same_O\"],out_overlap[\"same_O\"]/out_overlap[\"all_O\"],\n",
    "                     results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                print(info)\n",
    "                title_perturb_cosine.loc[l_index]=info\n",
    "\n",
    "\n",
    "        if lang2['model_type']=='xlmroberta':\n",
    "#             if not ((title_perturb_cosine['l1']==language_source)&(title_perturb_cosine['l2']==language_target)&(title_perturb_cosine['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_xlmr=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/xlmroberta'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'xlm-roberta-base',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_xlmr, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=XLMRobertaForMultipleChoice.from_pretrained(outputdir_xlmr)\n",
    "                \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=True,perturb_type=perturb_type,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     out_overlap[\"all_O\"],out_overlap[\"same_O\"],out_overlap[\"same_O\"]/out_overlap[\"all_O\"],\n",
    "                     results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                print(info)\n",
    "                title_perturb_cosine.loc[l_index]=info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e00aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_perturb_cosine.to_csv('title_perturb_cosine6',sep=',', encoding='utf-8',index=False)\n",
    "title_perturb_cosine.to_excel('title_perturb_cosine6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf4f1258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l1-name</th>\n",
       "      <th>l2-name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>eval_loss_before</th>\n",
       "      <th>precision_before</th>\n",
       "      <th>recall_before</th>\n",
       "      <th>f1_score_before</th>\n",
       "      <th>total_O_l2</th>\n",
       "      <th>same_O_l2</th>\n",
       "      <th>percentage_O_l2</th>\n",
       "      <th>eval_loss_after</th>\n",
       "      <th>precision_after</th>\n",
       "      <th>recall_after</th>\n",
       "      <th>f1_score_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.420322</td>\n",
       "      <td>0.660287</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.665862</td>\n",
       "      <td>96782</td>\n",
       "      <td>9197</td>\n",
       "      <td>0.095028</td>\n",
       "      <td>1.591118</td>\n",
       "      <td>0.637717</td>\n",
       "      <td>0.625304</td>\n",
       "      <td>0.631450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.365322</td>\n",
       "      <td>0.615183</td>\n",
       "      <td>0.571776</td>\n",
       "      <td>0.592686</td>\n",
       "      <td>95428</td>\n",
       "      <td>9007</td>\n",
       "      <td>0.094385</td>\n",
       "      <td>1.440016</td>\n",
       "      <td>0.556995</td>\n",
       "      <td>0.523114</td>\n",
       "      <td>0.539523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.828790</td>\n",
       "      <td>0.795459</td>\n",
       "      <td>0.790377</td>\n",
       "      <td>0.792910</td>\n",
       "      <td>1370517</td>\n",
       "      <td>200844</td>\n",
       "      <td>0.146546</td>\n",
       "      <td>0.878958</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.767419</td>\n",
       "      <td>0.771191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.665573</td>\n",
       "      <td>0.780644</td>\n",
       "      <td>0.779397</td>\n",
       "      <td>0.780020</td>\n",
       "      <td>1516814</td>\n",
       "      <td>227707</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.750748</td>\n",
       "      <td>0.738836</td>\n",
       "      <td>0.739868</td>\n",
       "      <td>0.739352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.253681</td>\n",
       "      <td>0.642059</td>\n",
       "      <td>0.629224</td>\n",
       "      <td>0.635577</td>\n",
       "      <td>509206</td>\n",
       "      <td>10813</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>1.314276</td>\n",
       "      <td>0.635174</td>\n",
       "      <td>0.623989</td>\n",
       "      <td>0.629532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.779259</td>\n",
       "      <td>0.756885</td>\n",
       "      <td>0.758686</td>\n",
       "      <td>0.757785</td>\n",
       "      <td>635212</td>\n",
       "      <td>12554</td>\n",
       "      <td>0.019763</td>\n",
       "      <td>0.800377</td>\n",
       "      <td>0.754817</td>\n",
       "      <td>0.745835</td>\n",
       "      <td>0.750299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.795869</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.852273</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>70465</td>\n",
       "      <td>20591</td>\n",
       "      <td>0.292216</td>\n",
       "      <td>1.175082</td>\n",
       "      <td>0.766284</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.095744</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.734848</td>\n",
       "      <td>0.754864</td>\n",
       "      <td>67738</td>\n",
       "      <td>19776</td>\n",
       "      <td>0.291948</td>\n",
       "      <td>1.596816</td>\n",
       "      <td>0.637795</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.625483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.425001</td>\n",
       "      <td>0.665775</td>\n",
       "      <td>0.684066</td>\n",
       "      <td>0.674797</td>\n",
       "      <td>169960</td>\n",
       "      <td>29425</td>\n",
       "      <td>0.173129</td>\n",
       "      <td>1.576923</td>\n",
       "      <td>0.629879</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.636302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.574561</td>\n",
       "      <td>0.611036</td>\n",
       "      <td>0.623626</td>\n",
       "      <td>0.617267</td>\n",
       "      <td>189323</td>\n",
       "      <td>32342</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>1.492930</td>\n",
       "      <td>0.574607</td>\n",
       "      <td>0.603022</td>\n",
       "      <td>0.588472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.956130</td>\n",
       "      <td>0.790754</td>\n",
       "      <td>0.795756</td>\n",
       "      <td>0.793247</td>\n",
       "      <td>1408210</td>\n",
       "      <td>241133</td>\n",
       "      <td>0.171234</td>\n",
       "      <td>1.088810</td>\n",
       "      <td>0.760821</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>0.758797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.399935</td>\n",
       "      <td>0.724187</td>\n",
       "      <td>0.731279</td>\n",
       "      <td>0.727716</td>\n",
       "      <td>1392222</td>\n",
       "      <td>239396</td>\n",
       "      <td>0.171952</td>\n",
       "      <td>1.374298</td>\n",
       "      <td>0.660645</td>\n",
       "      <td>0.664150</td>\n",
       "      <td>0.662393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.918968</td>\n",
       "      <td>0.803396</td>\n",
       "      <td>0.802872</td>\n",
       "      <td>0.803134</td>\n",
       "      <td>645873</td>\n",
       "      <td>156704</td>\n",
       "      <td>0.242624</td>\n",
       "      <td>1.147653</td>\n",
       "      <td>0.755454</td>\n",
       "      <td>0.757180</td>\n",
       "      <td>0.756316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.722836</td>\n",
       "      <td>0.795833</td>\n",
       "      <td>0.810379</td>\n",
       "      <td>0.803040</td>\n",
       "      <td>733988</td>\n",
       "      <td>177465</td>\n",
       "      <td>0.241782</td>\n",
       "      <td>0.954391</td>\n",
       "      <td>0.729835</td>\n",
       "      <td>0.735313</td>\n",
       "      <td>0.732564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.943245</td>\n",
       "      <td>0.804444</td>\n",
       "      <td>0.793592</td>\n",
       "      <td>0.798981</td>\n",
       "      <td>771367</td>\n",
       "      <td>280302</td>\n",
       "      <td>0.363383</td>\n",
       "      <td>1.419727</td>\n",
       "      <td>0.684441</td>\n",
       "      <td>0.683980</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.744757</td>\n",
       "      <td>0.792069</td>\n",
       "      <td>0.774705</td>\n",
       "      <td>0.783291</td>\n",
       "      <td>839256</td>\n",
       "      <td>304953</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>1.342536</td>\n",
       "      <td>0.587833</td>\n",
       "      <td>0.580101</td>\n",
       "      <td>0.583942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.199417</td>\n",
       "      <td>0.759441</td>\n",
       "      <td>0.773504</td>\n",
       "      <td>0.766408</td>\n",
       "      <td>172438</td>\n",
       "      <td>39816</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>1.324535</td>\n",
       "      <td>0.723343</td>\n",
       "      <td>0.715100</td>\n",
       "      <td>0.719198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.084407</td>\n",
       "      <td>0.672993</td>\n",
       "      <td>0.656695</td>\n",
       "      <td>0.664744</td>\n",
       "      <td>169161</td>\n",
       "      <td>38982</td>\n",
       "      <td>0.230443</td>\n",
       "      <td>1.231245</td>\n",
       "      <td>0.595652</td>\n",
       "      <td>0.585470</td>\n",
       "      <td>0.590517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.150736</td>\n",
       "      <td>0.784011</td>\n",
       "      <td>0.785112</td>\n",
       "      <td>0.784561</td>\n",
       "      <td>394555</td>\n",
       "      <td>89272</td>\n",
       "      <td>0.226260</td>\n",
       "      <td>1.525563</td>\n",
       "      <td>0.723618</td>\n",
       "      <td>0.707865</td>\n",
       "      <td>0.715655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.901855</td>\n",
       "      <td>0.755994</td>\n",
       "      <td>0.752809</td>\n",
       "      <td>0.754398</td>\n",
       "      <td>418037</td>\n",
       "      <td>94369</td>\n",
       "      <td>0.225743</td>\n",
       "      <td>1.152242</td>\n",
       "      <td>0.655076</td>\n",
       "      <td>0.661517</td>\n",
       "      <td>0.658281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1.541083</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>16013</td>\n",
       "      <td>4162</td>\n",
       "      <td>0.259914</td>\n",
       "      <td>1.896788</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.661765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.498948</td>\n",
       "      <td>0.630769</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.607407</td>\n",
       "      <td>15997</td>\n",
       "      <td>4137</td>\n",
       "      <td>0.258611</td>\n",
       "      <td>1.900114</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.467742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.763670</td>\n",
       "      <td>0.854406</td>\n",
       "      <td>0.867704</td>\n",
       "      <td>0.861004</td>\n",
       "      <td>64518</td>\n",
       "      <td>16005</td>\n",
       "      <td>0.248070</td>\n",
       "      <td>1.071652</td>\n",
       "      <td>0.755725</td>\n",
       "      <td>0.770428</td>\n",
       "      <td>0.763006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.741497</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.766537</td>\n",
       "      <td>0.769531</td>\n",
       "      <td>63833</td>\n",
       "      <td>15812</td>\n",
       "      <td>0.247709</td>\n",
       "      <td>1.195259</td>\n",
       "      <td>0.563265</td>\n",
       "      <td>0.536965</td>\n",
       "      <td>0.549801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.777532</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.840761</td>\n",
       "      <td>0.841445</td>\n",
       "      <td>1142383</td>\n",
       "      <td>338008</td>\n",
       "      <td>0.295880</td>\n",
       "      <td>1.077213</td>\n",
       "      <td>0.775890</td>\n",
       "      <td>0.773909</td>\n",
       "      <td>0.774898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.761837</td>\n",
       "      <td>0.769825</td>\n",
       "      <td>0.763928</td>\n",
       "      <td>0.766865</td>\n",
       "      <td>1094383</td>\n",
       "      <td>324467</td>\n",
       "      <td>0.296484</td>\n",
       "      <td>1.234623</td>\n",
       "      <td>0.603276</td>\n",
       "      <td>0.589833</td>\n",
       "      <td>0.596479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.935757</td>\n",
       "      <td>0.727735</td>\n",
       "      <td>0.695864</td>\n",
       "      <td>0.711443</td>\n",
       "      <td>96782</td>\n",
       "      <td>26837</td>\n",
       "      <td>0.277293</td>\n",
       "      <td>1.047752</td>\n",
       "      <td>0.680412</td>\n",
       "      <td>0.642336</td>\n",
       "      <td>0.660826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.971476</td>\n",
       "      <td>0.609254</td>\n",
       "      <td>0.576642</td>\n",
       "      <td>0.592500</td>\n",
       "      <td>95428</td>\n",
       "      <td>26398</td>\n",
       "      <td>0.276627</td>\n",
       "      <td>1.070033</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.549878</td>\n",
       "      <td>0.551893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.836097</td>\n",
       "      <td>0.817325</td>\n",
       "      <td>0.813735</td>\n",
       "      <td>0.815526</td>\n",
       "      <td>1370517</td>\n",
       "      <td>622926</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>0.905204</td>\n",
       "      <td>0.795037</td>\n",
       "      <td>0.786784</td>\n",
       "      <td>0.790889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.627099</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.806149</td>\n",
       "      <td>0.810111</td>\n",
       "      <td>1516814</td>\n",
       "      <td>688212</td>\n",
       "      <td>0.453722</td>\n",
       "      <td>0.742005</td>\n",
       "      <td>0.770838</td>\n",
       "      <td>0.758834</td>\n",
       "      <td>0.764789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.854561</td>\n",
       "      <td>0.740067</td>\n",
       "      <td>0.735840</td>\n",
       "      <td>0.737947</td>\n",
       "      <td>509206</td>\n",
       "      <td>25444</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>0.903202</td>\n",
       "      <td>0.724122</td>\n",
       "      <td>0.725845</td>\n",
       "      <td>0.724982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.685936</td>\n",
       "      <td>0.776934</td>\n",
       "      <td>0.779153</td>\n",
       "      <td>0.778042</td>\n",
       "      <td>635212</td>\n",
       "      <td>30647</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.769815</td>\n",
       "      <td>0.772013</td>\n",
       "      <td>0.770913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.398354</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>70465</td>\n",
       "      <td>56074</td>\n",
       "      <td>0.795771</td>\n",
       "      <td>0.661071</td>\n",
       "      <td>0.804511</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.807547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.841936</td>\n",
       "      <td>0.747934</td>\n",
       "      <td>0.685606</td>\n",
       "      <td>0.715415</td>\n",
       "      <td>67738</td>\n",
       "      <td>53817</td>\n",
       "      <td>0.794488</td>\n",
       "      <td>1.070783</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.602273</td>\n",
       "      <td>0.602273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.687705</td>\n",
       "      <td>0.760487</td>\n",
       "      <td>0.771978</td>\n",
       "      <td>0.766190</td>\n",
       "      <td>169960</td>\n",
       "      <td>134207</td>\n",
       "      <td>0.789639</td>\n",
       "      <td>0.785082</td>\n",
       "      <td>0.717277</td>\n",
       "      <td>0.752747</td>\n",
       "      <td>0.734584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.711184</td>\n",
       "      <td>0.708223</td>\n",
       "      <td>0.733516</td>\n",
       "      <td>0.720648</td>\n",
       "      <td>189323</td>\n",
       "      <td>149728</td>\n",
       "      <td>0.790860</td>\n",
       "      <td>0.855092</td>\n",
       "      <td>0.660053</td>\n",
       "      <td>0.685440</td>\n",
       "      <td>0.672507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.684347</td>\n",
       "      <td>0.859273</td>\n",
       "      <td>0.858396</td>\n",
       "      <td>0.858834</td>\n",
       "      <td>1408210</td>\n",
       "      <td>532876</td>\n",
       "      <td>0.378407</td>\n",
       "      <td>0.833230</td>\n",
       "      <td>0.831424</td>\n",
       "      <td>0.829219</td>\n",
       "      <td>0.830320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.603371</td>\n",
       "      <td>0.835458</td>\n",
       "      <td>0.842277</td>\n",
       "      <td>0.838854</td>\n",
       "      <td>1392222</td>\n",
       "      <td>527252</td>\n",
       "      <td>0.378713</td>\n",
       "      <td>0.796401</td>\n",
       "      <td>0.778161</td>\n",
       "      <td>0.782289</td>\n",
       "      <td>0.780220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.728607</td>\n",
       "      <td>0.834366</td>\n",
       "      <td>0.835183</td>\n",
       "      <td>0.834774</td>\n",
       "      <td>645873</td>\n",
       "      <td>335697</td>\n",
       "      <td>0.519757</td>\n",
       "      <td>0.944422</td>\n",
       "      <td>0.781483</td>\n",
       "      <td>0.787859</td>\n",
       "      <td>0.784658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.591326</td>\n",
       "      <td>0.813976</td>\n",
       "      <td>0.832572</td>\n",
       "      <td>0.823169</td>\n",
       "      <td>733988</td>\n",
       "      <td>383005</td>\n",
       "      <td>0.521814</td>\n",
       "      <td>0.812356</td>\n",
       "      <td>0.742502</td>\n",
       "      <td>0.759465</td>\n",
       "      <td>0.750887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.779327</td>\n",
       "      <td>0.823687</td>\n",
       "      <td>0.830354</td>\n",
       "      <td>0.827007</td>\n",
       "      <td>771367</td>\n",
       "      <td>331462</td>\n",
       "      <td>0.429707</td>\n",
       "      <td>1.172527</td>\n",
       "      <td>0.715243</td>\n",
       "      <td>0.720067</td>\n",
       "      <td>0.717647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.655074</td>\n",
       "      <td>0.797738</td>\n",
       "      <td>0.808769</td>\n",
       "      <td>0.803216</td>\n",
       "      <td>839256</td>\n",
       "      <td>359935</td>\n",
       "      <td>0.428874</td>\n",
       "      <td>1.182004</td>\n",
       "      <td>0.621151</td>\n",
       "      <td>0.625970</td>\n",
       "      <td>0.623551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.818197</td>\n",
       "      <td>0.773842</td>\n",
       "      <td>0.809117</td>\n",
       "      <td>0.791086</td>\n",
       "      <td>172438</td>\n",
       "      <td>131762</td>\n",
       "      <td>0.764112</td>\n",
       "      <td>0.960369</td>\n",
       "      <td>0.743056</td>\n",
       "      <td>0.762108</td>\n",
       "      <td>0.752461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.859494</td>\n",
       "      <td>0.685544</td>\n",
       "      <td>0.655271</td>\n",
       "      <td>0.670066</td>\n",
       "      <td>169161</td>\n",
       "      <td>129106</td>\n",
       "      <td>0.763214</td>\n",
       "      <td>0.990663</td>\n",
       "      <td>0.618287</td>\n",
       "      <td>0.606838</td>\n",
       "      <td>0.612509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.751312</td>\n",
       "      <td>0.813071</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.812785</td>\n",
       "      <td>394555</td>\n",
       "      <td>179439</td>\n",
       "      <td>0.454788</td>\n",
       "      <td>0.996556</td>\n",
       "      <td>0.744531</td>\n",
       "      <td>0.740871</td>\n",
       "      <td>0.742696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.674126</td>\n",
       "      <td>0.778098</td>\n",
       "      <td>0.758427</td>\n",
       "      <td>0.768137</td>\n",
       "      <td>418037</td>\n",
       "      <td>190324</td>\n",
       "      <td>0.455280</td>\n",
       "      <td>0.894702</td>\n",
       "      <td>0.674286</td>\n",
       "      <td>0.662921</td>\n",
       "      <td>0.668555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.964477</td>\n",
       "      <td>0.630137</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.643357</td>\n",
       "      <td>16013</td>\n",
       "      <td>11024</td>\n",
       "      <td>0.688441</td>\n",
       "      <td>1.089440</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1.386095</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15997</td>\n",
       "      <td>10999</td>\n",
       "      <td>0.687566</td>\n",
       "      <td>1.386100</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.390244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.489250</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.832685</td>\n",
       "      <td>0.834308</td>\n",
       "      <td>64518</td>\n",
       "      <td>48917</td>\n",
       "      <td>0.758192</td>\n",
       "      <td>0.709256</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.778210</td>\n",
       "      <td>0.767754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.705437</td>\n",
       "      <td>0.758475</td>\n",
       "      <td>0.696498</td>\n",
       "      <td>0.726166</td>\n",
       "      <td>63833</td>\n",
       "      <td>48345</td>\n",
       "      <td>0.757367</td>\n",
       "      <td>0.961214</td>\n",
       "      <td>0.608163</td>\n",
       "      <td>0.579767</td>\n",
       "      <td>0.593625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>0.732145</td>\n",
       "      <td>0.853352</td>\n",
       "      <td>0.850975</td>\n",
       "      <td>0.852162</td>\n",
       "      <td>1142383</td>\n",
       "      <td>926865</td>\n",
       "      <td>0.811343</td>\n",
       "      <td>1.041132</td>\n",
       "      <td>0.785631</td>\n",
       "      <td>0.781801</td>\n",
       "      <td>0.783711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>0.609784</td>\n",
       "      <td>0.805231</td>\n",
       "      <td>0.800371</td>\n",
       "      <td>0.802794</td>\n",
       "      <td>1094383</td>\n",
       "      <td>887307</td>\n",
       "      <td>0.810783</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.688540</td>\n",
       "      <td>0.672238</td>\n",
       "      <td>0.680291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1   l2     l1-name    l2-name  model_type  eval_loss_before  \\\n",
       "0    fr   br      french     breton        bert          1.420322   \n",
       "1    fr   br      french     breton  xlmroberta          1.365322   \n",
       "2    ar   fa      arabic    persian        bert          0.828790   \n",
       "3    ar   fa      arabic    persian  xlmroberta          0.665573   \n",
       "4    ar   hi      arabic      hindi        bert          1.253681   \n",
       "5    ar   hi      arabic      hindi  xlmroberta          0.779259   \n",
       "6    en  sco     english      scots        bert          0.795869   \n",
       "7    en  sco     english      scots  xlmroberta          1.095744   \n",
       "8    en   cy     english      welsh        bert          1.425001   \n",
       "9    en   cy     english      welsh  xlmroberta          1.574561   \n",
       "10   es   ca     spanish    catalan        bert          0.956130   \n",
       "11   es   ca     spanish    catalan  xlmroberta          1.399935   \n",
       "12   cs   sk       czech     slovak        bert          0.918968   \n",
       "13   cs   sk       czech     slovak  xlmroberta          0.722836   \n",
       "14   id   ms  indonesian      malay        bert          0.943245   \n",
       "15   id   ms  indonesian      malay  xlmroberta          0.744757   \n",
       "16   fr   oc      french    occitan        bert          1.199417   \n",
       "17   fr   oc      french    occitan  xlmroberta          1.084407   \n",
       "18   nl   af       dutch    african        bert          1.150736   \n",
       "19   nl   af       dutch    african  xlmroberta          0.901855   \n",
       "20   it  scn     italian   sicilian        bert          1.541083   \n",
       "21   it  scn     italian   sicilian  xlmroberta          1.498948   \n",
       "22   es   an     spanish  aragonese        bert          0.763670   \n",
       "23   es   an     spanish  aragonese  xlmroberta          0.741497   \n",
       "24   es  ast     spanish   asturian        bert          0.777532   \n",
       "25   es  ast     spanish   asturian  xlmroberta          0.761837   \n",
       "26   br   br      breton     breton        bert          0.935757   \n",
       "27   br   br      breton     breton  xlmroberta          0.971476   \n",
       "28   fa   fa     persian    persian        bert          0.836097   \n",
       "29   fa   fa     persian    persian  xlmroberta          0.627099   \n",
       "30   hi   hi       hindi      hindi        bert          0.854561   \n",
       "31   hi   hi       hindi      hindi  xlmroberta          0.685936   \n",
       "32  sco  sco       scots      scots        bert          0.398354   \n",
       "33  sco  sco       scots      scots  xlmroberta          0.841936   \n",
       "34   cy   cy       welsh      welsh        bert          0.687705   \n",
       "35   cy   cy       welsh      welsh  xlmroberta          0.711184   \n",
       "36   ca   ca     catalan    catalan        bert          0.684347   \n",
       "37   ca   ca     catalan    catalan  xlmroberta          0.603371   \n",
       "38   sk   sk      slovak     slovak        bert          0.728607   \n",
       "39   sk   sk      slovak     slovak  xlmroberta          0.591326   \n",
       "40   ms   ms       malay      malay        bert          0.779327   \n",
       "41   ms   ms       malay      malay  xlmroberta          0.655074   \n",
       "42   oc   oc     occitan    occitan        bert          0.818197   \n",
       "43   oc   oc     occitan    occitan  xlmroberta          0.859494   \n",
       "44   af   af     african    african        bert          0.751312   \n",
       "45   af   af     african    african  xlmroberta          0.674126   \n",
       "46  scn  scn    sicilian   sicilian        bert          0.964477   \n",
       "47  scn  scn    sicilian   sicilian  xlmroberta          1.386095   \n",
       "48   an   an   aragonese  aragonese        bert          0.489250   \n",
       "49   an   an   aragonese  aragonese  xlmroberta          0.705437   \n",
       "50  ast  ast    asturian   asturian        bert          0.732145   \n",
       "51  ast  ast    asturian   asturian  xlmroberta          0.609784   \n",
       "\n",
       "    precision_before  recall_before  f1_score_before  total_O_l2  same_O_l2  \\\n",
       "0           0.660287       0.671533         0.665862       96782       9197   \n",
       "1           0.615183       0.571776         0.592686       95428       9007   \n",
       "2           0.795459       0.790377         0.792910     1370517     200844   \n",
       "3           0.780644       0.779397         0.780020     1516814     227707   \n",
       "4           0.642059       0.629224         0.635577      509206      10813   \n",
       "5           0.756885       0.758686         0.757785      635212      12554   \n",
       "6           0.862069       0.852273         0.857143       70465      20591   \n",
       "7           0.776000       0.734848         0.754864       67738      19776   \n",
       "8           0.665775       0.684066         0.674797      169960      29425   \n",
       "9           0.611036       0.623626         0.617267      189323      32342   \n",
       "10          0.790754       0.795756         0.793247     1408210     241133   \n",
       "11          0.724187       0.731279         0.727716     1392222     239396   \n",
       "12          0.803396       0.802872         0.803134      645873     156704   \n",
       "13          0.795833       0.810379         0.803040      733988     177465   \n",
       "14          0.804444       0.793592         0.798981      771367     280302   \n",
       "15          0.792069       0.774705         0.783291      839256     304953   \n",
       "16          0.759441       0.773504         0.766408      172438      39816   \n",
       "17          0.672993       0.656695         0.664744      169161      38982   \n",
       "18          0.784011       0.785112         0.784561      394555      89272   \n",
       "19          0.755994       0.752809         0.754398      418037      94369   \n",
       "20          0.720588       0.700000         0.710145       16013       4162   \n",
       "21          0.630769       0.585714         0.607407       15997       4137   \n",
       "22          0.854406       0.867704         0.861004       64518      16005   \n",
       "23          0.772549       0.766537         0.769531       63833      15812   \n",
       "24          0.842130       0.840761         0.841445     1142383     338008   \n",
       "25          0.769825       0.763928         0.766865     1094383     324467   \n",
       "26          0.727735       0.695864         0.711443       96782      26837   \n",
       "27          0.609254       0.576642         0.592500       95428      26398   \n",
       "28          0.817325       0.813735         0.815526     1370517     622926   \n",
       "29          0.814113       0.806149         0.810111     1516814     688212   \n",
       "30          0.740067       0.735840         0.737947      509206      25444   \n",
       "31          0.776934       0.779153         0.778042      635212      30647   \n",
       "32          0.886364       0.886364         0.886364       70465      56074   \n",
       "33          0.747934       0.685606         0.715415       67738      53817   \n",
       "34          0.760487       0.771978         0.766190      169960     134207   \n",
       "35          0.708223       0.733516         0.720648      189323     149728   \n",
       "36          0.859273       0.858396         0.858834     1408210     532876   \n",
       "37          0.835458       0.842277         0.838854     1392222     527252   \n",
       "38          0.834366       0.835183         0.834774      645873     335697   \n",
       "39          0.813976       0.832572         0.823169      733988     383005   \n",
       "40          0.823687       0.830354         0.827007      771367     331462   \n",
       "41          0.797738       0.808769         0.803216      839256     359935   \n",
       "42          0.773842       0.809117         0.791086      172438     131762   \n",
       "43          0.685544       0.655271         0.670066      169161     129106   \n",
       "44          0.813071       0.812500         0.812785      394555     179439   \n",
       "45          0.778098       0.758427         0.768137      418037     190324   \n",
       "46          0.630137       0.657143         0.643357       16013      11024   \n",
       "47          0.400000       0.400000         0.400000       15997      10999   \n",
       "48          0.835938       0.832685         0.834308       64518      48917   \n",
       "49          0.758475       0.696498         0.726166       63833      48345   \n",
       "50          0.853352       0.850975         0.852162     1142383     926865   \n",
       "51          0.805231       0.800371         0.802794     1094383     887307   \n",
       "\n",
       "    percentage_O_l2  eval_loss_after  precision_after  recall_after  \\\n",
       "0          0.095028         1.591118         0.637717      0.625304   \n",
       "1          0.094385         1.440016         0.556995      0.523114   \n",
       "2          0.146546         0.878958         0.775000      0.767419   \n",
       "3          0.150122         0.750748         0.738836      0.739868   \n",
       "4          0.021235         1.314276         0.635174      0.623989   \n",
       "5          0.019763         0.800377         0.754817      0.745835   \n",
       "6          0.292216         1.175082         0.766284      0.757576   \n",
       "7          0.291948         1.596816         0.637795      0.613636   \n",
       "8          0.173129         1.576923         0.629879      0.642857   \n",
       "9          0.170830         1.492930         0.574607      0.603022   \n",
       "10         0.171234         1.088810         0.760821      0.756784   \n",
       "11         0.171952         1.374298         0.660645      0.664150   \n",
       "12         0.242624         1.147653         0.755454      0.757180   \n",
       "13         0.241782         0.954391         0.729835      0.735313   \n",
       "14         0.363383         1.419727         0.684441      0.683980   \n",
       "15         0.363361         1.342536         0.587833      0.580101   \n",
       "16         0.230900         1.324535         0.723343      0.715100   \n",
       "17         0.230443         1.231245         0.595652      0.585470   \n",
       "18         0.226260         1.525563         0.723618      0.707865   \n",
       "19         0.225743         1.152242         0.655076      0.661517   \n",
       "20         0.259914         1.896788         0.681818      0.642857   \n",
       "21         0.258611         1.900114         0.537037      0.414286   \n",
       "22         0.248070         1.071652         0.755725      0.770428   \n",
       "23         0.247709         1.195259         0.563265      0.536965   \n",
       "24         0.295880         1.077213         0.775890      0.773909   \n",
       "25         0.296484         1.234623         0.603276      0.589833   \n",
       "26         0.277293         1.047752         0.680412      0.642336   \n",
       "27         0.276627         1.070033         0.553922      0.549878   \n",
       "28         0.454519         0.905204         0.795037      0.786784   \n",
       "29         0.453722         0.742005         0.770838      0.758834   \n",
       "30         0.049968         0.903202         0.724122      0.725845   \n",
       "31         0.048247         0.705056         0.769815      0.772013   \n",
       "32         0.795771         0.661071         0.804511      0.810606   \n",
       "33         0.794488         1.070783         0.602273      0.602273   \n",
       "34         0.789639         0.785082         0.717277      0.752747   \n",
       "35         0.790860         0.855092         0.660053      0.685440   \n",
       "36         0.378407         0.833230         0.831424      0.829219   \n",
       "37         0.378713         0.796401         0.778161      0.782289   \n",
       "38         0.519757         0.944422         0.781483      0.787859   \n",
       "39         0.521814         0.812356         0.742502      0.759465   \n",
       "40         0.429707         1.172527         0.715243      0.720067   \n",
       "41         0.428874         1.182004         0.621151      0.625970   \n",
       "42         0.764112         0.960369         0.743056      0.762108   \n",
       "43         0.763214         0.990663         0.618287      0.606838   \n",
       "44         0.454788         0.996556         0.744531      0.740871   \n",
       "45         0.455280         0.894702         0.674286      0.662921   \n",
       "46         0.688441         1.089440         0.603175      0.542857   \n",
       "47         0.687566         1.386100         0.452830      0.342857   \n",
       "48         0.758192         0.709256         0.757576      0.778210   \n",
       "49         0.757367         0.961214         0.608163      0.579767   \n",
       "50         0.811343         1.041132         0.785631      0.781801   \n",
       "51         0.810783         0.943498         0.688540      0.672238   \n",
       "\n",
       "    f1_score_after  \n",
       "0         0.631450  \n",
       "1         0.539523  \n",
       "2         0.771191  \n",
       "3         0.739352  \n",
       "4         0.629532  \n",
       "5         0.750299  \n",
       "6         0.761905  \n",
       "7         0.625483  \n",
       "8         0.636302  \n",
       "9         0.588472  \n",
       "10        0.758797  \n",
       "11        0.662393  \n",
       "12        0.756316  \n",
       "13        0.732564  \n",
       "14        0.684211  \n",
       "15        0.583942  \n",
       "16        0.719198  \n",
       "17        0.590517  \n",
       "18        0.715655  \n",
       "19        0.658281  \n",
       "20        0.661765  \n",
       "21        0.467742  \n",
       "22        0.763006  \n",
       "23        0.549801  \n",
       "24        0.774898  \n",
       "25        0.596479  \n",
       "26        0.660826  \n",
       "27        0.551893  \n",
       "28        0.790889  \n",
       "29        0.764789  \n",
       "30        0.724982  \n",
       "31        0.770913  \n",
       "32        0.807547  \n",
       "33        0.602273  \n",
       "34        0.734584  \n",
       "35        0.672507  \n",
       "36        0.830320  \n",
       "37        0.780220  \n",
       "38        0.784658  \n",
       "39        0.750887  \n",
       "40        0.717647  \n",
       "41        0.623551  \n",
       "42        0.752461  \n",
       "43        0.612509  \n",
       "44        0.742696  \n",
       "45        0.668555  \n",
       "46        0.571429  \n",
       "47        0.390244  \n",
       "48        0.767754  \n",
       "49        0.593625  \n",
       "50        0.783711  \n",
       "51        0.680291  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "title_perturb_cosine=pd.read_csv('title_perturb_cosine6',sep=',', encoding='utf-8')\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test5',sep=',', encoding='utf-8')\n",
    "\n",
    "perturb_5_compare=language_accuracy.merge(title_perturb_cosine,on=['l1','l2','l1-name','l2-name','model_type'],suffixes=('_before', '_after'))\n",
    "perturb_5_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d3ced8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturb_5_compare.to_csv('perturb_5_compare_cosine',sep=',', encoding='utf-8',index=False)\n",
    "perturb_5_compare.to_excel('perturb_5_compare_cosine.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "507ed4c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perturb_5_compare=pd.read_csv('perturb_5_compare_cosine',sep=',', encoding='utf-8')\n",
    "# perturb_5_compare\n",
    "# both_perturb_compare=perturb_1_compare.merge(perturb_2_compare,on=['l1','l2','l1-name','l2-name','model_type'],suffixes=('_1', '_2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33890bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_lang_info=perturb_5_compare\n",
    "# all_lang_info=pd.read_csv('train_test_perturb_1_compare',sep=',', encoding='utf-8')\n",
    "# all_lang_info[['l1','l2','l1-name','l2-name','model_type','f1_score_before','f1_score_after']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af291d90",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7f6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"\n",
    "run=1\n",
    "import pandas as pd\n",
    "language_accuracy=pd.read_csv('all_langs_overlap_train_test5', sep=',', encoding='utf-8')\n",
    "# language_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15cc43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.disable(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60291499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "br\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df852e22590e4b61832c7b2e58c8f4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474f0ba18a1749fabca4ba8858dc848e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'bert', 96782, 9197, 0.09502800107457998, 1.6766805672178082, 0.5979899497487438, 0.5790754257907542, 0.588380716934487]\n",
      "fr\n",
      "br\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad88a22dd894532ad4f4ee8b0f4c674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d60bfa44a348dda46747b67a85f009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'br', 'french', 'breton', 'xlmroberta', 95428, 9007, 0.09438529572033365, 1.4542133130279242, 0.5552825552825553, 0.5498783454987834, 0.5525672371638142]\n",
      "ar\n",
      "fa\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af2106ba4e842d591709da88301e923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f1cf68898c4be9b590e815085ec96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'fa', 'arabic', 'persian', 'bert', 1370517, 200844, 0.14654615739899615, 0.9852651148319245, 0.7408597830453998, 0.7362747055300459, 0.7385601281666166]\n",
      "ar\n",
      "fa\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be74365ae14ab28a123f700b1ca888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ecd6f6dd834d66a242023d95c0fe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'fa', 'arabic', 'persian', 'xlmroberta', 1516814, 227707, 0.1501219002461739, 0.7949778755187988, 0.7307383627608347, 0.7270912357756039, 0.7289102371660162]\n",
      "ar\n",
      "hi\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17e227ea14641939e585da933f1eb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf17476ea4a47438f41cf37b2814191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'hi', 'arabic', 'hindi', 'bert', 509206, 10813, 0.021235020797084087, 1.3027956535045366, 0.6308209313490158, 0.6254164683484055, 0.6281070745697895]\n",
      "ar\n",
      "hi\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d868094ff784434bfe310cee747f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a7cb827b994bd7bec48c63802d4047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ar', 'hi', 'arabic', 'hindi', 'xlmroberta', 635212, 12554, 0.019763480538780753, 0.8011177899245929, 0.7543186180422264, 0.7482151356496907, 0.7512544802867385]\n",
      "en\n",
      "sco\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d90ebf20ca436c8a42acd96d93a6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9f2975d5d4ce8b72efcc9fffe4b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'sco', 'english', 'scots', 'bert', 70465, 20591, 0.2922159937557653, 1.3037817534059286, 0.7388059701492538, 0.75, 0.7443609022556392]\n",
      "en\n",
      "sco\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2706b06d584580838b462e40dd4a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccd179e3fb14e6595ea1031c203d638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'sco', 'english', 'scots', 'xlmroberta', 67738, 19776, 0.2919483893826213, 1.7929965816438198, 0.5968379446640316, 0.571969696969697, 0.5841392649903289]\n",
      "en\n",
      "cy\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34517de8e9eb4213889925cc0c815173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc1325b97d24f968ed4ff00419b3ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'cy', 'english', 'welsh', 'bert', 169960, 29425, 0.17312897152271123, 1.6042741828908522, 0.6266846361185984, 0.6387362637362637, 0.6326530612244898]\n",
      "en\n",
      "cy\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0684c34650764c688bc504a06a236038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fec84ebe8349d092e060773b140a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en', 'cy', 'english', 'welsh', 'xlmroberta', 189323, 32342, 0.1708297459896579, 1.4944494074831407, 0.5614266842800528, 0.5837912087912088, 0.5723905723905724]\n",
      "es\n",
      "ca\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bb053f47b748de9291450fbf2b23e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fc2baba09944829270550e4e934191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ca', 'spanish', 'catalan', 'bert', 1408210, 241133, 0.17123369383827697, 1.2921902175801687, 0.7166090668835129, 0.7192409712303611, 0.7179226069246435]\n",
      "es\n",
      "ca\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0af4194f18d4c9ab0dde3b976b4bda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700bb1d91d48414ba92ba968f9711f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ca', 'spanish', 'catalan', 'xlmroberta', 1392222, 239396, 0.17195246160454294, 1.5463007787863414, 0.6545820745216515, 0.6631299734748011, 0.658828299209406]\n",
      "cs\n",
      "sk\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27124ab8a7f1412f8a6695fa1d54692b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d28f920a988494ab6fd9389a4387ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'sk', 'czech', 'slovak', 'bert', 645873, 156704, 0.24262354983100393, 1.4607958604840083, 0.6802194256211681, 0.6879895561357703, 0.6840824273892584]\n",
      "cs\n",
      "sk\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdc9645497b4fde8015555c27f37d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c21d098a97405392877c38753d8a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs', 'sk', 'czech', 'slovak', 'xlmroberta', 733988, 177465, 0.24178188199262113, 1.0967904330860259, 0.6846526655896608, 0.691579634464752, 0.688098717324241]\n",
      "id\n",
      "ms\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c2bb4c06f7436d94cc427d6a65b736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2428e7f696b413ba1e792937acda6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'ms', 'indonesian', 'malay', 'bert', 771367, 280302, 0.3633834478270395, 1.8375903079088063, 0.6075993091537133, 0.5932546374367622, 0.6003412969283276]\n",
      "id\n",
      "ms\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319cf725e0e142a8a3ce0bba41e69a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e797f2dc92144f11932fd111503c7056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'ms', 'indonesian', 'malay', 'xlmroberta', 839256, 304953, 0.3633611198490091, 1.4636742578859026, 0.5529531568228105, 0.5494097807757167, 0.5511757739807139]\n",
      "fr\n",
      "oc\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c98e8e65e1b4e9d989e38c426b4e846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f1a1a84b8648a4bc6ade0c8fee8b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'oc', 'french', 'occitan', 'bert', 172438, 39816, 0.23090038158642526, 1.5236528502113518, 0.6839080459770115, 0.6780626780626781, 0.6809728183118742]\n",
      "fr\n",
      "oc\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ae9e08c5494d54b1778ee637b878f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be31b1170eb498b9c3bf893d48f1845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fr', 'oc', 'french', 'occitan', 'xlmroberta', 169161, 38982, 0.2304431872594747, 1.2645269505593968, 0.5825958702064897, 0.5626780626780626, 0.572463768115942]\n",
      "nl\n",
      "af\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b1f121ac1247af9b2a485c82d0d521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffe9cabdafe460ea95d0db3be5b2720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nl', 'af', 'dutch', 'african', 'bert', 394555, 89272, 0.22625996375663723, 1.8404947738389712, 0.6502428868841083, 0.6580056179775281, 0.6541012216404887]\n",
      "nl\n",
      "af\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e5bf82ade744588deecf7a00620fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fdef3dbbb24312b700da53fa48201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nl', 'af', 'dutch', 'african', 'xlmroberta', 418037, 94369, 0.2257431758432866, 1.3172787421458476, 0.6085434173669467, 0.610252808988764, 0.6093969144460027]\n",
      "it\n",
      "scn\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a910c80dbeb4c2fb3c1d81b97f3e6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13895d2c989c4b5ca648a4129cd0423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'scn', 'italian', 'sicilian', 'bert', 16013, 4162, 0.25991382002123276, 1.914520886209276, 0.6323529411764706, 0.6142857142857143, 0.6231884057971014]\n",
      "it\n",
      "scn\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2a464c2c5649a18f74257b483e3a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2e18a73f324f0eb1cb3a420d9c2514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'scn', 'italian', 'sicilian', 'xlmroberta', 15997, 4137, 0.2586109895605426, 1.8947719070646498, 0.5263157894736842, 0.42857142857142855, 0.4724409448818898]\n",
      "es\n",
      "an\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8680dce9aa4d4a47bda6ae8913f98240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f19f973e1c4fb0bc4b45978ffc74a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'an', 'spanish', 'aragonese', 'bert', 64518, 16005, 0.24807030596112714, 1.2337825344875455, 0.7115384615384616, 0.7198443579766537, 0.7156673114119924]\n",
      "es\n",
      "an\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2800d7b0344a7fad724802df4dad47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f3482a1a5b49c9af63e10d7040f6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'an', 'spanish', 'aragonese', 'xlmroberta', 63833, 15812, 0.24770886532044553, 1.2377831526100636, 0.5720164609053497, 0.5408560311284046, 0.5559999999999998]\n",
      "es\n",
      "ast\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e97902055c54a81999ab047a7b2a951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712a0db482764f02ade14e0b123c0c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ast', 'spanish', 'asturian', 'bert', 1142383, 338008, 0.2958797531125726, 1.4537998140416342, 0.6918831915382847, 0.6984679665738162, 0.6951599861383851]\n",
      "es\n",
      "ast\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b4ddf37fd4f5487fbdcf956f2c728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "convert examples to features: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1633a477a5f0427e86b1292c3df77c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['es', 'ast', 'spanish', 'asturian', 'xlmroberta', 1094383, 324467, 0.2964839548859951, 1.3666101516074456, 0.5650444548432382, 0.5605849582172702, 0.5628058727569331]\n",
      "br\n",
      "br\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101b192b9b1e47d29aa4d2a508a989ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'br', 'breton', 'breton', 'bert', 96782, 26837, 0.2772932983406005, 1.0657697799159032, 0.660759493670886, 0.635036496350365, 0.6476426799007444]\n",
      "br\n",
      "br\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b7e8d1a0c2482bbaa937f937a658fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['br', 'br', 'breton', 'breton', 'xlmroberta', 95428, 26398, 0.2766274049545207, 1.0775888760884602, 0.5793450881612091, 0.559610705596107, 0.5693069306930694]\n",
      "fa\n",
      "fa\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5035ea3e0d5d4583b75daf447adf15e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fa', 'fa', 'persian', 'persian', 'bert', 1370517, 622926, 0.454518988089896, 1.0255508167028426, 0.7705575611712796, 0.767019365142743, 0.768784392196098]\n",
      "fa\n",
      "fa\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494d1089003840ceb7fbcc93552b19a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fa', 'fa', 'persian', 'persian', 'xlmroberta', 1516814, 688212, 0.45372207798714936, 0.7899912783384323, 0.7568937550689375, 0.7452585346376522, 0.7510310833920129]\n",
      "hi\n",
      "hi\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95e9f0daad34f928d413160ffc30858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'hindi', 'hindi', 'bert', 509206, 25444, 0.04996798937954384, 0.8929560320045715, 0.7283653846153846, 0.7210851975249881, 0.7247070078928486]\n",
      "hi\n",
      "hi\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb0ea37081e4fdb894c199493ec156e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hi', 'hindi', 'hindi', 'xlmroberta', 635212, 30647, 0.04824688450470079, 0.7060413956642151, 0.7663068981698733, 0.7772489290813898, 0.7717391304347826]\n",
      "sco\n",
      "sco\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c03745c1554852945a27a70c521f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sco', 'sco', 'scots', 'scots', 'bert', 70465, 56074, 0.7957709501170794, 0.762499482370913, 0.7785977859778598, 0.7992424242424242, 0.788785046728972]\n",
      "sco\n",
      "sco\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbb78e7d8234dc0b3d44bf5d62f7ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sco', 'sco', 'scots', 'scots', 'xlmroberta', 67738, 53817, 0.7944875845168148, 1.1076788399368525, 0.5968379446640316, 0.571969696969697, 0.5841392649903289]\n",
      "cy\n",
      "cy\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9e4e949bd745e88c4e3cf57f8d2bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cy', 'cy', 'welsh', 'welsh', 'bert', 169960, 134207, 0.7896387385267122, 0.8372502454246084, 0.7177097203728362, 0.7403846153846154, 0.7288708586883029]\n",
      "cy\n",
      "cy\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b0146603bf4f05b99da69984d4ec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cy', 'cy', 'welsh', 'welsh', 'xlmroberta', 189323, 149728, 0.7908600645457763, 0.8997286303589741, 0.6299212598425197, 0.6593406593406593, 0.6442953020134228]\n",
      "ca\n",
      "ca\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046af51b1eca41518970f492bb585aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', 'ca', 'catalan', 'catalan', 'bert', 1408210, 532876, 0.37840662969301453, 0.9968777566742247, 0.8013965906757035, 0.7961640481534381, 0.7987717502558853]\n",
      "ca\n",
      "ca\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f657add9e04eee9307fcb1890575f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ca', 'ca', 'catalan', 'catalan', 'xlmroberta', 1392222, 527252, 0.3787125903771094, 0.8517588299746888, 0.7620984139894266, 0.7647418894103244, 0.7634178633262041]\n",
      "sk\n",
      "sk\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015ba45aa09d4e2da581b0ffe8c87d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk', 'sk', 'slovak', 'slovak', 'bert', 645873, 335697, 0.5197569800874166, 1.2147542753924874, 0.7201672563525249, 0.7307441253263708, 0.725417139154382]\n",
      "sk\n",
      "sk\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc12d4d6603e4379ac286b8f359ad562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sk', 'sk', 'slovak', 'slovak', 'xlmroberta', 733988, 383005, 0.5218137081260185, 0.9311451255026912, 0.6972767574414186, 0.7186684073107049, 0.7078109932497588]\n",
      "ms\n",
      "ms\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1cd63f15434bec8fe125e1626aabd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', 'ms', 'malay', 'malay', 'bert', 771367, 331462, 0.4297072599683419, 1.5317738900218714, 0.6339979722879351, 0.6327150084317033, 0.6333558406482106]\n",
      "ms\n",
      "ms\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8d054e71e04d96863f535d6c26fd4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ms', 'ms', 'malay', 'malay', 'xlmroberta', 839256, 359935, 0.4288739073655714, 1.2849430009088618, 0.5923822251921149, 0.5979763912310286, 0.595166163141994]\n",
      "oc\n",
      "oc\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59119ef1433d454e8b6a4187352e7516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oc', 'oc', 'occitan', 'occitan', 'bert', 172438, 131762, 0.7641123186304642, 1.1273392277887497, 0.7, 0.707977207977208, 0.7039660056657223]\n",
      "oc\n",
      "oc\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e15facce244073bcfa2fb5ad58b566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oc', 'oc', 'occitan', 'occitan', 'xlmroberta', 169161, 129106, 0.7632137431204592, 1.033921749427401, 0.5955223880597015, 0.5683760683760684, 0.5816326530612246]\n",
      "af\n",
      "af\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76e098bf62a4ce3981af031cf96c90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', 'af', 'african', 'african', 'bert', 394555, 179439, 0.4547883058128778, 1.2131793423278912, 0.6830294530154277, 0.6839887640449438, 0.6835087719298245]\n",
      "af\n",
      "af\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c0b5cdb1a04dd6ab5d0a7a7c468fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['af', 'af', 'african', 'african', 'xlmroberta', 418037, 190324, 0.4552802742340989, 1.0087024741881603, 0.6282051282051282, 0.6193820224719101, 0.6237623762376238]\n",
      "scn\n",
      "scn\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb19ebf0a57479b9327ef4a84caadc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scn', 'scn', 'sicilian', 'sicilian', 'bert', 16013, 11024, 0.6884406419783926, 1.158578634262085, 0.5357142857142857, 0.42857142857142855, 0.47619047619047616]\n",
      "scn\n",
      "scn\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7c2024956c4a3aa8d0a5b90ca847a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scn', 'scn', 'sicilian', 'sicilian', 'xlmroberta', 15997, 10999, 0.6875664187035069, 1.386136531829834, 0.43661971830985913, 0.44285714285714284, 0.43971631205673756]\n",
      "an\n",
      "an\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a34cccf0f6a4ee790de8f006b79616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'an', 'aragonese', 'aragonese', 'bert', 64518, 48917, 0.758191512446139, 0.828799931332469, 0.7269076305220884, 0.7042801556420234, 0.7154150197628459]\n",
      "an\n",
      "an\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af01aa3bb0af44dd83f3b2909c629b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'an', 'aragonese', 'aragonese', 'xlmroberta', 63833, 48345, 0.7573668792004136, 1.016292380169034, 0.5776892430278885, 0.5642023346303502, 0.5708661417322834]\n",
      "ast\n",
      "ast\n",
      "bert\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d065b023d908479f9fb69e3926fbecd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ast', 'ast', 'asturian', 'asturian', 'bert', 1142383, 926865, 0.8113434811267325, 1.3339503923717062, 0.7171575502571295, 0.7121634168987929, 0.7146517586769159]\n",
      "ast\n",
      "ast\n",
      "xlmroberta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91824787ce54f63b34ca68c5759f0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/s/babbage/b/nobackup/nblancha/merry/conda/envs/mbert_ner/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: A seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ast', 'ast', 'asturian', 'asturian', 'xlmroberta', 1094383, 887307, 0.8107828794855183, 1.086593729335717, 0.6464742072882158, 0.6341689879294337, 0.6402624794937896]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# title_perturb_random=pd.read_csv('title_perturb_random',sep=',', encoding='utf-8')\n",
    "title_perturb_random=pd.DataFrame(columns=[\"l1\",\"l2\",\"l1-name\",\"l2-name\",\"model_type\",\n",
    "                                           \"total_O_l2\",\"same_O_l2\",\"percentage_O_l2\",\n",
    "                                    \"eval_loss\",\"precision\",\"recall\",\"f1_score\"])\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import BertForMultipleChoice\n",
    "from transformers import XLMRobertaForMultipleChoice\n",
    "from SimpleTransformers import titleModel\n",
    "perturb_type='random_text'\n",
    "\n",
    "for l_index,lang2 in language_accuracy.iterrows():\n",
    "        language_source=lang2['l1']\n",
    "        language_target=lang2['l2']\n",
    "        print(language_source)\n",
    "        print(language_target)\n",
    "#         with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "#             target_test=pickle.load(file)\n",
    "        model_type=lang2['model_type']\n",
    "        print(model_type)\n",
    "#         with open(path+language_target+'/title_test.pkl','rb') as file:\n",
    "#             target_test=pickle.load(file)\n",
    "        with open(path+language_target+'/128_title_test_examples_with_'+model_type+'.pkl','rb') as file:\n",
    "            target_test=pickle.load(file)\n",
    "                \n",
    "        testdataset=target_test\n",
    "        if lang2['model_type']=='bert':\n",
    "#                 if not ((title_perturb_random['l1']==language_source)&(title_perturb_random['l2']==language_target)&(title_perturb_random['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_bert=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/bert'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'bert-base-multilingual-cased',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_bert, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=BertForMultipleChoice.from_pretrained(outputdir_bert)\n",
    "                     \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=True,perturb_type=perturb_type,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     out_overlap[\"all_O\"],out_overlap[\"same_O\"],out_overlap[\"same_O\"]/out_overlap[\"all_O\"],\n",
    "                      results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                print(info)\n",
    "                title_perturb_random.loc[l_index]=info\n",
    "\n",
    "\n",
    "        if lang2['model_type']=='xlmroberta':\n",
    "#             if not ((title_perturb_random['l1']==language_source)&(title_perturb_random['l2']==language_target)&(title_perturb_random['model_type']==lang2['model_type'])).any():\n",
    "                outputdir_xlmr=path+'/title_results_lan/'+language_source+'_'+language_target+'/{dataset_number}/xlmroberta'\n",
    "                model = titleModel(lang2['model_type'], \n",
    "                            'xlm-roberta-base',\n",
    "                            labels=['A', 'B', 'C', 'D'],\n",
    "                            use_cuda=True,\n",
    "                            args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir_xlmr, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False,'cache_dir':path+'/title_cache_dir'})\n",
    "                model.model=XLMRobertaForMultipleChoice.from_pretrained(outputdir_xlmr)\n",
    "                \n",
    "                out_overlap=dict()\n",
    "                results, model_outputs, preds_list,accuracy_result=model.eval_model(testdataset,lang=language_target,run=run,perturb=True,perturb_type=perturb_type,source_lang=language_source,out_overlap=out_overlap)\n",
    "\n",
    "                info=[lang2['l1'],lang2['l2'],lang2['l1-name'],lang2['l2-name'],lang2['model_type'],\n",
    "                     out_overlap[\"all_O\"],out_overlap[\"same_O\"],out_overlap[\"same_O\"]/out_overlap[\"all_O\"],\n",
    "                      results[\"eval_loss\"],results[\"precision\"],results[\"recall\"],results[\"f1_score\"]]\n",
    "                print(info)\n",
    "                title_perturb_random.loc[l_index]=info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30bddb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_perturb_random.to_csv('title_perturb_random5',sep=',', encoding='utf-8',index=False)\n",
    "title_perturb_random.to_excel('title_perturb_random5.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2436927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l1-name</th>\n",
       "      <th>l2-name</th>\n",
       "      <th>model_type</th>\n",
       "      <th>total_O_l2</th>\n",
       "      <th>same_O_l2</th>\n",
       "      <th>percentage_O_l2</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>96782</td>\n",
       "      <td>9197</td>\n",
       "      <td>0.095028</td>\n",
       "      <td>1.676681</td>\n",
       "      <td>0.597990</td>\n",
       "      <td>0.579075</td>\n",
       "      <td>0.588381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fr</td>\n",
       "      <td>br</td>\n",
       "      <td>french</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>95428</td>\n",
       "      <td>9007</td>\n",
       "      <td>0.094385</td>\n",
       "      <td>1.454213</td>\n",
       "      <td>0.555283</td>\n",
       "      <td>0.549878</td>\n",
       "      <td>0.552567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1370517</td>\n",
       "      <td>200844</td>\n",
       "      <td>0.146546</td>\n",
       "      <td>0.985265</td>\n",
       "      <td>0.740860</td>\n",
       "      <td>0.736275</td>\n",
       "      <td>0.738560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ar</td>\n",
       "      <td>fa</td>\n",
       "      <td>arabic</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1516814</td>\n",
       "      <td>227707</td>\n",
       "      <td>0.150122</td>\n",
       "      <td>0.794978</td>\n",
       "      <td>0.730738</td>\n",
       "      <td>0.727091</td>\n",
       "      <td>0.728910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>509206</td>\n",
       "      <td>10813</td>\n",
       "      <td>0.021235</td>\n",
       "      <td>1.302796</td>\n",
       "      <td>0.630821</td>\n",
       "      <td>0.625416</td>\n",
       "      <td>0.628107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ar</td>\n",
       "      <td>hi</td>\n",
       "      <td>arabic</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>635212</td>\n",
       "      <td>12554</td>\n",
       "      <td>0.019763</td>\n",
       "      <td>0.801118</td>\n",
       "      <td>0.754319</td>\n",
       "      <td>0.748215</td>\n",
       "      <td>0.751254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>70465</td>\n",
       "      <td>20591</td>\n",
       "      <td>0.292216</td>\n",
       "      <td>1.303782</td>\n",
       "      <td>0.738806</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.744361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>sco</td>\n",
       "      <td>english</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>67738</td>\n",
       "      <td>19776</td>\n",
       "      <td>0.291948</td>\n",
       "      <td>1.792997</td>\n",
       "      <td>0.596838</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>0.584139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>169960</td>\n",
       "      <td>29425</td>\n",
       "      <td>0.173129</td>\n",
       "      <td>1.604274</td>\n",
       "      <td>0.626685</td>\n",
       "      <td>0.638736</td>\n",
       "      <td>0.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>cy</td>\n",
       "      <td>english</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>189323</td>\n",
       "      <td>32342</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>1.494449</td>\n",
       "      <td>0.561427</td>\n",
       "      <td>0.583791</td>\n",
       "      <td>0.572391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>1408210</td>\n",
       "      <td>241133</td>\n",
       "      <td>0.171234</td>\n",
       "      <td>1.292190</td>\n",
       "      <td>0.716609</td>\n",
       "      <td>0.719241</td>\n",
       "      <td>0.717923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>es</td>\n",
       "      <td>ca</td>\n",
       "      <td>spanish</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1392222</td>\n",
       "      <td>239396</td>\n",
       "      <td>0.171952</td>\n",
       "      <td>1.546301</td>\n",
       "      <td>0.654582</td>\n",
       "      <td>0.663130</td>\n",
       "      <td>0.658828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>645873</td>\n",
       "      <td>156704</td>\n",
       "      <td>0.242624</td>\n",
       "      <td>1.460796</td>\n",
       "      <td>0.680219</td>\n",
       "      <td>0.687990</td>\n",
       "      <td>0.684082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cs</td>\n",
       "      <td>sk</td>\n",
       "      <td>czech</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>733988</td>\n",
       "      <td>177465</td>\n",
       "      <td>0.241782</td>\n",
       "      <td>1.096790</td>\n",
       "      <td>0.684653</td>\n",
       "      <td>0.691580</td>\n",
       "      <td>0.688099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>771367</td>\n",
       "      <td>280302</td>\n",
       "      <td>0.363383</td>\n",
       "      <td>1.837590</td>\n",
       "      <td>0.607599</td>\n",
       "      <td>0.593255</td>\n",
       "      <td>0.600341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>id</td>\n",
       "      <td>ms</td>\n",
       "      <td>indonesian</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>839256</td>\n",
       "      <td>304953</td>\n",
       "      <td>0.363361</td>\n",
       "      <td>1.463674</td>\n",
       "      <td>0.552953</td>\n",
       "      <td>0.549410</td>\n",
       "      <td>0.551176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>172438</td>\n",
       "      <td>39816</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>1.523653</td>\n",
       "      <td>0.683908</td>\n",
       "      <td>0.678063</td>\n",
       "      <td>0.680973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fr</td>\n",
       "      <td>oc</td>\n",
       "      <td>french</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>169161</td>\n",
       "      <td>38982</td>\n",
       "      <td>0.230443</td>\n",
       "      <td>1.264527</td>\n",
       "      <td>0.582596</td>\n",
       "      <td>0.562678</td>\n",
       "      <td>0.572464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>394555</td>\n",
       "      <td>89272</td>\n",
       "      <td>0.226260</td>\n",
       "      <td>1.840495</td>\n",
       "      <td>0.650243</td>\n",
       "      <td>0.658006</td>\n",
       "      <td>0.654101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nl</td>\n",
       "      <td>af</td>\n",
       "      <td>dutch</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>418037</td>\n",
       "      <td>94369</td>\n",
       "      <td>0.225743</td>\n",
       "      <td>1.317279</td>\n",
       "      <td>0.608543</td>\n",
       "      <td>0.610253</td>\n",
       "      <td>0.609397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>16013</td>\n",
       "      <td>4162</td>\n",
       "      <td>0.259914</td>\n",
       "      <td>1.914521</td>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.623188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>it</td>\n",
       "      <td>scn</td>\n",
       "      <td>italian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>15997</td>\n",
       "      <td>4137</td>\n",
       "      <td>0.258611</td>\n",
       "      <td>1.894772</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.472441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>64518</td>\n",
       "      <td>16005</td>\n",
       "      <td>0.248070</td>\n",
       "      <td>1.233783</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.719844</td>\n",
       "      <td>0.715667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>es</td>\n",
       "      <td>an</td>\n",
       "      <td>spanish</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>63833</td>\n",
       "      <td>15812</td>\n",
       "      <td>0.247709</td>\n",
       "      <td>1.237783</td>\n",
       "      <td>0.572016</td>\n",
       "      <td>0.540856</td>\n",
       "      <td>0.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1142383</td>\n",
       "      <td>338008</td>\n",
       "      <td>0.295880</td>\n",
       "      <td>1.453800</td>\n",
       "      <td>0.691883</td>\n",
       "      <td>0.698468</td>\n",
       "      <td>0.695160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>es</td>\n",
       "      <td>ast</td>\n",
       "      <td>spanish</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1094383</td>\n",
       "      <td>324467</td>\n",
       "      <td>0.296484</td>\n",
       "      <td>1.366610</td>\n",
       "      <td>0.565044</td>\n",
       "      <td>0.560585</td>\n",
       "      <td>0.562806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>bert</td>\n",
       "      <td>96782</td>\n",
       "      <td>26837</td>\n",
       "      <td>0.277293</td>\n",
       "      <td>1.065770</td>\n",
       "      <td>0.660759</td>\n",
       "      <td>0.635036</td>\n",
       "      <td>0.647643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>br</td>\n",
       "      <td>br</td>\n",
       "      <td>breton</td>\n",
       "      <td>breton</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>95428</td>\n",
       "      <td>26398</td>\n",
       "      <td>0.276627</td>\n",
       "      <td>1.077589</td>\n",
       "      <td>0.579345</td>\n",
       "      <td>0.559611</td>\n",
       "      <td>0.569307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1370517</td>\n",
       "      <td>622926</td>\n",
       "      <td>0.454519</td>\n",
       "      <td>1.025551</td>\n",
       "      <td>0.770558</td>\n",
       "      <td>0.767019</td>\n",
       "      <td>0.768784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>fa</td>\n",
       "      <td>fa</td>\n",
       "      <td>persian</td>\n",
       "      <td>persian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1516814</td>\n",
       "      <td>688212</td>\n",
       "      <td>0.453722</td>\n",
       "      <td>0.789991</td>\n",
       "      <td>0.756894</td>\n",
       "      <td>0.745259</td>\n",
       "      <td>0.751031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>bert</td>\n",
       "      <td>509206</td>\n",
       "      <td>25444</td>\n",
       "      <td>0.049968</td>\n",
       "      <td>0.892956</td>\n",
       "      <td>0.728365</td>\n",
       "      <td>0.721085</td>\n",
       "      <td>0.724707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>hi</td>\n",
       "      <td>hi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>hindi</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>635212</td>\n",
       "      <td>30647</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.706041</td>\n",
       "      <td>0.766307</td>\n",
       "      <td>0.777249</td>\n",
       "      <td>0.771739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>bert</td>\n",
       "      <td>70465</td>\n",
       "      <td>56074</td>\n",
       "      <td>0.795771</td>\n",
       "      <td>0.762499</td>\n",
       "      <td>0.778598</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.788785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sco</td>\n",
       "      <td>sco</td>\n",
       "      <td>scots</td>\n",
       "      <td>scots</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>67738</td>\n",
       "      <td>53817</td>\n",
       "      <td>0.794488</td>\n",
       "      <td>1.107679</td>\n",
       "      <td>0.596838</td>\n",
       "      <td>0.571970</td>\n",
       "      <td>0.584139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>bert</td>\n",
       "      <td>169960</td>\n",
       "      <td>134207</td>\n",
       "      <td>0.789639</td>\n",
       "      <td>0.837250</td>\n",
       "      <td>0.717710</td>\n",
       "      <td>0.740385</td>\n",
       "      <td>0.728871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cy</td>\n",
       "      <td>cy</td>\n",
       "      <td>welsh</td>\n",
       "      <td>welsh</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>189323</td>\n",
       "      <td>149728</td>\n",
       "      <td>0.790860</td>\n",
       "      <td>0.899729</td>\n",
       "      <td>0.629921</td>\n",
       "      <td>0.659341</td>\n",
       "      <td>0.644295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>bert</td>\n",
       "      <td>1408210</td>\n",
       "      <td>532876</td>\n",
       "      <td>0.378407</td>\n",
       "      <td>0.996878</td>\n",
       "      <td>0.801397</td>\n",
       "      <td>0.796164</td>\n",
       "      <td>0.798772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ca</td>\n",
       "      <td>ca</td>\n",
       "      <td>catalan</td>\n",
       "      <td>catalan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1392222</td>\n",
       "      <td>527252</td>\n",
       "      <td>0.378713</td>\n",
       "      <td>0.851759</td>\n",
       "      <td>0.762098</td>\n",
       "      <td>0.764742</td>\n",
       "      <td>0.763418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>bert</td>\n",
       "      <td>645873</td>\n",
       "      <td>335697</td>\n",
       "      <td>0.519757</td>\n",
       "      <td>1.214754</td>\n",
       "      <td>0.720167</td>\n",
       "      <td>0.730744</td>\n",
       "      <td>0.725417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sk</td>\n",
       "      <td>sk</td>\n",
       "      <td>slovak</td>\n",
       "      <td>slovak</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>733988</td>\n",
       "      <td>383005</td>\n",
       "      <td>0.521814</td>\n",
       "      <td>0.931145</td>\n",
       "      <td>0.697277</td>\n",
       "      <td>0.718668</td>\n",
       "      <td>0.707811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>bert</td>\n",
       "      <td>771367</td>\n",
       "      <td>331462</td>\n",
       "      <td>0.429707</td>\n",
       "      <td>1.531774</td>\n",
       "      <td>0.633998</td>\n",
       "      <td>0.632715</td>\n",
       "      <td>0.633356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ms</td>\n",
       "      <td>ms</td>\n",
       "      <td>malay</td>\n",
       "      <td>malay</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>839256</td>\n",
       "      <td>359935</td>\n",
       "      <td>0.428874</td>\n",
       "      <td>1.284943</td>\n",
       "      <td>0.592382</td>\n",
       "      <td>0.597976</td>\n",
       "      <td>0.595166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>bert</td>\n",
       "      <td>172438</td>\n",
       "      <td>131762</td>\n",
       "      <td>0.764112</td>\n",
       "      <td>1.127339</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.707977</td>\n",
       "      <td>0.703966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>oc</td>\n",
       "      <td>oc</td>\n",
       "      <td>occitan</td>\n",
       "      <td>occitan</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>169161</td>\n",
       "      <td>129106</td>\n",
       "      <td>0.763214</td>\n",
       "      <td>1.033922</td>\n",
       "      <td>0.595522</td>\n",
       "      <td>0.568376</td>\n",
       "      <td>0.581633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>bert</td>\n",
       "      <td>394555</td>\n",
       "      <td>179439</td>\n",
       "      <td>0.454788</td>\n",
       "      <td>1.213179</td>\n",
       "      <td>0.683029</td>\n",
       "      <td>0.683989</td>\n",
       "      <td>0.683509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>af</td>\n",
       "      <td>af</td>\n",
       "      <td>african</td>\n",
       "      <td>african</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>418037</td>\n",
       "      <td>190324</td>\n",
       "      <td>0.455280</td>\n",
       "      <td>1.008702</td>\n",
       "      <td>0.628205</td>\n",
       "      <td>0.619382</td>\n",
       "      <td>0.623762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>bert</td>\n",
       "      <td>16013</td>\n",
       "      <td>11024</td>\n",
       "      <td>0.688441</td>\n",
       "      <td>1.158579</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.476190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>scn</td>\n",
       "      <td>scn</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>sicilian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>15997</td>\n",
       "      <td>10999</td>\n",
       "      <td>0.687566</td>\n",
       "      <td>1.386137</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.439716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>bert</td>\n",
       "      <td>64518</td>\n",
       "      <td>48917</td>\n",
       "      <td>0.758192</td>\n",
       "      <td>0.828800</td>\n",
       "      <td>0.726908</td>\n",
       "      <td>0.704280</td>\n",
       "      <td>0.715415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>aragonese</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>63833</td>\n",
       "      <td>48345</td>\n",
       "      <td>0.757367</td>\n",
       "      <td>1.016292</td>\n",
       "      <td>0.577689</td>\n",
       "      <td>0.564202</td>\n",
       "      <td>0.570866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>bert</td>\n",
       "      <td>1142383</td>\n",
       "      <td>926865</td>\n",
       "      <td>0.811343</td>\n",
       "      <td>1.333950</td>\n",
       "      <td>0.717158</td>\n",
       "      <td>0.712163</td>\n",
       "      <td>0.714652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ast</td>\n",
       "      <td>ast</td>\n",
       "      <td>asturian</td>\n",
       "      <td>asturian</td>\n",
       "      <td>xlmroberta</td>\n",
       "      <td>1094383</td>\n",
       "      <td>887307</td>\n",
       "      <td>0.810783</td>\n",
       "      <td>1.086594</td>\n",
       "      <td>0.646474</td>\n",
       "      <td>0.634169</td>\n",
       "      <td>0.640262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1   l2     l1-name    l2-name  model_type  total_O_l2  same_O_l2  \\\n",
       "0    fr   br      french     breton        bert       96782       9197   \n",
       "1    fr   br      french     breton  xlmroberta       95428       9007   \n",
       "2    ar   fa      arabic    persian        bert     1370517     200844   \n",
       "3    ar   fa      arabic    persian  xlmroberta     1516814     227707   \n",
       "4    ar   hi      arabic      hindi        bert      509206      10813   \n",
       "5    ar   hi      arabic      hindi  xlmroberta      635212      12554   \n",
       "6    en  sco     english      scots        bert       70465      20591   \n",
       "7    en  sco     english      scots  xlmroberta       67738      19776   \n",
       "8    en   cy     english      welsh        bert      169960      29425   \n",
       "9    en   cy     english      welsh  xlmroberta      189323      32342   \n",
       "10   es   ca     spanish    catalan        bert     1408210     241133   \n",
       "11   es   ca     spanish    catalan  xlmroberta     1392222     239396   \n",
       "12   cs   sk       czech     slovak        bert      645873     156704   \n",
       "13   cs   sk       czech     slovak  xlmroberta      733988     177465   \n",
       "14   id   ms  indonesian      malay        bert      771367     280302   \n",
       "15   id   ms  indonesian      malay  xlmroberta      839256     304953   \n",
       "16   fr   oc      french    occitan        bert      172438      39816   \n",
       "17   fr   oc      french    occitan  xlmroberta      169161      38982   \n",
       "18   nl   af       dutch    african        bert      394555      89272   \n",
       "19   nl   af       dutch    african  xlmroberta      418037      94369   \n",
       "20   it  scn     italian   sicilian        bert       16013       4162   \n",
       "21   it  scn     italian   sicilian  xlmroberta       15997       4137   \n",
       "22   es   an     spanish  aragonese        bert       64518      16005   \n",
       "23   es   an     spanish  aragonese  xlmroberta       63833      15812   \n",
       "24   es  ast     spanish   asturian        bert     1142383     338008   \n",
       "25   es  ast     spanish   asturian  xlmroberta     1094383     324467   \n",
       "26   br   br      breton     breton        bert       96782      26837   \n",
       "27   br   br      breton     breton  xlmroberta       95428      26398   \n",
       "28   fa   fa     persian    persian        bert     1370517     622926   \n",
       "29   fa   fa     persian    persian  xlmroberta     1516814     688212   \n",
       "30   hi   hi       hindi      hindi        bert      509206      25444   \n",
       "31   hi   hi       hindi      hindi  xlmroberta      635212      30647   \n",
       "32  sco  sco       scots      scots        bert       70465      56074   \n",
       "33  sco  sco       scots      scots  xlmroberta       67738      53817   \n",
       "34   cy   cy       welsh      welsh        bert      169960     134207   \n",
       "35   cy   cy       welsh      welsh  xlmroberta      189323     149728   \n",
       "36   ca   ca     catalan    catalan        bert     1408210     532876   \n",
       "37   ca   ca     catalan    catalan  xlmroberta     1392222     527252   \n",
       "38   sk   sk      slovak     slovak        bert      645873     335697   \n",
       "39   sk   sk      slovak     slovak  xlmroberta      733988     383005   \n",
       "40   ms   ms       malay      malay        bert      771367     331462   \n",
       "41   ms   ms       malay      malay  xlmroberta      839256     359935   \n",
       "42   oc   oc     occitan    occitan        bert      172438     131762   \n",
       "43   oc   oc     occitan    occitan  xlmroberta      169161     129106   \n",
       "44   af   af     african    african        bert      394555     179439   \n",
       "45   af   af     african    african  xlmroberta      418037     190324   \n",
       "46  scn  scn    sicilian   sicilian        bert       16013      11024   \n",
       "47  scn  scn    sicilian   sicilian  xlmroberta       15997      10999   \n",
       "48   an   an   aragonese  aragonese        bert       64518      48917   \n",
       "49   an   an   aragonese  aragonese  xlmroberta       63833      48345   \n",
       "50  ast  ast    asturian   asturian        bert     1142383     926865   \n",
       "51  ast  ast    asturian   asturian  xlmroberta     1094383     887307   \n",
       "\n",
       "    percentage_O_l2  eval_loss  precision    recall  f1_score  \n",
       "0          0.095028   1.676681   0.597990  0.579075  0.588381  \n",
       "1          0.094385   1.454213   0.555283  0.549878  0.552567  \n",
       "2          0.146546   0.985265   0.740860  0.736275  0.738560  \n",
       "3          0.150122   0.794978   0.730738  0.727091  0.728910  \n",
       "4          0.021235   1.302796   0.630821  0.625416  0.628107  \n",
       "5          0.019763   0.801118   0.754319  0.748215  0.751254  \n",
       "6          0.292216   1.303782   0.738806  0.750000  0.744361  \n",
       "7          0.291948   1.792997   0.596838  0.571970  0.584139  \n",
       "8          0.173129   1.604274   0.626685  0.638736  0.632653  \n",
       "9          0.170830   1.494449   0.561427  0.583791  0.572391  \n",
       "10         0.171234   1.292190   0.716609  0.719241  0.717923  \n",
       "11         0.171952   1.546301   0.654582  0.663130  0.658828  \n",
       "12         0.242624   1.460796   0.680219  0.687990  0.684082  \n",
       "13         0.241782   1.096790   0.684653  0.691580  0.688099  \n",
       "14         0.363383   1.837590   0.607599  0.593255  0.600341  \n",
       "15         0.363361   1.463674   0.552953  0.549410  0.551176  \n",
       "16         0.230900   1.523653   0.683908  0.678063  0.680973  \n",
       "17         0.230443   1.264527   0.582596  0.562678  0.572464  \n",
       "18         0.226260   1.840495   0.650243  0.658006  0.654101  \n",
       "19         0.225743   1.317279   0.608543  0.610253  0.609397  \n",
       "20         0.259914   1.914521   0.632353  0.614286  0.623188  \n",
       "21         0.258611   1.894772   0.526316  0.428571  0.472441  \n",
       "22         0.248070   1.233783   0.711538  0.719844  0.715667  \n",
       "23         0.247709   1.237783   0.572016  0.540856  0.556000  \n",
       "24         0.295880   1.453800   0.691883  0.698468  0.695160  \n",
       "25         0.296484   1.366610   0.565044  0.560585  0.562806  \n",
       "26         0.277293   1.065770   0.660759  0.635036  0.647643  \n",
       "27         0.276627   1.077589   0.579345  0.559611  0.569307  \n",
       "28         0.454519   1.025551   0.770558  0.767019  0.768784  \n",
       "29         0.453722   0.789991   0.756894  0.745259  0.751031  \n",
       "30         0.049968   0.892956   0.728365  0.721085  0.724707  \n",
       "31         0.048247   0.706041   0.766307  0.777249  0.771739  \n",
       "32         0.795771   0.762499   0.778598  0.799242  0.788785  \n",
       "33         0.794488   1.107679   0.596838  0.571970  0.584139  \n",
       "34         0.789639   0.837250   0.717710  0.740385  0.728871  \n",
       "35         0.790860   0.899729   0.629921  0.659341  0.644295  \n",
       "36         0.378407   0.996878   0.801397  0.796164  0.798772  \n",
       "37         0.378713   0.851759   0.762098  0.764742  0.763418  \n",
       "38         0.519757   1.214754   0.720167  0.730744  0.725417  \n",
       "39         0.521814   0.931145   0.697277  0.718668  0.707811  \n",
       "40         0.429707   1.531774   0.633998  0.632715  0.633356  \n",
       "41         0.428874   1.284943   0.592382  0.597976  0.595166  \n",
       "42         0.764112   1.127339   0.700000  0.707977  0.703966  \n",
       "43         0.763214   1.033922   0.595522  0.568376  0.581633  \n",
       "44         0.454788   1.213179   0.683029  0.683989  0.683509  \n",
       "45         0.455280   1.008702   0.628205  0.619382  0.623762  \n",
       "46         0.688441   1.158579   0.535714  0.428571  0.476190  \n",
       "47         0.687566   1.386137   0.436620  0.442857  0.439716  \n",
       "48         0.758192   0.828800   0.726908  0.704280  0.715415  \n",
       "49         0.757367   1.016292   0.577689  0.564202  0.570866  \n",
       "50         0.811343   1.333950   0.717158  0.712163  0.714652  \n",
       "51         0.810783   1.086594   0.646474  0.634169  0.640262  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_perturb_random=pd.read_csv('title_perturb_random5',sep=',', encoding='utf-8')\n",
    "perturb_5_compare=language_accuracy.merge(title_perturb_random,on=['l1','l2','l1-name','l2-name','model_type'],suffixes=('_before', '_after'))\n",
    "perturb_5_compare.to_csv('perturb_5_compare_random',sep=',', encoding='utf-8',index=False)\n",
    "perturb_5_compare.to_excel('perturb_5_compare_random.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "794870ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "perturb_5_compare=pd.read_csv('perturb_5_compare_random',sep=',', encoding='utf-8')\n",
    "all_lang_info=perturb_5_compare\n",
    "# all_lang_info=pd.read_csv('train_test_perturb_1_compare',sep=',', encoding='utf-8')\n",
    "# all_lang_info[['l1','l2','l1-name','l2-name','model_type','f1_score_before','f1_score_after']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce1cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
