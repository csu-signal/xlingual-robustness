{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b3cc57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n",
      "you shouldn't go in here\n"
     ]
    }
   ],
   "source": [
    "# preprocessing/run_limit_and_split_datasets.sh\n",
    "from preprocessing.run_limit_and_split_datasets import split_datasets\n",
    "split_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c945cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/run_limit_datasets.sh\n",
    "from preprocessing.run_limit_datasets import limit_datasets\n",
    "limit_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef6d3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing/run_limit_datasets_loop.sh\n",
    "from preprocessing.run_limit_datasets_loop import loop_datasets\n",
    "loop_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86bdedaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_source='br'\n",
    "language_target='br'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ddc0a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing.take_sentences import preprocess\n",
    "from SimpleTransformers import NERModel\n",
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"\n",
    "model_type='bert'\n",
    "for i in range(4):\n",
    "    preprocess( path+language_source+'/dataset_{dataset_number}.txt'.format(dataset_number=i)\n",
    "           ,path+language_source+'/dataset_train.txt', path+language_source+'/dataset_eval.txt', \"0\")\n",
    "    preprocess( path+language_target+'/dataset_{dataset_number}.txt'.format(dataset_number=i)\n",
    "           ,path+language_target+'/dataset_train.txt', path+language_target+'/dataset_eval.txt', \"0\")\n",
    "    outputdir=path+language_source+'/results_lan/results_0.0/output_0.0/'+language_source+'_'+language_target+'_{dataset_number}_'.format(dataset_number=i)+model_type\n",
    "    model = NERModel(model_type, \n",
    "                'bert-base-multilingual-cased',\n",
    "                labels=[\"O\",\n",
    "                \"B-MISC\",\n",
    "                \"I-MISC\",\n",
    "                \"B-PER\",\n",
    "                \"I-PER\",\n",
    "                \"B-ORG\",\n",
    "                \"I-ORG\",\n",
    "                \"B-LOC\",\n",
    "                \"I-LOC\"],\n",
    "                use_cuda=False,\n",
    "                args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d7a830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49235fa5e204e32a2cc880fc9653954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/red/a/nobackup/cwc-ro/.conda/mbert_ner/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191f4f81e63e46b38cd9b4ec6c6d8760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98455943a3d049729c6b1c8e74c99a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current iteration:   0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.643964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/red/a/nobackup/cwc-ro/.conda/mbert_ner/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.009008"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e2532c21af4a9e91368c7e5f3a2daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfa8aa7bcfa47cbb68d1828144cdffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainingdataset=path+language_source+'/dataset_train.txt'\n",
    "testdataset=path+language_target+'/dataset_eval.txt'\n",
    "model.train_model(trainingdataset,eval_df=testdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa41d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from preprocessing.take_sentences import preprocess\n",
    "from SimpleTransformers import NERModel\n",
    "path=\"/s/red/a/nobackup/cwc-ro/shadim/languages/\"\n",
    "model_type='xlmroberta'\n",
    "for i in range(4):\n",
    "    preprocess( path+language_source+'/dataset_{dataset_number}.txt'.format(dataset_number=i)\n",
    "           ,path+language_source+'/dataset_train.txt', path+language_source+'/dataset_eval.txt', \"0\")\n",
    "    preprocess( path+language_target+'/dataset_{dataset_number}.txt'.format(dataset_number=i)\n",
    "           ,path+language_target+'/dataset_train.txt', path+language_target+'/dataset_eval.txt', \"0\")\n",
    "    outputdir=path+language_source+'/results_lan/results_0.0/output_0.0/'+language_source+'_'+language_target+'_{dataset_number}_'.format(dataset_number=i)+model_type\n",
    "    model = NERModel(model_type, \n",
    "                'xlm-roberta-base',\n",
    "                labels=[\"O\",\n",
    "                \"B-MISC\",\n",
    "                \"I-MISC\",\n",
    "                \"B-PER\",\n",
    "                \"I-PER\",\n",
    "                \"B-ORG\",\n",
    "                \"I-ORG\",\n",
    "                \"B-LOC\",\n",
    "                \"I-LOC\"],\n",
    "                use_cuda=False,\n",
    "                args={'save_model_every_epoch':False, 'save_steps': 10000,'output_dir':outputdir, 'evaluate_during_training':True,'overwrite_output_dir':True, 'classification_report': True, 'save_eval_checkpoints':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fc671c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ea2ad386c46c0b8644d2418fd0aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13602 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d989eb1f49f448ceacc33cc6a03bf18f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e6f215cb9840e5867fd5256690f739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Current iteration:   0%|          | 0/1701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.049118"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026af7344f834ec2a048c988f3b87d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22452b3ff8e24579aababa3a50efed10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/426 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainingdataset=path+language_source+'/dataset_train.txt'\n",
    "testdataset=path+language_target+'/dataset_eval.txt'\n",
    "model.train_model(trainingdataset,eval_df=testdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1599e16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67b3aba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m                                                                                                                                                                            \n\u001b[1;32m      2\u001b[0m transformers\u001b[38;5;241m.\u001b[39mis_torch_available()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import transformers                                                                                                                                                                            \n",
    "transformers.is_torch_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83053bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
